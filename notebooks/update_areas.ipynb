{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767f87e9",
   "metadata": {},
   "source": [
    "# Estimate glacier areas in each image, update transient AARs and snowline altitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c422f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from p_tqdm import p_map\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "# Define paths in directoru for convenience\n",
    "data_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites'\n",
    "figures_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis/figures'\n",
    "out_path = os.path.join(data_path, '..', 'analysis')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3808cd",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aois_fn = os.path.join(out_path, 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5aa9f4",
   "metadata": {},
   "source": [
    "## Define processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6df6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to solve for optimal UTM zone\n",
    "def convert_wgs_to_utm(lon: float, lat: float):\n",
    "    \"\"\"\n",
    "    Return best UTM epsg-code based on WGS84 lat and lon coordinate pair\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lon: float\n",
    "        longitude coordinate\n",
    "    lat: float\n",
    "        latitude coordinate\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    epsg_code: str\n",
    "        optimal UTM zone, e.g. \"EPSG:32606\"\n",
    "    \"\"\"\n",
    "    utm_band = str(int((np.floor((lon + 180) / 6) % 60) + 1))\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0' + utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = 'EPSG:326' + utm_band\n",
    "        return epsg_code\n",
    "    epsg_code = 'EPSG:327' + utm_band\n",
    "    return epsg_code\n",
    "\n",
    "\n",
    "# Define function to check classified images for debris cover\n",
    "def estimate_areas_sla(aoi, classified_folder, dem_fn, crs_utm=None, plot_ts=True):\n",
    "    # get classified image file names\n",
    "    classified_fns = sorted(glob.glob(os.path.join(classified_folder, '*.nc')))\n",
    "\n",
    "    # initialize debris variable\n",
    "    glacier_area_list = []\n",
    "    snow_area_list = []\n",
    "    debris_area_list = []\n",
    "    real_area_list = []\n",
    "    masked_area_list = []\n",
    "    aar_list = []\n",
    "    sla_list = []\n",
    "    sla_lower_list = []\n",
    "    sla_upper_list = []\n",
    "    im_dts_list = []\n",
    "\n",
    "    # identify optimal UTM zone for better area calculations\n",
    "    if crs_utm is None:\n",
    "        aoi = aoi.to_crs(\"EPSG:4326\")\n",
    "        aoi_centroid = aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0]\n",
    "        crs_utm = convert_wgs_to_utm(aoi_centroid[0], aoi_centroid[1])\n",
    "        aoi = aoi.to_crs(crs_utm)\n",
    "        # print('Optimal UTM zone =', crs_utm)\n",
    "    \n",
    "    # Load DEM\n",
    "    dem = rxr.open_rasterio(dem_fn).squeeze()\n",
    "    dem_crs = dem.rio.crs\n",
    "    if 'band' in dem.coords:\n",
    "        if len(np.shape(dem.coords['band'])) > 0: # NASADEM sometimes has extra bands, only need the first one\n",
    "            dem = dem.isel(band=0) \n",
    "        dem = dem.reset_coords('band', drop=True) # drop the \"band\" coordinate\n",
    "    dem = dem.rio.clip(aoi.geometry) # clip DEM to AOI to speed up later computations\n",
    "    dem = xr.where((dem < -1e3) | (dem > 1e4), np.nan, dem) # mask no-data values\n",
    "    dem = dem.rio.write_crs(dem_crs) # re-assign CRS\n",
    "    dem = dem.rio.reproject(crs_utm) # reproject to UTM\n",
    "    dem_min = np.nanmin(dem.data)\n",
    "    dem_max = np.nanmax(dem.data)\n",
    "\n",
    "    # iterate over classified images\n",
    "    for i, fn in enumerate(classified_fns):\n",
    "        # get image datetime from file name\n",
    "        im_dt = datetime.datetime.strptime(os.path.basename(fn).split('_')[0], '%Y%m%dT%H%M%S')\n",
    "\n",
    "        # open classified image\n",
    "        im_classified = rxr.open_rasterio(fn).squeeze()\n",
    "        if f\"EPSG:{im_classified.rio.crs.to_epsg()}\" != crs_utm:\n",
    "            im_classified = im_classified.rio.reproject(crs_utm)\n",
    "        im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "        im_res = im_classified.rio.resolution()[0]\n",
    "        im_classified = im_classified.rio.write_crs(crs_utm)\n",
    "\n",
    "        # Create masks of snow, ice, and debris-covered areas\n",
    "        im_classified['glacier_mask'] = xr.where(im_classified.isin([1, 2, 3]), 1, 0)\n",
    "        im_classified['snow_mask'] = xr.where(im_classified.isin([1,2]), 1, 0)\n",
    "        im_classified['debris_mask'] = xr.where(im_classified.isin([5]), 1, 0)\n",
    "\n",
    "        # Calculate areas of each mask\n",
    "        glacier_area = (im_classified['glacier_mask'] == 1).sum().values * im_res**2\n",
    "        snow_area = (im_classified['snow_mask'] == 1).sum().values * im_res**2\n",
    "        debris_area = (im_classified['debris_mask'] == 1).sum().values * im_res**2\n",
    "        real_area = (~im_classified.isnull()).sum().values * im_res**2\n",
    "        masked_area = (im_classified.isnull()).sum().values * im_res**2\n",
    "        if glacier_area == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate transient AAR\n",
    "        aar = snow_area / glacier_area\n",
    "\n",
    "        # Calculate snowliine altitude (SLA) from DEM and AAR\n",
    "        dem_glacier = dem.rio.reproject_match(im_classified) # reproject DEM to image coordinates\n",
    "        im_classified['glacier_elevations'] = dem_glacier\n",
    "        im_classified['glacier_elevations'] = xr.where(im_classified['glacier_mask']==1, \n",
    "                                                       im_classified['glacier_elevations'], \n",
    "                                                       np.nan) # mask non-glacier elevations\n",
    "        elev_glacier = np.ravel(im_classified['glacier_elevations'].data) # ravel glacier elevations\n",
    "        sla_percentile = 1 - aar # calculate the percentile to sample\n",
    "        if aar == 1:\n",
    "            sla = dem_min\n",
    "            sla_lower = np.nan\n",
    "            sla_upper = np.nan\n",
    "        elif aar == 0:\n",
    "            sla = dem_max\n",
    "            sla_lower = np.nan\n",
    "            sla_upper = np.nan\n",
    "        else:\n",
    "            sla = np.nanquantile(elev_glacier, sla_percentile)\n",
    "            # Calculate lower and upper bounds on the SLA\n",
    "            # identify snow-free pixels above the SLA and snow-covered pixels below the SLA\n",
    "            snow_free_above_sla = xr.where((dem_glacier > sla) & (im_classified['snow_mask'] == 0), 1, 0)\n",
    "            snow_free_above_sla_area = len(np.argwhere(snow_free_above_sla.data.ravel()==1).ravel()) * im_res**2\n",
    "            snow_covered_below_sla = xr.where((dem_glacier < sla) & (im_classified['snow_mask'] == 1), 1, 0)\n",
    "            snow_covered_below_sla_area = len(np.argwhere(snow_covered_below_sla.data.ravel()==1).ravel()) * im_res**2\n",
    "            # convert areas to percentiles\n",
    "            delta_up = snow_free_above_sla_area / glacier_area\n",
    "            delta_down = snow_covered_below_sla_area / glacier_area\n",
    "            # adjust SLA percentiles\n",
    "            upper_sla_percentile = sla_percentile + delta_up\n",
    "            lower_sla_percentile = sla_percentile - delta_down\n",
    "            # make sure percentiles are within [0,1]\n",
    "            upper_sla_percentile, lower_sla_percentile = np.clip([upper_sla_percentile, lower_sla_percentile], 0, 1)\n",
    "            sla_upper = np.nanpercentile(elev_glacier, upper_sla_percentile * 100)\n",
    "            sla_lower = np.nanpercentile(elev_glacier, lower_sla_percentile * 100)\n",
    "\n",
    "        # Add results to lists\n",
    "        glacier_area_list += [glacier_area]\n",
    "        snow_area_list += [snow_area]\n",
    "        debris_area_list += [debris_area]\n",
    "        real_area_list += [real_area]\n",
    "        masked_area_list += [masked_area]\n",
    "        aar_list += [aar]\n",
    "        sla_list += [sla]        \n",
    "        sla_lower_list += [sla_lower]\n",
    "        sla_upper_list += [sla_upper]\n",
    "        im_dts_list += [im_dt]\n",
    "\n",
    "    # compile in dataframe\n",
    "    df = pd.DataFrame({'datetime': im_dts_list, \n",
    "                       'snow_area_m2': snow_area_list,\n",
    "                       'glacier_area_m2': glacier_area_list,\n",
    "                       'debris_area_m2': debris_area_list, \n",
    "                       'total_area_m2': real_area_list,\n",
    "                       'masked_area_m2': masked_area_list,\n",
    "                       'transient_AAR': aar_list,\n",
    "                       'SLA_m': sla_list,\n",
    "                       'SLA_upper_bound_m': sla_upper_list,\n",
    "                       'SLA_lower_bound_m': sla_lower_list})\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # plot time series\n",
    "    if plot_ts:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "        ax.plot(df['datetime'], df['total_area_m2'] / 1e6, 'o', color='k', label='Total (image px)')\n",
    "        ax.plot(df['datetime'], df['debris_area_m2'] / 1e6, '^', color='#bf812d', label='Debris')\n",
    "        ax.plot(df['datetime'], df['glacier_area_m2'] / 1e6, '.', color='#35978f', label='Glacier')\n",
    "        ax.plot(df['datetime'], df['snow_area_m2'] / 1e6, '*b', label='Snow')\n",
    "        ax.legend(loc='best')\n",
    "        ax.set_ylabel('Area (km$^2$)')\n",
    "        fig.suptitle(aoi['RGIId'].values[0] + '\\nAOI area = ' + str(aoi['Area'].values[0]) + ' km$^2$')\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return df, fig\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf753a",
   "metadata": {},
   "source": [
    "## Run and save new snow cover stats for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2311fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define order of columns for output files\n",
    "col_order = ['RGIId', 'datetime', 'source', 'snow_area_m2', 'glacier_area_m2', 'debris_area_m2', 'masked_area_m2',\n",
    "             'transient_AAR', 'SLA_m', 'SLA_lower_bound_m', 'SLA_upper_bound_m', \n",
    "             'snowline_elevs_m', 'snowline_elevs_median_m', 'snowline_geometry']\n",
    "\n",
    "# Iterate over sites\n",
    "for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "    # define output file name\n",
    "    sc_stats_new_fn = os.path.join(data_path, rgi_id, f\"{rgi_id}_snow_cover_stats_adjusted.csv\")\n",
    "    if os.path.exists(sc_stats_new_fn):\n",
    "        continue\n",
    "    \n",
    "    # Load snow cover stats\n",
    "    sc_stats_fn = os.path.join(data_path, rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "    sc_stats = pd.read_csv(sc_stats_fn)\n",
    "    sc_stats['datetime'] = pd.to_datetime(sc_stats['datetime'])\n",
    "\n",
    "    # Subset AOIs to site\n",
    "    aoi = aois.loc[aois['RGIId'] == rgi_id]\n",
    "    aoi.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Define DEM file name\n",
    "    dem_fn = glob.glob(os.path.join(data_path, rgi_id, 'DEMs', f\"{rgi_id}*clip*.tif\"))[0]\n",
    "\n",
    "    # Calculate areas of each variable\n",
    "    classified_folder = os.path.join(data_path, aoi['RGIId'].values[0], 'classified')\n",
    "    df = estimate_areas_sla(aoi, classified_folder, dem_fn, plot_ts=False)\n",
    "\n",
    "    # Merge dataframes\n",
    "    sc_stats = sc_stats.merge(df, on='datetime')\n",
    "    sc_stats = sc_stats[col_order]\n",
    "\n",
    "    # Save to file\n",
    "    sc_stats.to_csv(sc_stats_new_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d7853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
