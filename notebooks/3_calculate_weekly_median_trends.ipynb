{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fee1d3-89a0-4332-91af-77fbee8cc9ed",
   "metadata": {},
   "source": [
    "# Calculate median weekly trends in snow cover for each study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded5b9-6a41-4e78-aa7d-dbec024f6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import iqr\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import median_abs_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d398a-23a0-4431-aee8-5fe622f39b68",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries (AOIs) and climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97356-a7e0-4c83-af84-ceee07b044fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define root path to data for convenience\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "# Define output directory\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "\n",
    "# Load AOIs\n",
    "aois_fn = os.path.join(out_path, 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d31fd3",
   "metadata": {},
   "source": [
    "## Pre-processing: Compile snow cover stats CSVs for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "from ast import literal_eval\n",
    "from pyproj import Transformer\n",
    "import shutil\n",
    "\n",
    "# Define columns to save\n",
    "out_cols = ['RGIId', 'datetime', 'source', 'SCA_m2', 'AAR', 'SLA_from_AAR_m', 'snowline_elevs_m', 'snowline_elevs_median_m', 'snowline_geometry']\n",
    "\n",
    "# Define function to convert lists of X and Y coordinates to a LineString and transform to WGS84\n",
    "def create_linestring_wgs84(x_coords, y_coords, transformer):\n",
    "    # Create list of (x, y) coordinate tuples\n",
    "    points = list(zip(x_coords, y_coords))\n",
    "    # Create LineString in UTM\n",
    "    line = LineString(points)\n",
    "    # Transform to WGS84\n",
    "    line_wgs84 = LineString([transformer.transform(x, y) for x, y in line.coords])\n",
    "    return line_wgs84\n",
    "\n",
    "# Iterate over sites\n",
    "for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "    # Define output file name\n",
    "    scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "    if not os.path.exists(scs_fn):\n",
    "        # Initialize dataframe\n",
    "        scs = pd.DataFrame()\n",
    "        \n",
    "        # Get snow cover stats file names\n",
    "        sc_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'snow_cover_stats', '*_snow_cover_stats.csv')))\n",
    "        if len(sc_fns) < 1:\n",
    "            continue\n",
    "        \n",
    "        # Merge files, not including PlanetScope\n",
    "        for fn in sc_fns:\n",
    "            if 'PlanetScope' not in fn:\n",
    "                sc = pd.read_csv(fn)\n",
    "                scs = pd.concat([scs, sc])\n",
    "                \n",
    "        # Merge any redundant columns (from old versions of the code)\n",
    "        cols = list(scs.keys())\n",
    "        if ('site_name' in cols) & ('RGIId' in cols):\n",
    "            scs['RGIId'] = scs['RGIId'].fillna(scs['site_name'])\n",
    "            scs.drop(columns=['site_name'], inplace=True)\n",
    "        elif 'site_name' in cols:\n",
    "            scs.rename(columns={'site_name': 'RGIId'}, inplace=True)\n",
    "        if ('dataset' in cols) & ('source' in cols):\n",
    "            scs['source'] = scs['source'].fillna(scs['dataset'])\n",
    "            scs.drop(columns=['dataset'], inplace=True)\n",
    "        elif 'dataset' in cols:\n",
    "            scs.rename(columns={'dataset': 'source'}, inplace=True)\n",
    "        scs.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Rename \"ELA_from_AAR_m\" column to \"SLA_from_AAR_m\" to be more representative\n",
    "        scs.rename(columns={'ELA_from_AAR_m': 'SLA_from_AAR_m'}, inplace=True)\n",
    "        \n",
    "        # Ensure coordinate lists are correctly formatted\n",
    "        scs['snowlines_coords_X'] = scs['snowlines_coords_X'].apply(lambda x: literal_eval(x) if x != \"[]\" else [])\n",
    "        scs['snowlines_coords_Y'] = scs['snowlines_coords_Y'].apply(lambda x: literal_eval(x) if x != \"[]\" else [])\n",
    "        \n",
    "        # Get the UTM CRS for transformation\n",
    "        crs_utm = scs['HorizontalCRS'].drop_duplicates().dropna().values[0]\n",
    "        transformer = Transformer.from_crs(crs_utm, \"EPSG:4326\", always_xy=True)\n",
    "        \n",
    "        # Create and transform snowline geometries\n",
    "        scs['snowline_geometry'] = scs.apply(lambda row: create_linestring_wgs84(row['snowlines_coords_X'], row['snowlines_coords_Y'], transformer), axis=1)\n",
    "        \n",
    "        # Select the relevant columns\n",
    "        scs = scs[out_cols]\n",
    "        \n",
    "        # Save merged, adjusted dataframe\n",
    "        scs.to_csv(scs_fn, index=False)\n",
    "        \n",
    "        # Remove old files\n",
    "        old_folder = os.path.join(scm_path, 'study-sites', rgi_id, 'snow_cover_stats')\n",
    "        shutil.rmtree(old_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada9ced-080b-493c-bdaf-8ac48a51bfb1",
   "metadata": {},
   "source": [
    "## Calculate weekly median trends for each site\n",
    "\n",
    "Conduct Monte Carlo simulations for sampling, with number of simulations = 100 and percent sampled = 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23ff0d-5260-47ad-86d0-9a053e716cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scs_MCs_fn = os.path.join(out_path, 'median_snow_cover_stats_MC_adjusted.nc')\n",
    "if not os.path.exists(scs_MCs_fn):\n",
    "    # Set up Monte Carlo parameters\n",
    "    nMC = 100  # Number of Monte Carlo simulations\n",
    "    sample_fraction = 0.8  # Fraction of data to sample in each simulation\n",
    "\n",
    "    # Initialize list to store results for all sites\n",
    "    scs_MCs_list = []\n",
    "\n",
    "    # Iterate over study sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats_adjusted.csv')\n",
    "\n",
    "        if not os.path.exists(scs_fn):\n",
    "            print(f'Skipping {rgi_id}, file not found.')\n",
    "            continue\n",
    "\n",
    "        # Read and preprocess data\n",
    "        scs_site = pd.read_csv(scs_fn)\n",
    "        scs_site['datetime'] = pd.to_datetime(scs_site['datetime'], errors='coerce')\n",
    "        scs_site.dropna(subset=['datetime'], inplace=True)\n",
    "        scs_site['WOY'] = scs_site['datetime'].dt.isocalendar().week\n",
    "\n",
    "        # Number of samples per simulation\n",
    "        nsamp = int(len(scs_site) * sample_fraction)\n",
    "\n",
    "        # Monte Carlo simulations\n",
    "        results = []\n",
    "        for i in range(nMC):\n",
    "            sampled_indices = np.random.choice(scs_site.index, size=nsamp, replace=False)\n",
    "            scs_site_MC = scs_site.loc[sampled_indices].sort_values(by='datetime')\n",
    "\n",
    "            # Calculate weekly medians for AAR, SCA, and ELA\n",
    "            weekly_medians = scs_site_MC.groupby('WOY')[['transient_AAR', 'snow_area_m2', 'SLA_m']].median()\n",
    "            weekly_medians['MC_run'] = i\n",
    "            results.append(weekly_medians.reset_index())\n",
    "\n",
    "        # Concatenate results for all Monte Carlo runs\n",
    "        all_results = pd.concat(results)\n",
    "        all_results['RGIId'] = rgi_id # add RGI ID\n",
    "        \n",
    "        # Convert to xarray.Dataset\n",
    "        ds = all_results.set_index(['RGIId', 'MC_run', 'WOY']).to_xarray()\n",
    "        scs_MCs_list.append(ds)\n",
    "\n",
    "    # Combine results into a single xarray.Dataset\n",
    "    scs_MCs_ds = xr.concat(scs_MCs_list, dim='RGIId')\n",
    "\n",
    "    # Save to file\n",
    "    scs_MCs_ds.to_netcdf(scs_MCs_fn)\n",
    "    print(\"Monte Carlo simulations completed and saved to file:\", scs_MCs_fn)\n",
    "\n",
    "else:\n",
    "    scs_MCs_ds = xr.open_dataset(scs_MCs_fn)\n",
    "    print('Monte Carlo simulations loaded from file:', scs_MCs_fn)\n",
    "    \n",
    "scs_MCs_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec25ef-248b-4fff-8a5e-b7ac70f2827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Compile minimum snow cover statistics\n",
    "min_aars_woys_fn = os.path.join(out_path, 'minimum_snow_cover_stats_adjusted.csv') \n",
    "# check if exists in directory\n",
    "if not os.path.exists(min_aars_woys_fn):\n",
    "    # initialize dataframe for RGI stats and minimum snow cover statts\n",
    "    min_aars_woys = pd.DataFrame()\n",
    "    \n",
    "    # iterate over site names in median snow cover stats dataframe\n",
    "    for rgi_id in tqdm(scs_MCs_ds.RGIId.values):\n",
    "        \n",
    "        # Calculate AAR median and MAD across MC simulations\n",
    "        aar_median = float(scs_MCs_ds.sel(RGIId=rgi_id)['transient_AAR'].min(dim='WOY').median().values)\n",
    "        aar_mad = float(median_abs_deviation(scs_MCs_ds.sel(RGIId=rgi_id)['transient_AAR'].min(dim='WOY').values))\n",
    "        \n",
    "        # Calculate WOY median and MAD across MC simulations\n",
    "        Imins = scs_MCs_ds.sel(RGIId=rgi_id)['transient_AAR'].argmin(dim='WOY').values\n",
    "        woys = scs_MCs_ds.WOY.values[Imins]\n",
    "        woy_median = int(np.nanmedian(woys))\n",
    "        woy_mad = int(median_abs_deviation(woys))\n",
    "\n",
    "        # Calculate AAR for only September (weeks 35-40)\n",
    "        aar_sept = float(scs_MCs_ds.sel(RGIId=rgi_id).sel(WOY=slice(35,40))['transient_AAR'].min(dim='WOY').median().values)\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'RGIId': [rgi_id],\n",
    "            'WOY_median': [woy_median],\n",
    "            'WOY_MAD': [woy_mad],\n",
    "            'AAR_median': [aar_median],\n",
    "            'AAR_MAD': [aar_mad],\n",
    "            'AAR_Sept': [aar_sept]\n",
    "            })\n",
    "        # concatenate to full dataframe\n",
    "        min_aars_woys = pd.concat([min_aars_woys, df], axis=0)\n",
    "\n",
    "    # save to file\n",
    "    min_aars_woys.to_csv(min_aars_woys_fn, index=False)\n",
    "    print('Minimum AARs and WOYs saved to file: ', min_aars_woys_fn)\n",
    "        \n",
    "else:\n",
    "    # load from file\n",
    "    min_aars_woys = pd.read_csv(min_aars_woys_fn)\n",
    "    print('Minimum AARs and WOYs loaded from file.')\n",
    "\n",
    "min_aars_woys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ce47e-be94-4518-bf30-c319db2e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subregion and cluster columns\n",
    "min_aars_woys = min_aars_woys.merge(aois[['RGIId', 'Subregion']], on='RGIId')\n",
    "min_aars_woys = min_aars_woys.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.kdeplot(data=min_aars_woys, x='AAR_median', palette='mako', cumulative=True, hue='Subregion', \n",
    "             ax=ax[0])\n",
    "sns.kdeplot(data=min_aars_woys, x='AAR_median', cumulative=True, hue='clustName', \n",
    "             ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781acfe-3e6d-46a1-8962-1086332f34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Median AAR MAD = ', min_aars_woys['AAR_MAD'].median())\n",
    "print('Median WOY MAD = ', min_aars_woys['WOY_MAD'].median())\n",
    "min_aars_woys['AAR_Sept-AAR_median'] = min_aars_woys['AAR_Sept'] - min_aars_woys['AAR_median']\n",
    "print('AAR September - AAR median')\n",
    "min_aars_woys['AAR_Sept-AAR_median'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2293b96-3a07-4559-9728-4569893eef51",
   "metadata": {},
   "source": [
    "## Assess interannual variability in AAR magnitude and timing at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac151-e7a3-419a-b51f-2c19fcc9a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats_fn = os.path.join(os.path.join(out_path, 'minimum_snow_cover_stats_interannual_variability_2016-2023_adjusted.csv'))\n",
    "if os.path.exists(aar_var_stats_fn):\n",
    "    aar_var_stats = pd.read_csv(aar_var_stats_fn)\n",
    "    print('AAR interannual variability stats loaded from file.')\n",
    "\n",
    "else:\n",
    "    aar_var_stats = pd.DataFrame()\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load snow cover stats\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        if not os.path.exists(scs_fn):\n",
    "            print(f'Skipping {rgi_id}, no snow cover stats file.')\n",
    "            continue\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        # Add Year and WOY columns\n",
    "        if 'datetime' not in scs.keys():\n",
    "            print(f'Error with {rgi_id}')\n",
    "            continue\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        scs['Year'] = scs['datetime'].dt.isocalendar().year\n",
    "        scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "        \n",
    "        # subset to 2016\n",
    "        scs = scs.loc[scs['Year'] >= 2016]\n",
    "        # identify annual AAR magnitudes and WOY timing\n",
    "        annual_mins_site = scs.groupby('Year')['AAR'].idxmin().reset_index()\n",
    "        annual_mins_site.rename(columns={'AAR': 'Imin'}, inplace=True)\n",
    "        annual_mins_site['AAR'] = [scs.loc[i, 'AAR'] for i in annual_mins_site['Imin'].values]\n",
    "        annual_mins_site['WOY'] = [scs.loc[i, 'WOY'] for i in annual_mins_site['Imin'].values]\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                           'AAR_min': [annual_mins_site['AAR'].min()],\n",
    "                           'AAR_max': [annual_mins_site['AAR'].max()],\n",
    "                           'AAR_median': [annual_mins_site['AAR'].median()],\n",
    "                           'AAR_IQR': [iqr(annual_mins_site['AAR'])],\n",
    "                           'WOY_min': [annual_mins_site['WOY'].min()],\n",
    "                           'WOY_max': [annual_mins_site['WOY'].max()],\n",
    "                           'WOY_median': [annual_mins_site['WOY'].median()],\n",
    "                           'WOY_IQR': [iqr(annual_mins_site['WOY'])]})  \n",
    "        aar_var_stats = pd.concat([aar_var_stats, df])\n",
    "    \n",
    "    aar_var_stats.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save to file\n",
    "    aar_var_stats.to_csv(aar_var_stats_fn, index=False)\n",
    "    print('AAR interannual variability stats saved to file:', aar_var_stats_fn)\n",
    "\n",
    "aar_var_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a3a37-6f68-4f8d-8a7c-611ed1511b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Print stats\n",
    "aar_var_stats['AAR_range'] = aar_var_stats['AAR_max'] - aar_var_stats['AAR_min']\n",
    "print(f\"AAR range for all sites: {iqr(aar_var_stats['AAR_min'])} - {iqr(aar_var_stats['AAR_max'])}\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1127fa-5146-4fad-98fd-344c66a8e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats['WOY_range'] = aar_var_stats['WOY_max'] - aar_var_stats['WOY_min']\n",
    "print(f\"AAR TIMING range for all sites: {aar_var_stats['WOY_range'].median()} +/- {iqr(aar_var_stats['WOY_range'])}\\n\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1886-3983-4022-afa2-5b8e32d52af7",
   "metadata": {},
   "source": [
    "## Identify the approximate start and end of the melt season in each subregion from ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1684a2-53a8-44e4-8d9f-c3f53fdb483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_season_fn = os.path.join(out_path, 'melt_season_timing.csv')\n",
    "\n",
    "if not os.path.exists(melt_season_fn):\n",
    "    melt_season_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load ERA data\n",
    "        era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f'{rgi_id}_ERA5-Land_daily_means.csv')\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['Date'] = pd.to_datetime(era['Date'])\n",
    "    \n",
    "        # Add WOY and year columns\n",
    "        era['WOY'] = era['Date'].dt.isocalendar().week\n",
    "        era['year'] = era['Date'].dt.isocalendar().year\n",
    "    \n",
    "        # Calculate weekly medians for 2013–2022\n",
    "        era = era.loc[era['year'] > 2012]\n",
    "        if '.geo' in era.keys():\n",
    "            era = era.drop(columns=['.geo'])\n",
    "        era_weekly_median = era.groupby('WOY').median().reset_index()\n",
    "    \n",
    "        # Estimate start and end of melt seasons\n",
    "        # Start = positive PDDs\n",
    "        try:\n",
    "            woy_start = era_weekly_median.loc[era_weekly_median['positive_degree_days_annual_cumsum'] > 0, 'WOY'].values[0]\n",
    "        except:\n",
    "            woy_start = 52\n",
    "        # End = after July, 0 PDDs, positive snowfall\n",
    "        woy_end = era_weekly_median.loc[(era_weekly_median['WOY'] > 30) \n",
    "                                     & (era_weekly_median['positive_degree_days'] == 0) \n",
    "                                     & (era_weekly_median['mean_snowfall_sum'] > 0), 'WOY'].values[0]\n",
    "        \n",
    "        # Add to full dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                           'melt_season_start_WOY': [woy_start],\n",
    "                           'melt_season_end_WOY': [woy_end],\n",
    "                          })\n",
    "        melt_season_df = pd.concat([melt_season_df, df], axis=0)\n",
    "    \n",
    "    # Save to file\n",
    "    melt_season_df.reset_index(drop=True, inplace=True)\n",
    "    melt_season_df.to_csv(melt_season_fn, index=False)\n",
    "    print('Melt season timing CSV saved to file:', melt_season_fn)\n",
    "\n",
    "else:\n",
    "    melt_season_df = pd.read_csv(melt_season_fn)\n",
    "    print('Melt season timing CSV loaded from file.')\n",
    "\n",
    "melt_season_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218cb7-b350-4169-9cbb-db194c816c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results\n",
    "\n",
    "# Add subregion and cluster columns\n",
    "if 'Subregion' not in melt_season_df.keys():\n",
    "    melt_season_df['Subregion'] = ''\n",
    "    melt_season_df['clustName'] = ''\n",
    "    for rgi_id in melt_season_df['RGIId'].drop_duplicates().values:\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'Subregion'] = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'clustName'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values\n",
    "\n",
    "nsubregions = len(melt_season_df['Subregion'].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(nsubregions, 1, figsize=(8, nsubregions*4))\n",
    "for i, subregion in enumerate(melt_season_df['Subregion'].drop_duplicates().values):\n",
    "    melt_season_subregion_df = melt_season_df.loc[melt_season_df['Subregion']==subregion]\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_start_WOY'], bins=20, facecolor='m', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_start_WOY'].mean(), color='m', linewidth=2)\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_end_WOY'], bins=20, facecolor='b', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_end_WOY'].mean(), color='b', linewidth=2)\n",
    "    ax[i].set_title(subregion)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18871c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check total PDDs across sites and climate clusters\n",
    "\n",
    "# Iterate over sites\n",
    "average_pdds_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "    # get the climate cluster\n",
    "    clust_name = clusters.loc[clusters['RGIId'] == rgi_id, 'clustName'].values[0]\n",
    "\n",
    "    # Load ERA data\n",
    "    era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f'{rgi_id}_ERA5-Land_daily_means.csv')\n",
    "    era = pd.read_csv(era_fn)\n",
    "    era['Date'] = pd.to_datetime(era['Date'])\n",
    "\n",
    "    # calculate average PDDs for May-August\n",
    "    era['month'] = era['Date'].dt.month\n",
    "    era['year'] = era['Date'].dt.isocalendar().year\n",
    "    era_sept = era.loc[era['month'].isin([9])]\n",
    "    pdd_avg = era_sept['positive_degree_days'].mean()\n",
    "    pdd_cumsum_avg = era_sept['positive_degree_days_annual_cumsum'].mean()\n",
    "\n",
    "    # Add to dataframe\n",
    "    df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                       'clustName': [clust_name], \n",
    "                       'average_PDDs_Sept': [pdd_avg],\n",
    "                       'average_cumulative_PDDs_Sept': [pdd_cumsum_avg]})\n",
    "    average_pdds_df = pd.concat([average_pdds_df, df], axis=0)\n",
    "\n",
    "average_pdds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1412c6-2c07-4d4b-a83d-04934f161dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "# Plot average PDDs by cluster\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.boxplot(data=average_pdds_df, x='clustName', y='average_PDDs_Sept', palette='Set3', ax=ax[0])\n",
    "sns.boxplot(data=average_pdds_df, x='clustName', y='average_cumulative_PDDs_Sept', palette='Set3', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "era_sept['positive_degree_days'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
