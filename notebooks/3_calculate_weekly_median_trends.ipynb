{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fee1d3-89a0-4332-91af-77fbee8cc9ed",
   "metadata": {},
   "source": [
    "# Calculate median weekly trends in snow cover for each study site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded5b9-6a41-4e78-aa7d-dbec024f6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import iqr\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import median_abs_deviation\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d398a-23a0-4431-aee8-5fe622f39b68",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries (AOIs) and climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97356-a7e0-4c83-af84-ceee07b044fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define root path to data for convenience\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "# Import utility functions\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis'\n",
    "sys.path.append(os.path.join(code_path, 'scripts'))\n",
    "import utils as f\n",
    "\n",
    "# Define output directory\n",
    "out_path = os.path.join(scm_path, 'dataset', 'analysis')\n",
    "\n",
    "# Load AOIs\n",
    "aois_fn = os.path.join(scm_path, 'dataset', 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d31fd3",
   "metadata": {},
   "source": [
    "## Pre-processing: Compile classified image and statistics files for each site into a single zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over study sites\n",
    "rgi_ids = aois['RGIId'].unique()\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    print(i+1, rgi_id)\n",
    "    out_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_classifications.zarr\")\n",
    "    if not os.path.exists(out_fn):\n",
    "        aoi = aois.loc[aois['RGIId']==rgi_id].reset_index(drop=True)\n",
    "        f.compile_classified_image_files(scm_path, rgi_id, aoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada9ced-080b-493c-bdaf-8ac48a51bfb1",
   "metadata": {},
   "source": [
    "## Calculate weekly median trends for each site\n",
    "\n",
    "Conduct Monte Carlo simulations for sampling, with number of simulations = 100 and percent sampled = 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23ff0d-5260-47ad-86d0-9a053e716cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scs_MCs_fn = os.path.join(out_path, 'median_snow_cover_stats_MC.nc')\n",
    "if not os.path.exists(scs_MCs_fn):\n",
    "    # Set up Monte Carlo parameters\n",
    "    nMC = 100  # Number of Monte Carlo simulations\n",
    "    sample_fraction = 0.8  # Fraction of data to sample in each simulation\n",
    "\n",
    "    # Initialize list to store results for all sites\n",
    "    scs_MCs_list = []\n",
    "\n",
    "    # Iterate over study sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values[0:1]):\n",
    "        # Load and preprocess data\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_classifications.zarr')\n",
    "        scs_site = f.load_snow_cover_stats(scs_fn)\n",
    "        scs_site = scs_site.assign_coords({'WOY': scs_site['time'].dt.isocalendar().week})\n",
    "\n",
    "        # Number of samples per simulation\n",
    "        nsamp = int(len(scs_site.time.data) * sample_fraction)\n",
    "\n",
    "        # Monte Carlo simulations\n",
    "        results = []\n",
    "        for i in range(nMC):\n",
    "            sampled_times = np.random.choice(scs_site.time.data, size=nsamp, replace=False)\n",
    "            scs_site_MC_list = [scs_site.sel(time=time) for time in sampled_times]\n",
    "            scs_site_MC = xr.concat(scs_site_MC_list, dim='time').sortby('time')\n",
    "\n",
    "            # Calculate weekly medians for AAR, SCA, and ELA\n",
    "            weekly_medians = scs_site_MC.groupby('WOY').median().to_dataframe().reset_index()\n",
    "            weekly_medians['MC_run'] = i\n",
    "            results.append(weekly_medians)\n",
    "\n",
    "        # Concatenate results for all Monte Carlo runs\n",
    "        all_results = pd.concat(results)\n",
    "        all_results['RGIId'] = rgi_id # add RGI ID\n",
    "        \n",
    "        # Convert to xarray.Dataset\n",
    "        ds = all_results.set_index(['RGIId', 'MC_run', 'WOY']).to_xarray()\n",
    "        scs_MCs_list.append(ds)\n",
    "\n",
    "    # Combine results into a single xarray.Dataset\n",
    "    scs_MCs_ds = xr.concat(scs_MCs_list, dim='RGIId')\n",
    "    \n",
    "    # Add data variable attributes\n",
    "    scs_MCs_ds.attrs['title'] = 'Weekly Snow Cover Monte Carlo Simulations'\n",
    "    scs_MCs_ds.attrs['description'] = ('For each study glacier, snow cover observations were grouped by week of year (WOY). '\n",
    "                                      'In each of 100 Monte Carlo iterations (MC_run), 80% of observations and their corresponding '\n",
    "                                      'snow cover statistics were randomly sampled.')\n",
    "    scs_MCs_ds.attrs['institution'] = 'Boise State University'\n",
    "    scs_MCs_ds.attrs['references'] = 'doi:10.1029/2025GL115523'\n",
    "    scs_MCs_ds.attrs['horizontal_CRS'] = 'WGS84 (EPSG:4326)'\n",
    "    scs_MCs_ds.attrs['vertical_CRS'] = 'EGM96 geoid (EPSG:5773)'\n",
    "    scs_MCs_ds.attrs['date_modified'] = '2025-06-07'\n",
    "    scs_MCs_ds.attrs['time_coverage_start'] = '2013-05-01'\n",
    "    scs_MCs_ds.attrs['time_coverage_end'] = '2023-10-31'\n",
    "    scs_MCs_ds['AAR'].attrs['long_name'] = 'accumulation area ratio'\n",
    "    scs_MCs_ds['AAR'].attrs['units'] = 'unitless'\n",
    "    scs_MCs_ds['SLA'].attrs['long_name'] = 'snowline altitude'\n",
    "    scs_MCs_ds['SLA'].attrs['units'] = 'meters above sea level'\n",
    "    scs_MCs_ds['SLA_lower_bound'].attrs['long_name'] = 'snowline altitude lower bound'\n",
    "    scs_MCs_ds['SLA_lower_bound'].attrs['units'] = 'meters above sea level'\n",
    "    scs_MCs_ds['SLA_upper_bound'].attrs['long_name'] = 'snowline altitude upper bound'\n",
    "    scs_MCs_ds['SLA_upper_bound'].attrs['units'] = 'meters above sea level'\n",
    "    for dv in ['snow_area', 'ice_area', 'water_area']:\n",
    "        scs_MCs_ds[dv].attrs['units'] = 'meters squared'\n",
    "        scs_MCs_ds[dv].attrs['long_name'] = (dv).split('_')[0] + ' cover area'\n",
    "    \n",
    "    # Make sure CRS is set\n",
    "    scs_MCs_ds = scs_MCs_ds.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Save to file\n",
    "    scs_MCs_ds.to_netcdf(scs_MCs_fn)\n",
    "    print(\"Monte Carlo simulations completed and saved to file:\", scs_MCs_fn)\n",
    "\n",
    "else:\n",
    "    scs_MCs_ds = xr.open_dataset(scs_MCs_fn)\n",
    "    print('Monte Carlo simulations loaded from file:', scs_MCs_fn)\n",
    "    \n",
    "scs_MCs_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec25ef-248b-4fff-8a5e-b7ac70f2827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Compile minimum snow cover statistics\n",
    "min_aars_woys_fn = os.path.join(out_path, 'minimum_snow_cover_stats.csv') \n",
    "# check if exists in directory\n",
    "if not os.path.exists(min_aars_woys_fn):\n",
    "    # initialize dataframe for RGI stats and minimum snow cover statts\n",
    "    min_aars_woys = pd.DataFrame()\n",
    "    \n",
    "    # iterate over site names in median snow cover stats dataframe\n",
    "    for rgi_id in tqdm(scs_MCs_ds.RGIId.values):\n",
    "        \n",
    "        # Calculate AAR median and MAD across MC simulations\n",
    "        aar_median = float(scs_MCs_ds.sel(RGIId=rgi_id)['AAR'].median(dim='MC_run').min(dim='WOY').values)\n",
    "        aar_mad = float(median_abs_deviation(scs_MCs_ds.sel(RGIId=rgi_id)['AAR'].min(dim='WOY').values))\n",
    "        \n",
    "        # Calculate WOY median and MAD across MC simulations\n",
    "        Imins = scs_MCs_ds.sel(RGIId=rgi_id)['AAR'].argmin(dim='WOY').values\n",
    "        woys = scs_MCs_ds.WOY.values[Imins]\n",
    "        woy_median = int(np.nanmedian(woys))\n",
    "        woy_mad = int(median_abs_deviation(woys))        \n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'RGIId': [rgi_id],\n",
    "            'WOY_median': [woy_median],\n",
    "            'WOY_MAD': [woy_mad],\n",
    "            'AAR_median': [aar_median],\n",
    "            'AAR_MAD': [aar_mad],\n",
    "            })\n",
    "        # concatenate to full dataframe\n",
    "        min_aars_woys = pd.concat([min_aars_woys, df], axis=0)\n",
    "\n",
    "    # save to file\n",
    "    min_aars_woys.to_csv(min_aars_woys_fn, index=False)\n",
    "    print('Minimum AARs and WOYs saved to file: ', min_aars_woys_fn)\n",
    "        \n",
    "else:\n",
    "    # load from file\n",
    "    min_aars_woys = pd.read_csv(min_aars_woys_fn)\n",
    "    print('Minimum AARs and WOYs loaded from file.')\n",
    "\n",
    "min_aars_woys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ce47e-be94-4518-bf30-c319db2e17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add subregion and cluster columns\n",
    "if 'Subregion' not in min_aars_woys.keys():\n",
    "    min_aars_woys = min_aars_woys.merge(aois[['RGIId', 'Subregion']], on='RGIId')\n",
    "if 'clustName' not in min_aars_woys.keys():\n",
    "    min_aars_woys = min_aars_woys.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.boxplot(data=min_aars_woys, x='AAR_median', palette='mako', hue='Subregion', ax=ax[0])\n",
    "sns.boxplot(data=min_aars_woys, x='AAR_median', hue='clustName', ax=ax[1])\n",
    "fig.suptitle('AARs')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.boxplot(data=min_aars_woys, x='WOY_median', palette='mako', hue='Subregion', ax=ax[0])\n",
    "sns.boxplot(data=min_aars_woys, x='WOY_median', hue='clustName', ax=ax[1])\n",
    "fig.suptitle('WOYs')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daeee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Median AAR = ', min_aars_woys['AAR_median'].median())\n",
    "print('Median WOY = ', min_aars_woys['WOY_median'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781acfe-3e6d-46a1-8962-1086332f34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Median AAR MAD = ', min_aars_woys['AAR_MAD'].median())\n",
    "print('Median WOY MAD = ', min_aars_woys['WOY_MAD'].median())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969212e",
   "metadata": {},
   "source": [
    "## Estimate debris cover area at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "debris_areas_fn = os.path.join(out_path, 'debris_cover_areas.csv')\n",
    "if not os.path.exists(debris_areas_fn):\n",
    "    # Initialize dataframe\n",
    "    debris_areas = pd.DataFrame()\n",
    "\n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # get median WOY of snow cover minimum\n",
    "        woy = min_aars_woys.loc[min_aars_woys['RGIId']==rgi_id, 'WOY_median'].values[0]\n",
    "\n",
    "        # load snow cover stats\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats_adjusted2.csv\")\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'])\n",
    "        # add WOY column\n",
    "        scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "\n",
    "        # estimate debris cover area and percentage of total area at WOY\n",
    "        df = scs.groupby('WOY')[['glacier_area_m2', 'debris_area_m2']].mean().reset_index()\n",
    "        df = df.loc[df['WOY']==woy].reset_index(drop=True)\n",
    "        df['RGIId'] = rgi_id\n",
    "\n",
    "        # concatenate to full site\n",
    "        debris_areas = pd.concat([debris_areas, df], axis=0)\n",
    "\n",
    "    # Add percent debris coverage column\n",
    "    debris_areas['debris_percent_area'] = debris_areas['debris_area_m2'] / debris_areas['glacier_area_m2']\n",
    "\n",
    "    # Save to file\n",
    "    debris_areas.reset_index(drop=True, inplace=True)\n",
    "    debris_areas.to_csv(debris_areas_fn, index=False)\n",
    "    print('Debris cover areas saved to file:', debris_areas_fn)\n",
    "\n",
    "else:\n",
    "    debris_areas = pd.read_csv(debris_areas_fn)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(debris_areas['debris_percent_area'], bins=100)\n",
    "plt.show()\n",
    "\n",
    "debris_areas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d51669",
   "metadata": {},
   "outputs": [],
   "source": [
    "debris_areas.loc[debris_areas['debris_percent_area'] > 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2293b96-3a07-4559-9728-4569893eef51",
   "metadata": {},
   "source": [
    "## Assess interannual variability in AAR magnitude and timing at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885ac151-e7a3-419a-b51f-2c19fcc9a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats_fn = os.path.join(out_path, 'minimum_snow_cover_stats_interannual_variability_2016-2023.csv')\n",
    "if os.path.exists(aar_var_stats_fn):\n",
    "    aar_var_stats = pd.read_csv(aar_var_stats_fn)\n",
    "    print('AAR interannual variability stats loaded from file.')\n",
    "\n",
    "else:\n",
    "    aar_var_stats = pd.DataFrame()\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_classifications.zarr')\n",
    "        scs = f.load_snow_cover_stats(scs_fn)\n",
    "        # Add Year and WOY coordinates\n",
    "        scs = scs.assign_coords({'Year': scs['time'].dt.isocalendar().year})\n",
    "        scs = scs.assign_coords({'WOY': scs['time'].dt.isocalendar().week})\n",
    "        # subset to 2016+\n",
    "        scs = scs.sel(time=slice(pd.Timestamp('2016-01-01'), None))\n",
    "        # identify annual AAR magnitudes and WOY timing\n",
    "        def get_min_time(group):\n",
    "            return group['time'].isel(time=group['AAR'].argmin(dim='time'))\n",
    "        min_times = scs.groupby('Year').apply(get_min_time)\n",
    "        annual_mins_site = xr.concat([scs.sel(time=time) for time in min_times.values], dim='time')\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                            'AAR_min': [float(annual_mins_site['AAR'].min().values)],\n",
    "                            'AAR_max': [float(annual_mins_site['AAR'].max().values)],\n",
    "                            'AAR_median': [float(annual_mins_site['AAR'].median().values)],\n",
    "                            'AAR_IQR': [iqr(annual_mins_site['AAR'].values)],\n",
    "                            'WOY_min': [float(annual_mins_site['WOY'].min().values)],\n",
    "                            'WOY_max': [float(annual_mins_site['WOY'].max().values)],\n",
    "                            'WOY_median': [float(annual_mins_site['WOY'].median().values)],\n",
    "                            'WOY_IQR': [iqr(annual_mins_site['WOY'].values)]})  \n",
    "        aar_var_stats = pd.concat([aar_var_stats, df])\n",
    "    \n",
    "    aar_var_stats.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save to file\n",
    "    aar_var_stats.to_csv(aar_var_stats_fn, index=False)\n",
    "    print('AAR interannual variability stats saved to file:', aar_var_stats_fn)\n",
    "\n",
    "aar_var_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a3a37-6f68-4f8d-8a7c-611ed1511b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Print stats\n",
    "aar_var_stats['AAR_range'] = aar_var_stats['AAR_max'] - aar_var_stats['AAR_min']\n",
    "print(f\"AAR range for all sites: {iqr(aar_var_stats['AAR_min'])} - {iqr(aar_var_stats['AAR_max'])}\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['AAR_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1127fa-5146-4fad-98fd-344c66a8e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_var_stats['WOY_range'] = aar_var_stats['WOY_max'] - aar_var_stats['WOY_min']\n",
    "print(f\"AAR TIMING range for all sites: {aar_var_stats['WOY_range'].median()} +/- {iqr(aar_var_stats['WOY_range'])}\\n\")\n",
    "# print('By subregion:')\n",
    "# print('Median')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].median())\n",
    "# print('\\n')\n",
    "# print('IQR')\n",
    "# print(aar_var_stats.groupby(['Subregion'])['WOY_range'].apply(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd1886-3983-4022-afa2-5b8e32d52af7",
   "metadata": {},
   "source": [
    "## Identify the approximate start and end of the melt season in each subregion from ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1684a2-53a8-44e4-8d9f-c3f53fdb483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "melt_season_fn = os.path.join(out_path, 'melt_season_timing.csv')\n",
    "\n",
    "if not os.path.exists(melt_season_fn):\n",
    "    melt_season_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load ERA data\n",
    "        era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f'{rgi_id}_ERA5-Land_daily_means.csv')\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['Date'] = pd.to_datetime(era['Date'])\n",
    "    \n",
    "        # Add WOY and year columns\n",
    "        era['WOY'] = era['Date'].dt.isocalendar().week\n",
    "        era['year'] = era['Date'].dt.isocalendar().year\n",
    "    \n",
    "        # Calculate weekly medians for 2013â€“2022\n",
    "        era = era.loc[era['year'] > 2012]\n",
    "        if '.geo' in era.keys():\n",
    "            era = era.drop(columns=['.geo'])\n",
    "        era_weekly_median = era.groupby('WOY').median().reset_index()\n",
    "    \n",
    "        # Estimate start and end of melt seasons\n",
    "        # Start = positive PDDs\n",
    "        try:\n",
    "            woy_start = era_weekly_median.loc[era_weekly_median['positive_degree_days_annual_cumsum'] > 0, 'WOY'].values[0]\n",
    "        except:\n",
    "            woy_start = 52\n",
    "        # End = after July, 0 PDDs, positive snowfall\n",
    "        woy_end = era_weekly_median.loc[(era_weekly_median['WOY'] > 30) \n",
    "                                     & (era_weekly_median['positive_degree_days'] == 0) \n",
    "                                     & (era_weekly_median['mean_snowfall_sum'] > 0), 'WOY'].values[0]\n",
    "        \n",
    "        # Add to full dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                           'melt_season_start_WOY': [woy_start],\n",
    "                           'melt_season_end_WOY': [woy_end],\n",
    "                          })\n",
    "        melt_season_df = pd.concat([melt_season_df, df], axis=0)\n",
    "    \n",
    "    # Save to file\n",
    "    melt_season_df.reset_index(drop=True, inplace=True)\n",
    "    melt_season_df.to_csv(melt_season_fn, index=False)\n",
    "    print('Melt season timing CSV saved to file:', melt_season_fn)\n",
    "\n",
    "else:\n",
    "    melt_season_df = pd.read_csv(melt_season_fn)\n",
    "    print('Melt season timing CSV loaded from file.')\n",
    "\n",
    "melt_season_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8218cb7-b350-4169-9cbb-db194c816c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some results\n",
    "\n",
    "# Add subregion and cluster columns\n",
    "if 'Subregion' not in melt_season_df.keys():\n",
    "    melt_season_df['Subregion'] = ''\n",
    "    melt_season_df['clustName'] = ''\n",
    "    for rgi_id in melt_season_df['RGIId'].drop_duplicates().values:\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'Subregion'] = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values\n",
    "        melt_season_df.loc[melt_season_df['RGIId']==rgi_id, 'clustName'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values\n",
    "\n",
    "nsubregions = len(melt_season_df['Subregion'].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(nsubregions, 1, figsize=(8, nsubregions*4))\n",
    "for i, subregion in enumerate(melt_season_df['Subregion'].drop_duplicates().values):\n",
    "    melt_season_subregion_df = melt_season_df.loc[melt_season_df['Subregion']==subregion]\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_start_WOY'], bins=20, facecolor='m', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_start_WOY'].mean(), color='m', linewidth=2)\n",
    "    ax[i].hist(melt_season_subregion_df['melt_season_end_WOY'], bins=20, facecolor='b', alpha=0.5)\n",
    "    ax[i].axvline(melt_season_subregion_df['melt_season_end_WOY'].mean(), color='b', linewidth=2)\n",
    "    ax[i].set_title(subregion)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90e309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca",
   "language": "python",
   "name": "gsca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
