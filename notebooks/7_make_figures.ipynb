{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a268cb2-8765-4785-a416-f29c9289f371",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886bd1-2d50-401f-aaef-8353f32db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import joblib\n",
    "from scipy.stats import median_abs_deviation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e6df1-03f5-472a-8c1c-222f8f92ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to this repository and import functions\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis/'\n",
    "sys.path.append(os.path.join(code_path, 'scripts'))\n",
    "import utils as f\n",
    "\n",
    "# Define input and output paths\n",
    "figures_out_path = os.path.join(code_path, 'figures')\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "\n",
    "# Define font and font size\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "\n",
    "# Define clusters and AOIs file names\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "aois_fn = os.path.join(out_path, 'AOIs.gpkg')\n",
    "\n",
    "# Define function for saving figure\n",
    "def save_figure(fig, fig_fn):\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7489240-51f6-4b35-8c16-4927ccc6c615",
   "metadata": {},
   "source": [
    "## Define colormaps and order of clusters and subregions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c52145-2f0b-4149-8645-7e855d2118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate clusters\n",
    "cluster_cmap_dict = {'W. Aleutians': '#dd3497', \n",
    "                     'Maritime': '#018571',\n",
    "                     'Transitional-Maritime': '#80cdc1',\n",
    "                     'Transitional-Continental': '#dfc27d',\n",
    "                     'Continental': '#a6611a',\n",
    "                     }\n",
    "\n",
    "cluster_order = ['W. Aleutians', 'Maritime', 'Transitional-Maritime', 'Transitional-Continental', 'Continental']\n",
    "subregion_order = ['N. Rockies', 'Alaska Range', 'W. Chugach Mtns.', 'St. Elias Mtns.', 'N. Coast Ranges',\n",
    "                   'Aleutians', 'N. Cascades', 'C. Rockies', 'S. Cascades', 'S. Rockies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc505172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of observations\n",
    "rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "nobs = np.zeros(len(rgi_ids))\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    im_classified_fns = (glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'classified', '20*.nc'))\n",
    "                         + glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'classified', '20*.nc')))\n",
    "    if len(im_classified_fns) < 1:\n",
    "        print(rgi_id)\n",
    "    nobs[i] = len(im_classified_fns)\n",
    "\n",
    "print(f'Total number of observations = {np.sum(nobs)}')\n",
    "print(f'Mean observations per site = {np.mean(nobs)}')\n",
    "print(f'Median observations per site = {np.median(nobs)}')\n",
    "print(f'Minimum observations per site = {np.min(nobs)}')\n",
    "print(f'Maximum observations per site = {np.max(nobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bed560",
   "metadata": {},
   "source": [
    "## Figure 1. Study sites and climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f99b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load climate clusters / mean climate\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Climate clusters loaded')\n",
    "\n",
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add climate clusters column\n",
    "aois = pd.merge(aois, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75365c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "plot_inset = True # whether to plot mean climate conditions inset\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "if plot_inset:\n",
    "    ax = [fig.add_subplot(1,1,1),\n",
    "        fig.add_axes([0.82, 0.58, 0.07, 0.07]),\n",
    "        fig.add_axes([0.26, 0.24, 0.27, 0.23])]\n",
    "    legend_position = [0.7, 0.46, 0.2, 0.2]\n",
    "else:\n",
    "    ax = [fig.add_subplot(1,1,1),\n",
    "          fig.add_axes([0.41, 0.31, 0.08, 0.08]),\n",
    "          None]\n",
    "    legend_position = [0.3, 0.2, 0.2, 0.2]    \n",
    "\n",
    "### a) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 66)\n",
    "ax[0].set_xlabel('Longitude ($^{\\circ}$E)')\n",
    "ax[0].set_ylabel('Latitude ($^{\\circ}$N)')\n",
    "ax[0].set_aspect(2.2)\n",
    "# Site locations\n",
    "sns.scatterplot(data=aois, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, s=20,\n",
    "                hue='clustName', palette=cluster_cmap_dict, hue_order=cluster_order, alpha=1, ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().remove()\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "background_color = [1, 1, 1, 0.5]\n",
    "ax[0].text(-160.5, 56.5, f\"Aleutians\\n(N={len(aois.loc[aois['Subregion']=='Aleutians'])})\", ha='center', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-157, 62.5, f\"Alaska Range\\n(N={len(aois.loc[aois['Subregion']=='Alaska Range'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-147.7, 57.4, f\"W. Chugach \\nMtns.\\n(N={len(aois.loc[aois['Subregion']=='W. Chugach Mtns.'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[0].text(-141.5, 57.4, f\"St. Elias \\nMtns.\\n(N={len(aois.loc[aois['Subregion']=='St. Elias Mtns.'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[0].text(-139.8, 55.5, f\"N. Coast \\nRanges\\n(N={len(aois.loc[aois['Subregion']=='N. Coast Ranges'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-137.1, 56.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-133.4, 50.9, f\"N. Cascades\\n(N={len(aois.loc[aois['Subregion']=='N. Cascades'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-129.9, 51.4, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-129.7, 47, f\"S. Cascades\\n(N={len(aois.loc[aois['Subregion']=='S. Cascades'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-126.3, 47.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-129.5, 64.5, f\"N. Rockies\\n(N={len(aois.loc[aois['Subregion']=='N. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-119, 54.5, f\"C. Rockies\\n(N={len(aois.loc[aois['Subregion']=='C. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-115, 47, f\"S. Rockies\\n(N={len(aois.loc[aois['Subregion']=='S. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "# stars for the USGS Benchmark Glaciers\n",
    "bbox_dict = dict(boxstyle='round,pad=0.3', linewidth=0.2, edgecolor='k', facecolor='w', alpha=0.5)\n",
    "arrow_dict = dict(arrowstyle='->', color='k', connectionstyle=\"arc3,rad=0\")\n",
    "for name in ['Gulkana Glacier', 'Wolverine Glacier', 'Lemon Creek Glacier', 'Sperry Glacier MT', 'South Cascade Glacier WA']:\n",
    "    aoi = aois.loc[aois['Name']==name]\n",
    "    centroid = (aoi['CenLon'].values[0], aoi['CenLat'].values[0])\n",
    "    ax[0].plot(*centroid, '*', markerfacecolor=cluster_cmap_dict[aoi['clustName'].values[0]], markeredgecolor='k', markeredgewidth=1, markersize=10)\n",
    "    if ('Gulkana' in name) | ('Wolverine' in name) | ('Sperry' in name):\n",
    "        x1, y1, x2, y2, ha = centroid[0], centroid[1]+1.5, centroid[0], centroid[1]+0.1, 'center'\n",
    "    elif 'Lemon' in name:\n",
    "        x1, y1, x2, y2, ha = centroid[0]+1.5, centroid[1], centroid[0]+0.1, centroid[1], 'left'\n",
    "    elif 'South' in name:\n",
    "        x1, y1, x2, y2, ha = centroid[0]+2, centroid[1]+0.3, centroid[0]+0.1, centroid[1], 'left'\n",
    "    ax[0].text(x1, y1, name.replace(' MT','').replace(' WA','').replace(' ','\\n'), color='k', fontsize=fontsize-5, ha=ha, bbox=bbox_dict)\n",
    "    ax[0].annotate(\"\", xy=(x2, y2), xytext=(x1, y1), arrowprops=arrow_dict)\n",
    "\n",
    "\n",
    "# Legend\n",
    "legend = fig.legend(handles, labels, loc='upper right', title='Climate cluster', bbox_to_anchor=legend_position, \n",
    "                    fontsize=fontsize-1, markerscale=2, alignment='left', labelspacing=0.6, framealpha=1)\n",
    "\n",
    "# pie chart inset\n",
    "pie_labels = cluster_order\n",
    "pie_sizes = [len(clusters.loc[clusters['clustName']==label]) for label in pie_labels]\n",
    "ax[1].pie(pie_sizes, colors=list(cluster_cmap_dict.values()), autopct=lambda p: f'{p * sum(pie_sizes) / 100:.0f}', \n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'k'}, textprops={'fontsize': 7, 'fontweight': 'bold', 'color': 'k'},\n",
    "        pctdistance=0.8)\n",
    "ax[1].set_zorder(legend.get_zorder() + 1)\n",
    "\n",
    "### b) Mean weather conditions\n",
    "if plot_inset:\n",
    "    sns.scatterplot(data=clusters, x='mean_annual_temp_range', y='mean_annual_precip_cumsum', s=20,\n",
    "                    edgecolor='k', linewidth=0.5, hue='clustName', palette=cluster_cmap_dict, alpha=1, \n",
    "                    hue_order=cluster_order, legend=False, ax=ax[2])\n",
    "    ax[2].set_xlabel('Air temperature range [$^{\\circ}$C]', fontsize=fontsize-2)\n",
    "    ax[2].set_ylabel('Precipitation sum [m]', fontsize=fontsize-2)\n",
    "    ax[2].tick_params(labelsize=fontsize-2)\n",
    "\n",
    "# Add text labels\n",
    "if plot_inset:\n",
    "    text_labels = ['a', 'b']\n",
    "    for i, axis in enumerate([ax[0], ax[2]]):\n",
    "        if i==1:\n",
    "            scale = 0.85\n",
    "        else:\n",
    "            scale = 0.93\n",
    "        axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*scale + axis.get_xlim()[0],\n",
    "                (axis.get_ylim()[1] - axis.get_ylim()[0])*scale + axis.get_ylim()[0],\n",
    "                text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig01_study_sites_clusters.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6f855-6b1b-46cc-b4a5-e8e2c27763ae",
   "metadata": {},
   "source": [
    "## Figure 2. Median AARs and timing comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df22e2b-3bf1-4602-ace4-d954143b840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('Glacier boundaries loaded')\n",
    "\n",
    "# -----Load climate clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_scs_fn = os.path.join(out_path, 'minimum_snow_cover_stats.csv')\n",
    "min_scs = pd.read_csv(min_scs_fn)\n",
    "# Add difference from September AAR\n",
    "# min_scs['AAR_P50_diff'] = min_scs['AAR_P50_WOY39'] - min_scs['AAR_P50_min']\n",
    "# Add Subregion and climate cluster info\n",
    "min_scs[['CenLon', 'CenLat', 'Subregion', 'clustName']] = 0, 0, '', ''\n",
    "for rgi_id in min_scs['RGIId'].drop_duplicates().values:\n",
    "    cenlon, cenlat, subregion = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']].values[0]\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    min_scs.loc[min_scs['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion', 'clustName']] = cenlon, cenlat, subregion, cluster\n",
    "# Sort by subregion order\n",
    "min_scs['Subregion'] = pd.Categorical(min_scs['Subregion'], subregion_order)\n",
    "min_scs.sort_values(by='Subregion', inplace=True)\n",
    "print('Median AARs loaded')\n",
    "\n",
    "# -----Load melt season timings estimate\n",
    "melt_season_fn = os.path.join(out_path, 'melt_season_timing.csv')\n",
    "melt_season = pd.read_csv(melt_season_fn)\n",
    "# Add subregion column\n",
    "melt_season = pd.merge(melt_season, aois[['RGIId', 'Subregion']], on='RGIId')\n",
    "# Sort by subregion order\n",
    "melt_season['Subregion'] = pd.Categorical(melt_season['Subregion'], subregion_order)\n",
    "melt_season.sort_values(by='Subregion', inplace=True)\n",
    "print('Melt season timing loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f7edb-9d20-4c6f-8687-2c135b4ed769",
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 1.0\n",
    "gs = matplotlib.gridspec.GridSpec(10,2, wspace=0.0, hspace=0.0)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = []\n",
    "\n",
    "# Iterate over subregions\n",
    "median_color = 'w'\n",
    "fill_color = '#4f97d6' #'#2171b5'\n",
    "Iax = -1\n",
    "for i, subregion in enumerate(min_scs['Subregion'].unique()):\n",
    "    min_scs_subregion = min_scs.loc[min_scs['Subregion']==subregion]\n",
    "    \n",
    "    # a) AARs\n",
    "    ax.append(fig.add_subplot(gs[i,0]))\n",
    "    Iax += 1\n",
    "    min_scs_subregion_melt = pd.melt(\n",
    "            min_scs_subregion,\n",
    "            id_vars=['RGIId', 'Subregion'],\n",
    "            value_vars=['AAR_median'],\n",
    "            var_name='AAR_type',\n",
    "            value_name='blank'\n",
    "    )\n",
    "    min_scs_subregion_melt.rename(columns={'blank': 'AAR'}, inplace=True)\n",
    "    sns.violinplot(data=min_scs_subregion, x='AAR_median', color=fill_color, width=0.8, \n",
    "                   inner='quart', ax=ax[Iax], linecolor='k', cut=0, legend=False)\n",
    "    ax[Iax].lines[1].set_color('w')\n",
    "    ax[Iax].lines[1].set_linestyle('-')\n",
    "    ax[Iax].set_xlim(0,1)\n",
    "    ax[Iax].spines[['right', 'top']].set_visible(False)\n",
    "    ax[Iax].set_ylabel('')\n",
    "    if i > 0:\n",
    "        ax[Iax].spines['top'].set_visible(True)\n",
    "        ax[Iax].spines['top'].set_color('gray')\n",
    "    if i==0:\n",
    "        ax[Iax].text(0.8, 0.7, 'a', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+4, color='k')\n",
    "    if i < 9:\n",
    "        ax[Iax].set_xticks([])\n",
    "        ax[Iax].set_xlabel('')\n",
    "        ax[Iax].spines['bottom'].set_color('gray')\n",
    "    else:\n",
    "        ax[Iax].set_xlabel('Accumulation area ratio')\n",
    "    ax[Iax].set_ylabel(subregion, ha='right', va='center', color='k', rotation=0)\n",
    "\n",
    "    # b) AAR timing and melt season duration\n",
    "    ax.append(fig.add_subplot(gs[i,1]))\n",
    "    Iax += 1\n",
    "    k = sns.kdeplot(min_scs_subregion['WOY_median'], vertical=False, color=fill_color, \n",
    "                    fill=True, edgecolor='k', linewidth=lw, alpha=1, ax=ax[Iax], zorder=2)\n",
    "    median = min_scs_subregion['WOY_median'].median()\n",
    "    ax[Iax].plot([median, median], [0, ax[Iax].get_ylim()[1]*0.9], '-', color='w', linewidth=lw+0.5, zorder=3)\n",
    "    ax[Iax].plot([14,45], [0,0], '-', color='k', linewidth=2)\n",
    "    melt_season_subregion = melt_season.loc[melt_season['Subregion']==subregion]\n",
    "    melt_season_start = melt_season_subregion['melt_season_start_WOY'].mean()\n",
    "    melt_season_end = melt_season_subregion['melt_season_end_WOY'].mean()\n",
    "    ax[Iax].fill_between([melt_season_start, melt_season_end],  \n",
    "                         [0, 0], [ax[Iax].get_ylim()[1], ax[Iax].get_ylim()[1]],\n",
    "                         facecolor='k', edgecolor='None', alpha=0.15, zorder=1, label='Melt season duration')\n",
    "    ax[Iax].set_xlim(13,45)\n",
    "    ax[Iax].set_yticks([])\n",
    "    ax[Iax].set_ylabel('')\n",
    "    ax[Iax].set_xticks([])\n",
    "    ax[Iax].set_xlabel('')\n",
    "    if i==9:\n",
    "        ax[Iax].set_xticks([18, 22, 26, 31, 35, 39, 44])\n",
    "        ax[Iax].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov'])\n",
    "        ax[Iax].set_xlabel('Snow minimum timing')\n",
    "    if i==0:\n",
    "        ax[Iax].legend(loc='upper center', frameon=False, bbox_to_anchor=[0.4, 1.6, 0.2, 0.2])\n",
    "        ax[Iax].text(0.8, 0.7, 'b', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+4, color='k')\n",
    "    ax[Iax].spines[['left', 'right', 'top', 'bottom']].set_visible(False)\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig02_median_aars+timings.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AAR as a function of terrain characteristics\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial', 'font.size': 12})\n",
    "subregions_cmap_dict = {'N. Rockies': '#b15928',\n",
    " 'Alaska Range': '#6a3d9a',\n",
    " 'W. Chugach Mtns.': '#cab2d6',\n",
    " 'St. Elias Mtns.': '#33a02c',\n",
    " 'N. Coast Ranges': '#b2df8a',\n",
    " 'Aleutians': '#fb9a99',\n",
    " 'N. Cascades': '#1f78b4',\n",
    " 'C. Rockies': '#ff7f00',\n",
    " 'S. Cascades': '#a6cee3',\n",
    " 'S. Rockies': '#fdbf6f'}\n",
    "\n",
    "if 'Area' not in min_scs.keys():\n",
    "    min_scs = pd.merge(min_scs, aois[['RGIId', 'Area', 'Zmin', 'Zmax', 'Aspect', 'Slope']], on='RGIId')\n",
    "    min_scs['Zrange'] = min_scs['Zmax'] - min_scs['Zmin']\n",
    "cols = ['Area', 'Zrange', 'Slope', 'Aspect']\n",
    "labels = ['Area [km$^2$]', 'Elevation range [m]', 'Slope [degrees]', 'Aspect']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "ax = ax.flatten()\n",
    "for i, (col, label) in enumerate(zip(cols, labels)):\n",
    "    # distribution\n",
    "    sns.scatterplot(min_scs, x=col, y='AAR_median', hue='Subregion', hue_order=subregion_order, palette=subregions_cmap_dict, ax=ax[i])\n",
    "    # add legend on first iteration\n",
    "    if i==0:\n",
    "        handles, labels = ax[i].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, loc='center right', bbox_to_anchor=[1.05, 0.4, 0.2, 0.2])\n",
    "        ax[i].set_xscale('log')\n",
    "    ax[i].legend().remove()\n",
    "    if i==len(cols)-1:\n",
    "        ax[i].set_xticks([0, 90, 180, 270])\n",
    "        ax[i].set_xticklabels(['N', 'E', 'S', 'W'])\n",
    "    # linear fit\n",
    "    fit = LinearRegression().fit(min_scs[col].values.reshape(-1,1), min_scs['AAR_median'])\n",
    "    x = np.arange(min_scs[col].min(), min_scs[col].max()).reshape(-1,1)\n",
    "    yfit = fit.predict(x)\n",
    "    r2 = fit.score(min_scs[col].values.reshape(-1,1), min_scs['AAR_median'])\n",
    "    ax[i].plot(x, yfit, '-k')\n",
    "    if i==0:\n",
    "        ax[i].text(100, 0.94, f\"R$^2$ = {np.round(r2,3)}\", ha='right')\n",
    "    else:\n",
    "        ax[i].text(np.nanmean(x), 0.94, f\"R$^2$ = {np.round(r2,3)}\", ha='center')\n",
    "    ax[i].set_ylim(-0.05, 1.0)\n",
    "    ax[i].set_xlabel(label)\n",
    "    ax[i].set_ylabel('AAR')\n",
    "    ax[i].grid()\n",
    "    fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'AAR_vs_area_elevation.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5d7b5-d596-4908-bf9b-2821e4b1dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WOY and months\n",
    "df = pd.DataFrame({'Date': pd.date_range('2013-01-01', '2023-12-30')})\n",
    "df['WOY'] = df['Date'].dt.isocalendar().week\n",
    "df['year'] = df['Date'].dt.year\n",
    "df['month'] = df['Date'].dt.month\n",
    "df['day'] = df['Date'].dt.day\n",
    "# df.loc[(df['month']==10) & (df['day']==1)]['WOY']#.median()\n",
    "df.loc[df['WOY']==38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bf271-88ea-4056-9048-04652a4b339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "\n",
    "print('All sites:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs['AAR_median'].describe())\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs['WOY_median'].describe()) \n",
    "\n",
    "print(' ')\n",
    "print('By subregion:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs.groupby('Subregion')['AAR_median'].describe().sort_values(by='50%'))\n",
    "\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs.groupby('Subregion')['WOY_median'].describe().sort_values(by='50%'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76ef00-65c4-465e-84f1-2bcf3ce753bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at which sites had AARs < 0.1\n",
    "aar_lt_p1 = min_scs.loc[min_scs['AAR_median'] < 0.1]\n",
    "print(len(aar_lt_p1))\n",
    "aar_lt_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scs_MCs_fn = os.path.join(out_path, 'median_snow_cover_stats_MC.nc')\n",
    "scs_MCs = xr.open_dataset(scs_MCs_fn)\n",
    "\n",
    "# AARs\n",
    "min_aar = scs_MCs['AAR'].min(dim='WOY')\n",
    "min_aar_ranges = [np.nanmax(min_aar.isel(RGIId=i).values) - np.nanmin(min_aar.isel(RGIId=i).values) for i in np.arange(len(min_aar.RGIId))]\n",
    "min_aar_mads = [median_abs_deviation(min_aar.isel(RGIId=i).values) for i in np.arange(len(min_aar.RGIId))]\n",
    "\n",
    "# Print results\n",
    "print(\"AAR:\")\n",
    "print(f\"Minimum range in AARs across all sites and simulations:\", np.nanmin(min_aar_ranges))\n",
    "print(f\"Maximum range in AARs across all sites and simulations:\", np.nanmax(min_aar_ranges))\n",
    "print(f\"Minimum MAD in AARs across simulations and sites:\", np.nanmin(min_aar_mads))\n",
    "print(f\"Maximum MAD in AARs across simulations and sites:\", np.nanmax(min_aar_mads))\n",
    "print(f\"Mean MAD in AARs across simulations and sites:\", np.nanmean(min_aar_mads))\n",
    "print('Mean range in AARs across sites and simulations:', np.nanmean(min_aar_ranges))\n",
    "\n",
    "# Snow minima timings\n",
    "min_aar_woy = scs_MCs['WOY'].isel(WOY=scs_MCs['AAR'].argmin(dim='WOY'))\n",
    "min_aar_woy_ranges = [np.nanmax(min_aar_woy.isel(RGIId=i).values) - np.nanmin(min_aar_woy.isel(RGIId=i).values) for i in np.arange(len(min_aar_woy.RGIId))]\n",
    "min_aar_woy_mads = [median_abs_deviation(min_aar_woy.isel(RGIId=i).values) for i in np.arange(len(min_aar_woy.RGIId))]\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSnow minima timings:\")\n",
    "print(f\"Minimum range in WOYs across all sites and simulations:\", np.nanmin(min_aar_woy_ranges))\n",
    "print(f\"Maximum range in WOYs across all sites and simulations:\", np.nanmax(min_aar_woy_ranges))\n",
    "\n",
    "print(f\"Minimum MAD in WOYs across simulations and sites:\", np.nanmin(min_aar_woy_mads))\n",
    "print(f\"Maximum MAD in WOYs across simulations and sites:\", np.nanmax(min_aar_woy_mads))\n",
    "print(f\"Mean MAD in WOYs across simulations and sites:\", np.nanmean(min_aar_woy_mads))\n",
    "print('Mean range in WOYs across sites and simulations:', np.nanmean(min_aar_woy_ranges))\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nSnow minima timings:\")\n",
    "# print(f\"Mean WOY range across sites: {mean_range_woy.values}\")\n",
    "# print(f\"Mean WOY IQR across sites: {mean_iqr_woy.values}\")\n",
    "# print(f\"Mean WOY standard deviation across sites: {mean_std_dev_woy.values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ad611-f047-4fae-95c5-865070735b4d",
   "metadata": {},
   "source": [
    "## Figure 3. Observed vs. modeled SMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9f0ca-49f0-4d0f-b177-9a01e0e34ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Compiled AOIs loaded')\n",
    "\n",
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded')\n",
    "\n",
    "# Load merged monthly SLAs, ELAs, \n",
    "slas_elas_merged_fn = os.path.join(out_path, 'monthly_SLAs_annual_ELAs_observed_modeled.nc')\n",
    "slas_elas_merged = xr.load_dataset(slas_elas_merged_fn)\n",
    "# Convert to pandas dataframes for compatibility with seaborn\n",
    "slas = slas_elas_merged[['SLA_obs', 'SLA_mod', 'SMB_at_SLA_obs']].to_dataframe().reset_index()\n",
    "slas['Month'] = slas['time'].dt.month\n",
    "elas = slas_elas_merged[['ELA_obs', 'ELA_mod']].to_dataframe().reset_index()\n",
    "print('SLAs and ELAs loaded')\n",
    "\n",
    "# Load PyGEM comparisons\n",
    "pygem_params_fn = os.path.join(out_path, 'PyGEM_comparison_params.csv')\n",
    "pygem_params = pd.read_csv(pygem_params_fn)\n",
    "print('PyGEM parameter comparisons loaded')\n",
    "\n",
    "# Add subregion, centroid coordinates, and climate cluster columns\n",
    "def merge_sort_df(df):\n",
    "    df = df.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "    df = df.merge(aois[['RGIId', 'CenLon', 'CenLat', 'Subregion']], on='RGIId')\n",
    "    df['clustName'] = pd.Categorical(df['clustName'], categories=cluster_order, ordered=True)\n",
    "    df.sort_values(by=['clustName', 'RGIId'], inplace=True)\n",
    "    return df\n",
    "slas = merge_sort_df(slas)\n",
    "elas = merge_sort_df(elas)\n",
    "pygem_params = merge_sort_df(pygem_params)\n",
    "print('Added columns for plotting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b035d-5552-4cf2-aa56-a8cefb9b37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,8), gridspec_kw=dict(width_ratios=[1.5,1]))\n",
    "ax = ax.flatten()\n",
    "\n",
    "### a) Monthly snowline differences\n",
    "slas['SLA_mod-obs'] = slas['SLA_mod'] - slas['SLA_obs']\n",
    "sns.boxplot(data=slas, x='Month', y='SLA_mod-obs', hue='clustName', hue_order=cluster_order,\n",
    "            palette=cluster_cmap_dict, saturation=1, showfliers=False,\n",
    "            boxprops=dict(linewidth=1, edgecolor='k'), whiskerprops=dict(linewidth=1, color='k'), \n",
    "            ax=ax[0])\n",
    "ax[0].set_xticks(np.arange(0,5))\n",
    "ax[0].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "ax[0].set_xlim(-0.5, 4.5)\n",
    "ax[0].set_ylim(-550, 1300)\n",
    "ax[0].set_title('a) Modeled $-$ observed snowline altitudes')\n",
    "ax[0].set_ylabel('Snowline altitude difference [m]')\n",
    "# add minor grid lines\n",
    "ax[0].xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.5))\n",
    "ax[0].grid(which='minor')\n",
    "# remove minor ticks\n",
    "ax[0].tick_params(axis='x', which='minor', bottom=False)\n",
    "\n",
    "### b) ELA differences\n",
    "elas['ELA_mod-obs'] = elas['ELA_mod'] - elas['ELA_obs']\n",
    "sns.kdeplot(data=elas, y='ELA_mod-obs', hue='clustName', hue_order=cluster_order, fill=True, alpha=0.1,\n",
    "            linewidth=2, palette=cluster_cmap_dict, ax=ax[1])\n",
    "ax[1].set_ylim(ax[0].get_ylim())\n",
    "ax[1].set_title('b) Modeled $-$ observed ELAs')\n",
    "ax[1].set_xlabel('Relative density')\n",
    "ax[1].set_ylabel('ELA difference [m]')\n",
    "ax[1].set_xticks([])\n",
    "\n",
    "### c) Modeled SMB at remotely-sensed snowlines\n",
    "sns.boxplot(data=slas, x='Month', y='SMB_at_SLA_obs', hue='clustName', hue_order=cluster_order,\n",
    "            palette=cluster_cmap_dict, saturation=1, showfliers=False, legend=False,\n",
    "            boxprops=dict(linewidth=1, edgecolor='k'), whiskerprops=dict(linewidth=1, color='k'), \n",
    "            ax=ax[2])\n",
    "ax[2].set_xlabel('Month')\n",
    "ax[2].set_xticks(np.arange(0,5))\n",
    "ax[2].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "ax[2].set_xlim(-0.5, 4.5)\n",
    "ax[2].set_ylabel('SMB [m.w.e.]')\n",
    "ax[2].set_title('c) Modeled SMB at observed snowline altitude')\n",
    "# add minor grid lines\n",
    "ax[2].xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.5))\n",
    "ax[2].grid(which='minor')\n",
    "# remove minor ticks\n",
    "ax[2].tick_params(axis='x', which='minor', bottom=False)\n",
    "\n",
    "### d) Original vs. adjusted PyGEM parameters\n",
    "def add_arrow(axis, x1, y1, x2, y2, arrowstyle='-|>', color='black'):\n",
    "    axis.annotate(\"\", xy=(x2, y2), xytext=(x1, y1),\n",
    "                  arrowprops=dict(arrowstyle=arrowstyle, color=color,\n",
    "                                  connectionstyle=\"arc3,rad=0\"))\n",
    "\n",
    "for rgi_id in pygem_params['RGIId'].drop_duplicates().values:\n",
    "    pygem_site = pygem_params.loc[pygem_params['RGIId']==rgi_id]\n",
    "    color = cluster_cmap_dict[pygem_site['clustName'].values[0]]\n",
    "    # line w/ arrow between points\n",
    "    add_arrow(ax[3], \n",
    "              pygem_site.loc[pygem_site['parameter']=='tbias', 'Original'].values[0],\n",
    "              pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Original'].values[0] * 1e3,\n",
    "              pygem_site.loc[pygem_site['parameter']=='tbias', 'Best'].values[0],\n",
    "              pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Best'].values[0] * 1e3,\n",
    "              color=color)\n",
    "    # original points\n",
    "    ax[3].plot(pygem_site.loc[pygem_site['parameter']=='tbias', 'Original'].values[0],\n",
    "               pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Original'].values[0] * 1e3, \n",
    "               'o', color=color,\n",
    "               markersize=pygem_site.loc[pygem_site['parameter']=='kp', 'Original'].values[0]*3)\n",
    "    # best points\n",
    "    ax[3].plot(pygem_site.loc[pygem_site['parameter']=='tbias', 'Best'].values[0],\n",
    "               pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Best'].values[0] * 1e3, \n",
    "               'o', color=color,\n",
    "               markersize=pygem_site.loc[pygem_site['parameter']=='kp', 'Best'].values[0]*3)\n",
    "    # Name annotation\n",
    "    if (pygem_site['Name'].values[0]=='South Cascade'):\n",
    "        ax[3].annotate(pygem_site['Name'].values[0].replace(' ','\\n'), \n",
    "                    xy=(pygem_site.loc[pygem_site['parameter']=='tbias', 'Best'].values[0] - 0.1, \n",
    "                        pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Best'].values[0] * 1e3 - 0.35),\n",
    "                        color='k', ha='left', fontsize=10)\n",
    "    else:\n",
    "        ax[3].annotate(pygem_site['Name'].values[0].replace(' ','\\n'), \n",
    "                    xy=(pygem_site.loc[pygem_site['parameter']=='tbias', 'Best'].values[0] + 0.2, \n",
    "                        pygem_site.loc[pygem_site['parameter']=='ddfsnow', 'Best'].values[0] * 1e3 - 0.1),\n",
    "                        color='k', ha='left', fontsize=10)\n",
    "    # dummy points for legend\n",
    "    if pygem_site['Name'].values[0]=='Gulkana':\n",
    "        for kp in [1, 2, 3]:\n",
    "            ax[3].plot(-100, -100, 'ok', markersize=kp*3, label=str(kp))\n",
    "    \n",
    "ax[3].set_xlim(-4,4.6)\n",
    "ax[3].set_xlabel('Temperature bias [$^{\\circ}$C]')\n",
    "ax[3].set_ylim(1.3, 4.7)\n",
    "ax[3].set_ylabel('Degree-day factor of snow [m $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "ax[3].set_title('d) Original vs. adjusted PyGEM parameters')\n",
    "ax[3].legend(loc='lower left', ncol=3, handletextpad=0.2, title='Precipitation factor')\n",
    "\n",
    "# Add legend\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().remove()\n",
    "ax[1].legend().remove()\n",
    "fig.legend(handles, labels, loc='lower center', ncols=5, frameon=False,\n",
    "           bbox_to_anchor=[0.43, -0.05, 0.2, 0.2], labelspacing=0.6, handletextpad=0.3)\n",
    "\n",
    "# Add text label and line at 0\n",
    "text_labels = ['a', 'b']\n",
    "for i, axis in enumerate(ax[0:3]):\n",
    "    axis.axhline(0, color='k')\n",
    "    \n",
    "# fig.subplots_adjust(wspace=0.2)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'fig03_modeled_observed_SMB_differences.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858b44f-472d-492e-87bc-ff3d04df7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA stats\n",
    "print(slas.groupby(['Month'])['SLA_mod-obs'].describe())\n",
    "slas.groupby(['clustName', 'Month'])['SLA_mod-obs'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELA stats\n",
    "print(elas['ELA_mod-obs'].describe())\n",
    "elas.groupby('clustName')['ELA_mod-obs'].describe().sort_values(by='50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyGEM simulations\n",
    "pygem_params.loc[pygem_params['parameter']=='ddfsnow']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d16aaf-de80-4b97-af97-29651c5f68b3",
   "metadata": {},
   "source": [
    "## Figure S1. Sites distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba3fc8-3053-4e30-a488-99df69cce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analyzed glacier boundaries\n",
    "aois = gpd.read_file(aois_fn)\n",
    "cols = ['O1Region', 'O2Region', 'Zmed', 'Aspect', 'Slope', 'Area']\n",
    "for col in cols:\n",
    "    aois[col] = aois[col].astype(float)\n",
    "    \n",
    "# Load all glacier boundaries in O1 regions 1 and 2\n",
    "rgi_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/RGI/'\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp',\n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "rgi = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    file = gpd.read_file(os.path.join(rgi_path, rgi_fn))\n",
    "    rgi = pd.concat([rgi, file])\n",
    "rgi[['O1Region', 'O2Region']] = rgi[['O1Region', 'O2Region']].astype(int)\n",
    "# Add column for subregion name\n",
    "rgi = pd.merge(rgi, aois[['O1Region', 'O2Region', 'Subregion']], on=['O1Region', 'O2Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d356a8-0d7c-48cd-bc3a-fea96869aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting variables\n",
    "columns = ['Zmed', 'Aspect', 'Slope', 'Area']\n",
    "xlabels = ['Median elevation [m]', 'Aspect [degrees]', 'Slope [degrees]', 'Area [km$^2$]']\n",
    "bins_list = [np.linspace(0, 361, num=20), # Aspect\n",
    "             np.linspace(0, 51, num=20), # Slope\n",
    "             np.linspace(0, 300, num=50)] # Area\n",
    "\n",
    "# Set up figure\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial', 'font.size': 12})\n",
    "fig, ax = plt.subplots(len(subregion_order)+1, 4, figsize=(12, (len(subregion_order)+1)*1.5))\n",
    "aois_color = '#b35806'\n",
    "\n",
    "# All subregions\n",
    "for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "    if column=='Zmed': \n",
    "        bins = np.linspace(rgi['Zmed'].min(), rgi['Zmed'].max(), num=20)\n",
    "    else:\n",
    "        bins  = bins_list[j-1]\n",
    "    ax[0,j].hist(rgi[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "    ax[0,j].set_title(xlabel)\n",
    "    ax2 = ax[0,j].twinx()\n",
    "    ax2.hist(aois[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "    ax2.set_yticks(ax2.get_yticks())\n",
    "    ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "    ax2.spines['right'].set_color(aois_color)\n",
    "    ax2.tick_params(axis='y', color=aois_color)\n",
    "ax[0,0].set_ylabel('All regions', fontweight='bold')\n",
    "\n",
    "# Individual subregions\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    # Subset glaciers\n",
    "    aois_subregion = aois.loc[aois['Subregion']==subregion]\n",
    "    rgi_subregion = rgi.loc[rgi['Subregion']==subregion]\n",
    "\n",
    "    # Plot all glaciers in subregion\n",
    "    for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "        if column=='Zmed':\n",
    "            bins = np.linspace(rgi_subregion['Zmed'].min(), rgi_subregion['Zmed'].max(), num=20)\n",
    "        else:\n",
    "            bins = bins_list[j-1]\n",
    "        ax[i+1,j].hist(rgi_subregion[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "        if j==0:\n",
    "            ax[i+1,j].set_ylabel(subregion)\n",
    "        ax2 = ax[i+1,j].twinx()\n",
    "        ax2.hist(aois_subregion[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "        ax2.set_yticks(ax2.get_yticks())\n",
    "        ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "        ax2.spines['right'].set_color(aois_color)\n",
    "        ax2.tick_params(axis='y', color=aois_color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_site_distributions.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44add00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rgi.loc[rgi['Area'] < 0.1]) / len(rgi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dc78f",
   "metadata": {},
   "source": [
    "## Figure S2. Snowline altitude uncertainty analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example site ID\n",
    "rgi_id = 'RGI60-01.01104'\n",
    "epsg_utm = \"EPSG:32606\"\n",
    "\n",
    "# Load SLA bounds\n",
    "sla_bounds_fn = os.path.join(out_path, 'SLA_uncertainty_analysis.nc')\n",
    "sla_bounds = xr.open_dataset(sla_bounds_fn)\n",
    "sla_bounds['SLA_bounds_range'] = sla_bounds['SLA_upper_bound'] - sla_bounds['SLA_lower_bound']\n",
    "\n",
    "# Load all AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add elevation range column\n",
    "aois['Zrange'] = aois['Zmax'] - aois['Zmin']\n",
    "# Merge with SLA bounds\n",
    "sla_bounds['Zrange'] = (('RGIId'), [aois.loc[aois['RGIId']==rgi_id, 'Zrange'].values[0] \n",
    "                                    for rgi_id in sla_bounds['RGIId'].values])\n",
    "# Load snow cover stats\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_classifications.zarr\")\n",
    "scs = xr.open_dataset(scs_fn)\n",
    "# Load DEM\n",
    "dem_fn = glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'DEMs', '*.tif'))[0]\n",
    "dem = rxr.open_rasterio(dem_fn).isel(band=0)\n",
    "dem = dem.rio.write_crs(\"EPSG:4326\")\n",
    "# Load AOI\n",
    "aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "# Clip DEM to AOI\n",
    "dem = dem.rio.clip(aoi.geometry)\n",
    "# Choose an image with some masking and relatively low transient AAR for demonstration\n",
    "sc = (scs.sel(time=slice(pd.Timestamp('2019-07-03'), pd.Timestamp('2019-07-05'))).isel(time=0))\n",
    "classified = sc.classification\n",
    "classified = xr.where(classified==0, np.nan, classified)\n",
    "classified = classified.rio.write_crs(\"EPSG:4326\")\n",
    "classified_utm = classified.rio.reproject(epsg_utm)\n",
    "classified_utm = xr.where(classified_utm > 1e30, np.nan, classified_utm)\n",
    "classified_utm = classified_utm.rio.write_crs(epsg_utm)\n",
    "\n",
    "# Create binary snow image\n",
    "snow_binary = xr.where((classified_utm==1) | (classified_utm==2), 1, 0)\n",
    "snow_binary = xr.where(np.isnan(classified_utm), np.nan, snow_binary) # re-insert no data values\n",
    "\n",
    "# Regrid DEM to classified image grid\n",
    "dem_utm = dem.rio.reproject_match(classified_utm)\n",
    "dem_utm = xr.where((dem_utm < -1e3) | (dem_utm > 1e4), np.nan, dem_utm)\n",
    "\n",
    "# Load RGB image from GEE\n",
    "import geedim as gd\n",
    "import wxee\n",
    "import ee\n",
    "ee.Initialize(project='ee-raineyaberle')\n",
    "aoi_ee = ee.Geometry.Polygon(list(zip(aoi.geometry[0].exterior.coords.xy[0], \n",
    "                                      aoi.geometry[0].exterior.coords.xy[1]))).buffer(1e3)\n",
    "im_col_ee = gd.MaskedCollection.from_name(\"COPERNICUS/S2_SR_HARMONIZED\").search(start_date='2019-07-03',\n",
    "                                                                            end_date='2019-07-04',\n",
    "                                                                            region=aoi_ee)\n",
    "im_ee = im_col_ee.ee_collection.first()\n",
    "im_ee = im_ee.select(['B2', 'B3', 'B4'])\n",
    "rgb_im = im_ee.wx.to_xarray(region=aoi_ee, scale=10)\n",
    "rgb_im = rgb_im.rio.reproject(epsg_utm)\n",
    "rgb_im = xr.where(rgb_im==-32768, np.nan, rgb_im/1e4).isel(time=0)\n",
    "\n",
    "# Calculate lower and upper bounds of snowline altitude\n",
    "sla_percentile = 1-sc.AAR.values\n",
    "sla = np.nanquantile(dem.data.ravel(), sla_percentile)\n",
    "sla_upper_percentile = (xr.where((dem_utm > sla) & (classified_utm > 2), 1, 0).sum().values / \n",
    "                        xr.where(~np.isnan(dem_utm), 1, 0).sum().values)\n",
    "sla_upper_bound = np.nanquantile(dem.data.ravel(), sla_percentile + sla_upper_percentile)\n",
    "sla_lower_percentile = (xr.where((dem_utm < sla) & (classified_utm <= 2), 1, 0).sum().values / \n",
    "                        xr.where(~np.isnan(dem_utm), 1, 0).sum().values)\n",
    "sla_lower_bound = np.nanquantile(dem.data.ravel(), sla_percentile - sla_lower_percentile)\n",
    "dem_utm = xr.where(dem_utm > 1e30, np.nan, dem_utm)\n",
    "# Define colormap for classified images\n",
    "cmap_dict = {\"Snow\": \"#4eb3d3\",  \"Shadowed_snow\": \"#4eb3d3\", \"Ice\": \"#084081\", \"Rock\": \"#fe9929\", \"Water\": \"#969696\"}\n",
    "colors = []\n",
    "for key in list(cmap_dict.keys()):\n",
    "    color = list(matplotlib.colors.to_rgb(cmap_dict[key]))\n",
    "    if key=='Rock':\n",
    "        color += [0.5]\n",
    "    colors.append(color)\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=11\n",
    "lw=1.5\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif': 'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(3,3)#, height_ratios=[1, 1.5, 1.5])\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[0,2]),\n",
    "      fig.add_subplot(gs[1,:]),\n",
    "      fig.add_subplot(gs[2,:])]\n",
    "\n",
    "# DEM\n",
    "im = ax[0].imshow(dem_utm.data, cmap='terrain', \n",
    "                  extent=(np.min(dem_utm.x)/1e3, np.max(dem_utm.x)/1e3, np.min(dem_utm.y)/1e3, np.max(dem_utm.y)/1e3))\n",
    "cbar = fig.colorbar(im, ax=ax[0], shrink=0.8, label='Elevation [m]')\n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "\n",
    "# RGB image\n",
    "ax[1].imshow(np.dstack([rgb_im['B4'].data, rgb_im['B3'], rgb_im['B2'].data]),\n",
    "             extent=(min(rgb_im.x)/1e3, max(rgb_im.x)/1e3, min(rgb_im.y)/1e3, max(rgb_im.y)/1e3))\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_yticklabels([])\n",
    "\n",
    "# classified image\n",
    "im = ax[2].imshow(classified_utm.data, cmap=cmap, clim=(0.5,5.5),\n",
    "                  extent=(np.min(classified_utm.x)/1e3, np.max(classified_utm.x)/1e3, np.min(classified_utm.y)/1e3, np.max(classified_utm.y)/1e3))\n",
    "cbar = fig.colorbar(im, ax=ax[2], shrink=0.8, ticks=[5, 4, 3, 2])\n",
    "cbar.ax.set_yticklabels(['Water', 'Rock', 'Ice/firn', 'Snow'])\n",
    "cbar.ax.set_ylim(1.5,5.5)\n",
    "cbar.ax.invert_yaxis()\n",
    "ax[2].set_yticklabels([])\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[2].set_ylabel('')\n",
    "\n",
    "ax[1].set_xlim(ax[2].get_xlim())\n",
    "ax[1].set_ylim(ax[2].get_ylim())\n",
    "for axis in ax[0:3]:\n",
    "    axis.set_yticks([6538, 6540, 6542, 6544])\n",
    "\n",
    "# SLA contours\n",
    "x_mesh, y_mesh = np.meshgrid(np.divide(dem_utm.x.data, 1e3), np.divide(dem_utm.y.data, 1e3))\n",
    "for axis in ax[0:3]:\n",
    "    axis.contour(x_mesh, y_mesh, dem_utm.data, levels=[sla], colors='k', linewidth=lw)\n",
    "\n",
    "# histograms and bounds\n",
    "bin_edges = np.arange(700, 1481, step=10)\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# all elevations\n",
    "counts, _ = np.histogram(dem_utm.data, bins=bin_edges)\n",
    "areas = counts * 10**2 / 1e6 # km^2\n",
    "ax[3].bar(bin_centers, areas, width=bin_edges[1]-bin_edges[0], facecolor='gray', \n",
    "          edgecolor='k', alpha=0.5, linewidth=lw-1, label='All elevations')\n",
    "# snow-covered elevations\n",
    "dem_snow = xr.where(classified_utm==1, dem_utm, np.nan)\n",
    "counts, _ = np.histogram(dem_snow.data, bins=bin_edges)\n",
    "areas = counts * 10**2 / 1e6 # km^2\n",
    "ax[3].bar(bin_centers, areas, width=bin_edges[1]-bin_edges[0], facecolor=cmap_dict['Snow'], \n",
    "          edgecolor='k', alpha=1.0, linewidth=lw-1, label='Snow-covered elevations')\n",
    "ax[3].axvline(sla, color='k', linewidth=lw, label='Original SLA')\n",
    "ax[3].axvline(sla_lower_bound, color='k', linestyle='--', linewidth=lw, label='SLA lower bound')\n",
    "ax[3].axvline(sla_upper_bound, color='k', linestyle=':', linewidth=lw, label='SLA upper bound')\n",
    "ax[3].text(850, 0.22, \"Snow-covered area \\nbelow SLA = 0.78 km$^2$\", fontsize=9, ha='center',\n",
    "           bbox=dict(facecolor='w', edgecolor='None', alpha=0.7))\n",
    "ax[3].text(1320, 0.22, \"Snow-free area \\nabove SLA = 0.66 km$^2$\", fontsize=9, ha='center')\n",
    "ax[3].set_xlim(np.min(bin_edges)-20, np.max(bin_edges)+20)\n",
    "ax[3].set_xlabel('Elevation [m]')\n",
    "ax[3].set_ylabel('Area [km$^2$]')\n",
    "ax[3].legend(loc='upper left')\n",
    "ax[3].set_xlim(700, 1400)\n",
    "\n",
    "# histogram of SLA ranges\n",
    "ax[4].hist(sla_bounds['SLA_bounds_range'], bins=np.linspace(0, 1000, num=101), \n",
    "           facecolor='gray', alpha=0.9, edgecolor='k', linewidth=0.5)\n",
    "range_median = np.nanmedian(sla_bounds['SLA_bounds_range'])\n",
    "range_mean = np.nanmean(sla_bounds['SLA_bounds_range'])\n",
    "range_p25 = np.nanpercentile(sla_bounds['SLA_bounds_range'], 25)\n",
    "range_p75 = np.nanpercentile(sla_bounds['SLA_bounds_range'], 75)\n",
    "ax[4].axvline(range_median, color='k', linewidth=lw, \n",
    "              label=f\"Median = {int(range_median)} m\")\n",
    "ax[4].axvline(range_mean, color='k', linewidth=lw, linestyle='--', \n",
    "              label=f\"Mean = {int(range_mean)} m\")\n",
    "ax[4].fill_between([range_p25, range_p75], [0,0], [3800,3800], color='k', alpha=0.1, edgecolor='k', linewidth=lw+1,\n",
    "                   label=f\"IQR = {int(range_p25)}–{int(range_p75)} m\")\n",
    "ax[4].legend(loc='center right')\n",
    "ax[4].set_xlabel('Range of all SLA bounds [m]')\n",
    "ax[4].set_ylabel('Counts')\n",
    "ax[4].set_xlim(0, 500)\n",
    "ax[4].set_ylim(0, 3800)\n",
    "# add panel labels\n",
    "labels = ['a', 'b', 'c', 'd', 'e']\n",
    "for i, axis in enumerate(ax):\n",
    "    if i < 3:\n",
    "        xscale=0.8\n",
    "        yscale=0.85\n",
    "    else:\n",
    "        xscale=0.95\n",
    "        yscale=0.9\n",
    "    axis.text(xscale, yscale, labels[i], transform=axis.transAxes, fontsize=fontsize+4, fontweight='bold',\n",
    "              bbox=dict(facecolor='w', edgecolor='w'))\n",
    "\n",
    "# fig.tight_layout()\n",
    "\n",
    "ax[1].set_position([0.44, ax[0].get_position().y0, ax[0].get_position().width, ax[0].get_position().height])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS2_SLA_uncertainties.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a4ec9",
   "metadata": {},
   "source": [
    "## Figure S3. Median snow cover minima sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "gs = matplotlib.gridspec.GridSpec(2,2, height_ratios=[1.5, 1])\n",
    "fig = plt.figure(figsize=(7,8))\n",
    "ax = [fig.add_subplot(gs[0,:]), \n",
    "      fig.add_subplot(gs[1,0]), fig.add_subplot(gs[1,1])]\n",
    "\n",
    "# Plot an example Monte Carlo simulation at South Cascade Glacier\n",
    "rgi_id = 'RGI60-02.18778'\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_classifications.zarr\")\n",
    "scs = f.load_snow_cover_stats(scs_fn)\n",
    "scs = scs.assign_coords({'WOY': scs['time'].dt.isocalendar().week,\n",
    "                         'Year': scs['time'].dt.isocalendar().year})\n",
    "scs['snow_area_km2'] = scs['snow_area'] / 1e6\n",
    "\n",
    "# Plot\n",
    "sns.scatterplot(scs, x='WOY', y='AAR', hue='Year', palette='viridis', s=10, legend=True, ax=ax[0])\n",
    "# Number of samples per simulation\n",
    "sample_fraction = 0.8\n",
    "nsamp = int(len(scs) * sample_fraction)\n",
    "nMC = 100\n",
    "# Monte Carlo simulations\n",
    "results = pd.DataFrame()\n",
    "for i in range(nMC):\n",
    "    sampled_indices = np.random.choice(scs.time.data, size=nsamp, replace=False)\n",
    "    scs_MC = scs.isel(time=i)\n",
    "    # Calculate weekly medians for AAR, SCA, and SLA\n",
    "    weekly_medians = scs[['AAR', 'snow_area_km2', 'SLA']].groupby('WOY').median().to_dataframe()\n",
    "    weekly_medians['MC_run'] = i    \n",
    "    # Plot\n",
    "    ax[0].plot(weekly_medians.index, weekly_medians['AAR'], '-', color='gray', linewidth=1)\n",
    "    results = pd.concat([results, weekly_medians])\n",
    "\n",
    "\n",
    "# Dummy line for legend\n",
    "ax[0].plot([0,0], [0,0], '-', color='gray', linewidth=2, label='MC simulation')\n",
    "# Estimate median AAR and snow minimum timing\n",
    "medians = results.groupby('WOY')['AAR'].median()\n",
    "value = medians.loc[medians==min(medians)]\n",
    "ax[0].scatter(value.index[0], medians.min(), marker='x', s=50, color='k', linewidth=2, label='Median AAR', zorder=5)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].set_ylabel('Transient accumulation area ratio')\n",
    "ax[0].set_xlabel('Week of year')\n",
    "ax[0].set_xlim(16,45)\n",
    "ax[0].set_ylim(-0.1,1.1)\n",
    "# add legend\n",
    "ax[0].legend().remove()\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend(handles, labels, loc='lower left', markerscale=2)\n",
    "\n",
    "# Plot AAR and snow minima timing distributions for all sites\n",
    "median_scs_fn = os.path.join(out_path, 'median_snow_cover_stats_MC.nc')\n",
    "median_scs = xr.open_dataset(median_scs_fn)\n",
    "aar_woy_mads = pd.DataFrame()\n",
    "# iterate over site names in median snow cover stats dataframe\n",
    "for rgi_id in tqdm(median_scs.RGIId.values):\n",
    "    # Calculate AAR median and MAD across MC simulations\n",
    "    aar_median = float(median_scs.sel(RGIId=rgi_id).AAR.min(dim='WOY').median().values)\n",
    "    aar_mad = median_abs_deviation(median_scs.sel(RGIId=rgi_id).AAR.min(dim='WOY').values)\n",
    "    Imins = median_scs.sel(RGIId=rgi_id).AAR.argmin(dim='WOY').values\n",
    "    woy_mad = median_abs_deviation(median_scs.WOY.values[Imins])\n",
    "    # Add to dataFrame\n",
    "    df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                       'AAR_MAD': [aar_mad],\n",
    "                       'WOY_MAD': [woy_mad]})\n",
    "    aar_woy_mads = pd.concat([aar_woy_mads, df], axis=0)\n",
    "\n",
    "\n",
    "# Plot\n",
    "ax[1].hist(aar_woy_mads['AAR_MAD'], bins=50, color='gray', edgecolor='k', linewidth=0.5)\n",
    "ax[1].text(0.95, 0.7, f\"Median = {np.round(np.nanmedian(aar_woy_mads['AAR_MAD']),2)}\", ha='right', transform=ax[1].transAxes)\n",
    "ax[1].text(0.95, 0.63, f\"Mean = {np.round(np.nanmean(aar_woy_mads['AAR_MAD']),2)}\", ha='right', transform=ax[1].transAxes)\n",
    "ax[1].set_xlabel('AAR MAD')\n",
    "ax[1].set_ylabel('Counts')\n",
    "ax[2].hist(aar_woy_mads['WOY_MAD'], bins=50, color='gray', edgecolor='k', linewidth=0.5)\n",
    "ax[2].text(0.95, 0.7, f\"Median = {np.round(np.nanmedian(aar_woy_mads['WOY_MAD']),2)}\", ha='right', transform=ax[2].transAxes)\n",
    "ax[2].text(0.95, 0.63, f\"Mean = {np.round(np.nanmean(aar_woy_mads['WOY_MAD']),2)}\", ha='right', transform=ax[2].transAxes)\n",
    "ax[2].set_xlabel('Snow minimum timing MAD [weeks]')\n",
    "ax[2].set_xlim(-0.1,3.5)\n",
    "\n",
    "# add panel labels\n",
    "labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.text(0.9, 0.92, labels[i], transform=axis.transAxes, fontsize=fontsize+2, fontweight='bold')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS3_AAR_MC_simulations.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ae401",
   "metadata": {},
   "source": [
    "## Figure S4. Climate clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d212582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded')\n",
    "\n",
    "# Load feature scaler\n",
    "scaler_fn = os.path.join(code_path, 'inputs-outputs', 'ERA5_feature_scaler.joblib')\n",
    "scaler = joblib.load(scaler_fn)\n",
    "\n",
    "# Apply the feature scaler for normalized values\n",
    "clusters[['mean_annual_precip_cumsum_norm', 'mean_annual_temp_range_norm']] = scaler.transform(clusters[['mean_annual_precip_cumsum', 'mean_annual_temp_range']])\n",
    "print('Added standardized feature columns')\n",
    "\n",
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add climate clusters column\n",
    "aois = pd.merge(aois, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f0bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get limits for scaled data\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(data=clusters, x='mean_annual_temp_range_norm', y='mean_annual_precip_cumsum_norm')\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "plt.close()\n",
    "\n",
    "# Set up figure\n",
    "ontsize = 12\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif': 'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(2,2, height_ratios=[1,1.5])\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[1,:])]\n",
    "\n",
    "### a) Feature values and cluster assignments\n",
    "# Input features and scaled features\n",
    "scat = sns.scatterplot(data=clusters, x='mean_annual_temp_range', y='mean_annual_precip_cumsum',\n",
    "                       edgecolor='k', linewidth=0.5, s=20, \n",
    "                       hue='clustName', hue_order=cluster_order, palette=cluster_cmap_dict, legend=False, ax=ax[0])\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel('Air temperature range [$^{\\circ}$C]')\n",
    "ax[0].set_ylabel('Precipitation sum [m]')\n",
    "# add secondary axes for scaled features\n",
    "ax_top = ax[0].twiny()\n",
    "ax_right = ax[0].twinx()\n",
    "ax_top.set_xlim(xmin, xmax)\n",
    "ax_right.set_ylim(ymin, ymax)\n",
    "ax_right.set_ylabel('Standardized air temperature range [unitless]', color='grey')\n",
    "ax_top.set_xlabel('Standardized precipitation sum [unitless]', color='grey')\n",
    "ax_right.spines['top'].set_color('grey')\n",
    "ax_right.spines['right'].set_color('grey')\n",
    "ax_top.set_xticklabels(ax_top.get_xticklabels(), color='grey')\n",
    "ax_right.set_yticklabels(ax_right.get_yticklabels(), color='grey')\n",
    "ax_top.tick_params(axis='x', colors='grey')\n",
    "ax_right.tick_params(axis='y', colors='grey')\n",
    "\n",
    "# Silhouette coefficient and intertia\n",
    "K = np.arange(2, 11)\n",
    "# copied outputs from notebook: 2_develop_climate_clusters.ipynb\n",
    "sil_coefs = [0.426, 0.419, 0.442, 0.460, 0.446, 0.415, 0.430, 0.439, 0.422]\n",
    "inertias = [198, 132, 91, 68, 56, 48, 41, 32, 30]\n",
    "inertia_color = '#d95f02'\n",
    "sil_color = '#7570b3'\n",
    "# silhouette coefficient\n",
    "ax[1].plot(K, sil_coefs, '.-', color=sil_color)\n",
    "Ibest = np.argwhere(sil_coefs==np.nanmax(sil_coefs))[0][0]\n",
    "ax[1].plot(K[Ibest], sil_coefs[Ibest], '*', color=sil_color, markersize=15)\n",
    "ax[1].set_xlabel('Number of clusters')\n",
    "ax[1].set_ylabel('Silhouette score', color=sil_color)\n",
    "ax[1].grid()\n",
    "ax[1].tick_params(axis='y', color=sil_color)\n",
    "ax[1].set_yticklabels(ax[1].get_yticklabels(), color=sil_color)\n",
    "# inertia\n",
    "ax2 = ax[1].twinx()\n",
    "ax2.plot(K, inertias, '.-', color=inertia_color)\n",
    "Ibest = 3\n",
    "ax2.plot(K[Ibest], inertias[Ibest], '*', color=inertia_color, markersize=15)\n",
    "ax2.spines['right'].set_color(inertia_color)\n",
    "ax2.spines['left'].set_color(sil_color)\n",
    "ax2.set_ylabel('Inertia', color=inertia_color)\n",
    "ax2.tick_params(axis='y', color=inertia_color)\n",
    "ax2.set_yticklabels(ax2.get_yticklabels(), color=inertia_color)\n",
    "ax2.text(4.8, 68, 'elbow', color=inertia_color, rotation=-35, fontsize=14)\n",
    "\n",
    "### c) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[2].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[2], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[2].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[2].set_xlim(-167, -112)\n",
    "ax[2].set_ylim(46, 66)\n",
    "ax[2].set_xlabel('Longitude ($^{\\circ}$E)')\n",
    "ax[2].set_ylabel('Latitude ($^{\\circ}$N)')\n",
    "ax[2].set_aspect(2.2)\n",
    "# Site locations\n",
    "sns.scatterplot(data=aois, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, s=20,\n",
    "                hue='clustName', palette=cluster_cmap_dict, hue_order=cluster_order, alpha=1, ax=ax[2])\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "ax[2].legend().remove()\n",
    "ax[2].legend(handles, labels, loc='lower left', markerscale=2, bbox_to_anchor=[0.08, 0.08, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "background_color = [1, 1, 1, 0.5]\n",
    "ax[2].text(-160.5, 56.5, \"Aleutians\", ha='center', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].text(-157, 62.7, \"Alaska Range\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].text(-147.7, 57.7, \"W. Chugach \\nMtns.\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].arrow(-147.6, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[2].text(-141.5, 57.7, \"St. Elias \\nMtns.\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].arrow(-141.5, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[2].text(-139.8, 56, \"N. Coast \\nRanges\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].arrow(-137.1, 56.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[2].text(-133.7, 51.3, \"N. Cascades\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].arrow(-129.9, 51.4, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[2].text(-130, 47.4, \"S. Cascades\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].arrow(-126.3, 47.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[2].text(-129.5, 64.5, \"N. Rockies\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].text(-119, 54.5, \"C. Rockies\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[2].text(-115, 47, \"S. Rockies\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "# Add panel labels\n",
    "labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.text(axis.get_xlim()[0] + (axis.get_xlim()[1] - axis.get_xlim()[0])*0.9,\n",
    "              axis.get_ylim()[0] + (axis.get_ylim()[1] - axis.get_ylim()[0])*0.9,\n",
    "              labels[i], fontweight='bold', fontsize=fontsize+4, ha='center')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS4_kmeans.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605a442",
   "metadata": {},
   "source": [
    "## Figure S5. Constraining mass balance model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glacier IDs from model runs\n",
    "pygem_new_path = os.path.join(scm_path, 'Brandon_new_PyGEM_runs')\n",
    "rgi_ids = [x for x in sorted(os.listdir(pygem_new_path)) if os.path.isdir(os.path.join(pygem_new_path, x))]\n",
    "names = ['Gulkana', 'Wolverine', 'Lemon Creek', 'Sperry', 'South Cascade']\n",
    "print('RGI IDs for glaciers with PyGEM runs:', rgi_ids)\n",
    "# Define path to original model runs\n",
    "model_path = os.path.join(scm_path, 'Rounce_et_al_2023')\n",
    "\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(len(rgi_ids), 2, figsize=(8, 12), gridspec_kw={'width_ratios': [1.5,1]})\n",
    "rmse_cmap = 'Blues'\n",
    "observed_color = 'k'\n",
    "original_color = '#fec44f'\n",
    "adjusted_color = '#993404'\n",
    "\n",
    "# Iterate over RGI IDs\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    name = names[i]\n",
    "    print(name, rgi_id)\n",
    "\n",
    "    # Load observed snow cover data\n",
    "    scs_fn = os.path.join(scm_path, 'study-sites', f\"RGI60-0{rgi_id}\", f\"RGI60-0{rgi_id}_snow_cover_stats_adjusted.csv\")\n",
    "    scs = pd.read_csv(scs_fn)\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'])\n",
    "    \n",
    "    # Load PyGEM runs\n",
    "    runs_fn = os.path.join(scm_path, 'analysis', f\"PyGEM_comparison_RGI60-0{rgi_id}.nc\")\n",
    "    runs = xr.open_dataset(runs_fn)\n",
    "\n",
    "    # Load original model parameters\n",
    "    modelprms_fn = os.path.join(model_path, '..', 'Rounce_et_al_2023', 'modelprms', f\"{rgi_id}-modelprms_dict.pkl\")\n",
    "    modelprms = pd.read_pickle(modelprms_fn)\n",
    "    # get the original values\n",
    "    original_ddfsnow = np.median(modelprms['MCMC']['ddfsnow']['chain_0'])\n",
    "    original_tbias = np.median(modelprms['MCMC']['tbias']['chain_0'])\n",
    "    original_kp = np.median(modelprms['MCMC']['kp']['chain_0'])\n",
    "\n",
    "    # Calculate RMSE for each run's snowline altitudes\n",
    "    diff = runs['mod-obs_SLA']\n",
    "    rmse_by_run = np.sqrt((diff**2).mean(dim='time'))\n",
    "    runs['rmse'] = rmse_by_run\n",
    "\n",
    "    # identify parameter combinations with the lowest RMSE\n",
    "    df_plot = runs[['tbias', 'ddfsnow', 'kp', 'rmse', 'mod-obs_SLA']].to_dataframe().reset_index()\n",
    "    df_plot = df_plot.dropna(subset=['rmse'])\n",
    "    df_plot_best = df_plot.loc[df_plot['rmse'].idxmin()]\n",
    "\n",
    "    # subset the model dataset for the original vs. best runs\n",
    "    # original\n",
    "    squared_diffs = sum((runs[var] - np.median(modelprms['MCMC'][var]['chain_0']))**2 for var in ['tbias', 'ddfsnow', 'kp'])\n",
    "    best_run_idx = squared_diffs.argmin(dim=\"run\")\n",
    "    combined_ds_original = runs.sel(run=best_run_idx, glac=0)\n",
    "    # best\n",
    "    combined_ds_best = runs.sel(run=int(df_plot_best['run']), glac=0)\n",
    "\n",
    "    # adjust ddf units for plotting\n",
    "    combined_ds_original['ddfsnow'] = combined_ds_original['ddfsnow'] * 1e3\n",
    "    combined_ds_best['ddfsnow'] = combined_ds_best['ddfsnow']\n",
    "    df_plot['ddfsnow'] = df_plot['ddfsnow'] * 1e3\n",
    "    df_plot_best['ddfsnow'] = df_plot_best['ddfsnow'] * 1e3\n",
    "    original_ddfsnow = original_ddfsnow * 1e3\n",
    "\n",
    "    # add plotting columns\n",
    "    df_plot['size'] = df_plot['kp']*10\n",
    "    # df_plot.sort_values(by='rmse', inplace=True, ascending=False) # plot lowest RMSE on top    \n",
    "\n",
    "    ### Plot RMSE as a function of tbias, ddfsnow, and kp\n",
    "    im = ax[i,0].scatter(df_plot['tbias'], df_plot['ddfsnow'], s=df_plot['size'], c=df_plot['rmse'], cmap=rmse_cmap, \n",
    "                         marker='o', edgecolor='k', linewidth=0.25)\n",
    "    # add colorbar\n",
    "    axin = inset_axes(ax[i,0], width=\"20%\", height=\"4%\", loc=\"lower left\")\n",
    "    cbar = plt.colorbar(im, cax=axin, orientation=\"horizontal\")\n",
    "    axin.xaxis.set_ticks_position('top')\n",
    "    cbar.ax.set_title('RMSE [m]', fontsize=9)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "    # original parameter combinations\n",
    "    ax[i,0].plot(original_tbias, original_ddfsnow, 's', markersize=10, markeredgecolor=original_color, \n",
    "                 markerfacecolor='None', markeredgewidth=2, label='Original')\n",
    "    # best parameter combination\n",
    "    ax[i,0].plot(df_plot_best['tbias'], df_plot_best['ddfsnow'], \n",
    "                 '*', markersize=15, markeredgecolor=adjusted_color, markerfacecolor='None', markeredgewidth=2, label='Adjusted')\n",
    "    ax[i,0].grid(True)\n",
    "    ax[i,0].set_ylabel('DDF$_{snow}$ [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "    ax[i,0].set_title(f'{name} Glacier')   \n",
    "    ax[i,0].set_ylim(0, 10)\n",
    "    # ax[i,0].set_xlim(-7,4)\n",
    "\n",
    "    ### Modeled and observed snowline time series by WOY\n",
    "    # original\n",
    "    mod_og_gb = combined_ds_original['glac_snowline_monthly'].groupby(combined_ds_original.time.dt.isocalendar().week).quantile([0.25, 0.5, 0.75])\n",
    "    mod_og_p25, mod_og_p50, mod_og_p75 = mod_og_gb.sel(quantile=0.25), mod_og_gb.sel(quantile=0.50), mod_og_gb.sel(quantile=0.75)\n",
    "    ax[i,1].fill_between(mod_og_p25.week, mod_og_p25, mod_og_p75, facecolor=original_color, edgecolor='None', alpha=0.2)\n",
    "    ax[i,1].plot(mod_og_p50.week, mod_og_p50.values, '-', color=original_color, label='Original')\n",
    "    # adjusted\n",
    "    mod_best_gb = combined_ds_best['glac_snowline_monthly'].groupby(combined_ds_best.time.dt.isocalendar().week).quantile([0.25, 0.5, 0.75])\n",
    "    mod_best_p25, mod_best_p50, mod_best_p75 = mod_best_gb.sel(quantile=0.25), mod_best_gb.sel(quantile=0.50), mod_best_gb.sel(quantile=0.75)\n",
    "    ax[i,1].fill_between(mod_best_p25.week, mod_best_p25, mod_best_p75, facecolor=adjusted_color, edgecolor='None', alpha=0.2)\n",
    "    ax[i,1].plot(mod_best_p50.week, mod_best_p50.values, '--', color=adjusted_color, label='Adjusted')\n",
    "    # observed\n",
    "    scs_p25 = scs.groupby(scs['datetime'].dt.isocalendar().week)['SLA_m'].quantile(0.25).reset_index()\n",
    "    scs_p50 = scs.groupby(scs['datetime'].dt.isocalendar().week)['SLA_m'].quantile(0.50).reset_index()\n",
    "    scs_p75 = scs.groupby(scs['datetime'].dt.isocalendar().week)['SLA_m'].quantile(0.75).reset_index()\n",
    "    ax[i,1].fill_between(scs_p25['week'].astype(float), scs_p25['SLA_m'], scs_p75['SLA_m'], color=observed_color, alpha=0.2)\n",
    "    ax[i,1].plot(scs_p50['week'].astype(float), scs_p50['SLA_m'], ':', color=observed_color, label='Observed')\n",
    "    ax[i,1].set_xlim(17, 41)\n",
    "    ax[i,1].set_ylabel('Elevation [m]')\n",
    "\n",
    "    # add x-labels and legends on last row\n",
    "    if i==len(rgi_ids)-1:\n",
    "        ax[i,0].set_xlabel('Temperature bias [$^{\\circ}$C]')\n",
    "        ax[i,1].set_xlabel('Week of year')\n",
    "        # custom frame for first two legends\n",
    "        patch = matplotlib.patches.FancyBboxPatch((-4.5, -10.4), 5.5, 6.1, facecolor='None', edgecolor='gray', \n",
    "                                                  linewidth=0.5, clip_on=False, boxstyle=matplotlib.patches.BoxStyle(\"round\", pad=0.2))\n",
    "        ax[4,0].add_patch(patch)\n",
    "        # first legend\n",
    "        first_legend = ax[i,0].legend(bbox_to_anchor=[0.1, -0.66, 0.2, 0.2], frameon=False)\n",
    "        ax[i,0].add_artist(first_legend)\n",
    "        # second legend for kp (using dummy points)\n",
    "        xmin, xmax = ax[i,0].get_xlim()\n",
    "        for s in [1,2,3]:\n",
    "            ax[i,0].plot(-100, -100, 'ok', markersize=s*3, label=str(s))\n",
    "        ax[i,0].set_xlim(xmin, xmax)\n",
    "        handles, labels = ax[i,0].get_legend_handles_labels()\n",
    "        ax[i,0].legend(handles[2:], labels[2:], loc='upper center', bbox_to_anchor=[0.6, -0.53, 0.2, 0.2], \n",
    "                       frameon=False, title='Precipitation factor')\n",
    "        # third legend\n",
    "        ax[i,1].legend(loc='upper center', bbox_to_anchor=[0.4, -0.5, 0.2, 0.2])\n",
    "    else:\n",
    "        ax[i,0].set_xlabel('')\n",
    "        ax[i,1].set_xlabel('')\n",
    "\n",
    "# add panel labels\n",
    "import string\n",
    "labels = list(string.ascii_lowercase)\n",
    "for i, axis in enumerate(ax.ravel()):\n",
    "    if i % 2 == 0:\n",
    "        xpos = 0.95\n",
    "    else:\n",
    "        xpos = 0.05\n",
    "    axis.text(xpos, 0.8, labels[i], transform=axis.transAxes, fontweight='bold', fontsize=14)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS5_PyGEM_constraints.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c75d3",
   "metadata": {},
   "source": [
    "## Table S1. Study sites with subregion and climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "aois = gpd.read_file(aois_fn)\n",
    "aois.rename(columns={'Subregion': 'Subregion name'}, inplace=True)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "\n",
    "# Add climate cluster\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "aois['Climate cluster'] = ''\n",
    "for rgi_id in aois['RGIId'].drop_duplicates().values:\n",
    "    aois.loc[aois['RGIId']==rgi_id, 'Climate cluster'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName']\n",
    "aois.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "\n",
    "# Format as LaTeX table\n",
    "columns = ['RGIId', 'O1Region', 'O2Region', 'Subregion name', 'Climate cluster']\n",
    "aois = aois[columns]\n",
    "\n",
    "# Save as Excel sheet\n",
    "aois_xl_fn = os.path.join(out_path, 'TableS1_study_sites.xlsx')\n",
    "aois.to_excel(aois_xl_fn, index=False)\n",
    "print('Table saved as Excel spreadsheet:', aois_xl_fn)\n",
    "aois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf30237",
   "metadata": {},
   "source": [
    "## Glacier area changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245443cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate minimum annual areas distribution\n",
    "# Load study sites\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Load climate clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "# Initialize dataframe to store area distribution results\n",
    "dfs_list = []\n",
    "\n",
    "# Iterate over study sites\n",
    "for rgi_id in tqdm(aois['RGIId'].unique()):\n",
    "    # load snow cover stats\n",
    "    scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats_adjusted2.csv\")\n",
    "    scs = pd.read_csv(scs_fn)\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'])\n",
    "    scs = scs.loc[scs['datetime'].dt.year >= 2016]\n",
    "    scs['glacier_area_km2'] = scs['glacier_area_m2'] / 1e6    \n",
    "\n",
    "    # subset to later in the season\n",
    "    scs = scs.loc[(scs['datetime'].dt.month >= 8) & (scs['datetime'].dt.month <= 10)]\n",
    "\n",
    "    # Calculate 25th, 50th, and 75th percentiles of minimum annual area\n",
    "    area_q25, area_q50, area_q75 = np.nanpercentile(scs.groupby(scs['datetime'].dt.year)['glacier_area_km2'].median(), \n",
    "                                                    [25, 50, 75])\n",
    "    \n",
    "    # Calculate as fraction of RGI area\n",
    "    rgi_area = aois.loc[aois['RGIId']==rgi_id, 'Area'].values[0]\n",
    "    area_q25_frac, area_q50_frac, area_q75_frac = np.array([area_q25, area_q50, area_q75]) / rgi_area\n",
    "\n",
    "    # Store in dataframe\n",
    "    df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                       'clustName': [clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]],\n",
    "                       'Subregion': [aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]],\n",
    "                       'RGI_area_km2': [rgi_area],\n",
    "                       'glacier_area_km2_P25': [area_q25],\n",
    "                       'glacier_area_km2_P50': [area_q50],\n",
    "                       'glacier_area_km2_P75': [area_q75],\n",
    "                       'glacier_area_fraction_P25': [area_q25_frac],\n",
    "                       'glacier_area_fraction_P50': [area_q50_frac],\n",
    "                       'glacier_area_fraction_P75': [area_q75_frac],\n",
    "                       })\n",
    "    \n",
    "    # Add to dataframe list\n",
    "    dfs_list.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "areas_df = pd.concat(dfs_list)\n",
    "areas_df.reset_index(drop=True, inplace=True)\n",
    "areas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf01c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution for each site\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16,5))\n",
    "for rgi_id in areas_df['RGIId'].unique():\n",
    "    id_display = rgi_id.replace('RGI60-0','')\n",
    "    area_df = areas_df.loc[areas_df['RGIId']==rgi_id]\n",
    "    ax.plot([id_display, id_display], \n",
    "            [area_df['glacier_area_fraction_P25'], area_df['glacier_area_fraction_P75']],\n",
    "            '-', color='gray')\n",
    "    ax.plot(id_display, area_df['glacier_area_fraction_P50'], '.k')\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=5)\n",
    "ax.set_xlim(-2, 202)\n",
    "ax.set_ylim(0, 0.9)\n",
    "ax.set_ylabel('Fraction of RGI v. 6 glacier area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab56f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot of IQRs by climate cluster\n",
    "sns.boxplot(areas_df, x='clustName', y='glacier_area_fraction_P50', hue='clustName', palette=cluster_cmap_dict, showfliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73eca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplot of IQRs by subregion\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "sns.boxplot(areas_df, y='Subregion', x='glacier_area_fraction_P50', \n",
    "            hue='Subregion', palette='mako', medianprops=dict(color=\"w\", linewidth=2), showfliers=False, ax=ax)\n",
    "ax.set_xlabel('Median glacier area / RGI v. 6 area')\n",
    "ax.set_ylabel('RGI O2 region')\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'glacier_area_change_2016–2023.png')\n",
    "save_figure(fig, fig_fn)\n",
    "\n",
    "# Print statistics by subregion\n",
    "print(areas_df['glacier_area_fraction_P50'].describe())\n",
    "areas_df.groupby('Subregion')['glacier_area_fraction_P50'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca2a12",
   "metadata": {},
   "source": [
    "## Number of observations at each site vs. region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "aois = gpd.read_file(aois_fn)\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "df_list = []\n",
    "for i in range(len(aois)):\n",
    "    aoi = aois.iloc[i]\n",
    "    rgi_id = aoi['RGIId']\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    classified_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'classified', '*classified.nc')))\n",
    "    df_list.append(pd.DataFrame({'RGIId': rgi_id,\n",
    "                                 'Subregion': [aoi['Subregion']],\n",
    "                                 'clustName': [cluster],\n",
    "                                 'nobs': [len(classified_fns)]}))\n",
    "    \n",
    "nobs_df = pd.concat(df_list)\n",
    "nobs_df.reset_index(drop=True, inplace=True)\n",
    "nobs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7705c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(nobs_df, y='clustName', x='nobs', hue='clustName', \n",
    "            palette=cluster_cmap_dict, hue_order=cluster_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.sans-serif': 'Arial', 'font.size': 12})\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(nobs_df, y='Subregion', x='nobs', hue='Subregion', \n",
    "            palette='mako', hue_order=subregion_order, ax=ax)\n",
    "ax.set_ylabel('RGI O2 region')\n",
    "ax.set_xlabel('Number of observations')\n",
    "plt.show()\n",
    "\n",
    "nobs_df.groupby('Subregion')['nobs'].describe().sort_values(by='50%')\n",
    "\n",
    "fig_fn = os.path.join(figures_out_path, 'number_of_observations_by_region.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cbf85",
   "metadata": {},
   "source": [
    "## README figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ed622",
   "metadata": {},
   "source": [
    "### Median weekly trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif': \"Arial\"})\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "# Load snow cover stats\n",
    "rgi_id = 'RGI60-02.18778'\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], errors='coerce')\n",
    "scs.dropna(subset=['datetime'], inplace=True)\n",
    "scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "scs['Year'] = scs['datetime'].dt.isocalendar().year\n",
    "scs['Year'] = pd.Categorical(scs['Year'])\n",
    "scs['SCA_km2'] = scs['snow_area'] / 1e6\n",
    "# Plot\n",
    "sns.scatterplot(scs, x='WOY', y='AAR', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[0])\n",
    "sns.scatterplot(scs, x='WOY', y='SCA_km2', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[1])\n",
    "sns.scatterplot(scs, x='WOY', y='ELA_from_AAR', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[2])\n",
    "\n",
    "\n",
    "# Number of samples per simulation\n",
    "sample_fraction = 0.8\n",
    "nsamp = int(len(scs) * sample_fraction)\n",
    "nMC = 100\n",
    "\n",
    "# Monte Carlo simulations\n",
    "results = pd.DataFrame()\n",
    "for i in range(nMC):\n",
    "    sampled_indices = np.random.choice(scs.index, size=nsamp, replace=False)\n",
    "    scs_MC = scs.loc[sampled_indices].sort_values(by='datetime')\n",
    "\n",
    "    # Calculate weekly medians for AAR, SCA, and ELA\n",
    "    weekly_medians = scs_MC.groupby('WOY')[['AAR', 'SCA_km2', 'ELA_from_AAR']].median()\n",
    "    weekly_medians['MC_run'] = i    \n",
    "    # Plot\n",
    "    if i==0:\n",
    "        label = 'MC simulations'\n",
    "    else:\n",
    "        label = '_nolegend'\n",
    "    ax[0].plot(weekly_medians.index, weekly_medians['AAR'], '-', color='gray', linewidth=0.1, label=label)\n",
    "    ax[1].plot(weekly_medians.index, weekly_medians['SCA_km2'], '-', color='gray', linewidth=0.1)\n",
    "    ax[2].plot(weekly_medians.index, weekly_medians['ELA_from_AAR'], '-', color='gray', linewidth=0.1)\n",
    "    results = pd.concat([results, weekly_medians])\n",
    "    \n",
    "# Estimate median AAR and snow minimum timing\n",
    "for i, column in enumerate(['AAR', 'SCA_km2', 'ELA_from_AAR']):\n",
    "    medians = results.groupby('WOY')[column].median()\n",
    "    if column!='ELA_from_AAR':\n",
    "        value = medians.loc[medians==min(medians)]\n",
    "    else:\n",
    "        value = medians.loc[medians==max(medians)]\n",
    "    ax[i].axvline(value.index[0], color='k', label='Minimum snow cover median')\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].set_ylabel('Transient accumulation area ratio')\n",
    "ax[1].set_ylabel('Snow covered area [km$^2$]')\n",
    "ax[2].set_ylabel('Snowline altitude [m]')\n",
    "for axis in ax:\n",
    "    axis.set_xlabel('Week of year')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'weekly_median_trends_example.png')\n",
    "# save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db3d9b-f9b5-4c28-8ad2-156f59b64e56",
   "metadata": {},
   "source": [
    "## AGU24 figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccff3b6",
   "metadata": {},
   "source": [
    "### Abstract figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc3a0c-69fc-44f1-91c2-29989208a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('mako', n_colors=len(subregion_order)+2)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'results', 'min_snow_cover_stats.csv')\n",
    "min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "# Sort subregions\n",
    "min_snow_cover_stats['order'] = ''\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'order'] = i\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'color'] = matplotlib.colors.to_hex(cmap[i])\n",
    "min_snow_cover_stats = min_snow_cover_stats.sort_values(by='order')\n",
    "print('Median AARs loaded from file')\n",
    "\n",
    "# # -----Load RGI O2 Regions\n",
    "# rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "#                                 'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "# rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# # remove Brooks Range\n",
    "# rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# # add subregion name and color column\n",
    "# rgi_O2[['Subregion', 'color']] = '', ''\n",
    "# for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "#     o1 = int(o1o2[0:2])\n",
    "#     o2 = int(o1o2[3:])\n",
    "#     subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'color'] = dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values)[subregion]\n",
    "# print('RGI O2 regions loaded from file')\n",
    "\n",
    "# # -----Load GTOPO30\n",
    "# gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "# gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "# gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "# print('GTOPO30 loaded from file')\n",
    "\n",
    "# # -----Load classified image\n",
    "# site_name = 'RGI60-01.00037'\n",
    "# im_classified_fn = f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/imagery/classified/20230802T152742_RGI60-01.00037_Sentinel-2_SR_classified.nc'\n",
    "# im_classified = xr.open_dataset(im_classified_fn)\n",
    "# print('Classified image loaded')\n",
    "\n",
    "# # -----Load classified images colormap\n",
    "# import json\n",
    "# datasets_dict_fn = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/inputs-outputs/datasets_characteristics.json'\n",
    "# datasets_dict = json.load(open(datasets_dict_fn))\n",
    "# cmap_classified = matplotlib.colors.ListedColormap(datasets_dict['classified_image']['class_colors'].values())\n",
    "\n",
    "# # -----Load Sentinel-2 image from GEE\n",
    "# import math\n",
    "# import wxee as wx\n",
    "# import geedim as gd\n",
    "# import ee\n",
    "# ee.Initialize()\n",
    "# def convert_wgs_to_utm(lon: float, lat: float):\n",
    "#     utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "#     if len(utm_band) == 1:\n",
    "#         utm_band = '0' + utm_band\n",
    "#     if lat >= 0:\n",
    "#         epsg_code = '326' + utm_band\n",
    "#         return epsg_code\n",
    "#     epsg_code = '327' + utm_band\n",
    "#     return epsg_code\n",
    "# # Load AOI\n",
    "# aoi_fn = glob.glob(f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/AOIs/*.shp')[0]\n",
    "# aoi = gpd.read_file(aoi_fn)\n",
    "# aoi_bounds = aoi.geometry[0].bounds\n",
    "# region = ee.Geometry.Polygon([[aoi_bounds[0], aoi_bounds[1]], \n",
    "#                               [aoi_bounds[2], aoi_bounds[1]],\n",
    "#                               [aoi_bounds[2], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[1]]])\n",
    "# # Load image collection\n",
    "# im_col = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date='2023-08-01',\n",
    "#                                                                              end_date='2023-08-03',\n",
    "#                                                                              region=region,\n",
    "#                                                                              mask=True)\n",
    "# im_ee = im_col.ee_collection.first()\n",
    "# im_ee = im_ee.clip(region)\n",
    "# im_ee = im_ee.select(['B4', 'B3', 'B2'])\n",
    "# # Convert to xarray.Dataset\n",
    "# im_xr = im_ee.wx.to_xarray(scale=30, crs='EPSG:4326')\n",
    "# im_xr = xr.where(im_xr==im_xr.attrs['_FillValue'], np.nan, im_xr / 1e4)\n",
    "# im_xr = im_xr.rio.write_crs('EPSG:4326')\n",
    "# print('Sentinel-2 SR image loaded')\n",
    "\n",
    "# # Reproject AOI and images to optimal UTM zone\n",
    "# epsg_utm = convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "# aoi_utm = aoi.to_crs(f'EPSG:{epsg_utm}')\n",
    "# im_xr = im_xr.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = im_classified.rio.write_crs(\"EPSG:4326\")\n",
    "# im_classified = im_classified.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = xr.where(im_classified < 1, np.nan, im_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa0d1b-715d-45e8-bdb3-593af6912041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=14\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": fontsize,\n",
    "    \"font.sans-serif\": \"Arial\",\n",
    "    # \"font.family\": \"sans-serif\",\n",
    "    # \"font.sans-serif\": \"Computer Modern Sans Serif\",\n",
    "    \"text.usetex\": False\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[1,2])\n",
    "ax = [fig.add_subplot(gs[2:]),\n",
    "      fig.add_subplot(gs[0]),\n",
    "      fig.add_subplot(gs[1])]\n",
    "\n",
    "# ----- Study sites\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.2)\n",
    "# Median AARs\n",
    "sns.scatterplot(data=min_snow_cover_stats, x='CenLon', y='CenLat', edgecolor='w', linewidth=0.5, \n",
    "                hue='Subregion', hue_order=subregion_order, palette=dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values), \n",
    "                alpha=1, size='AAR_P50_min', sizes=(2,100), ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "Ikeep = np.argwhere(['0.' in x for x in np.array(labels)]).flatten()\n",
    "handles, labels = [handles[i] for i in Ikeep], [labels[i] for i in Ikeep] \n",
    "ax[0].legend(handles, labels, loc='lower left', title='2013–2023 median AAR', bbox_to_anchor=[0.2, 0.05, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "# Example site location\n",
    "min_snow_cover_stats_site = min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name]\n",
    "ax[0].plot(min_snow_cover_stats_site['CenLon'], min_snow_cover_stats_site['CenLat'], '*', \n",
    "           markeredgecolor='k', markerfacecolor='#e7298a', markersize=15, linewidth=2)\n",
    "    \n",
    "# -----b) Sentinel-2 image\n",
    "ax[1].imshow(np.dstack([im_xr.B4.data[0], im_xr.B3.data[0], im_xr.B2.data[0]]),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, \n",
    "                     np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('Northing [km]')\n",
    "\n",
    "# -----c) Classified image\n",
    "ax[2].imshow(im_classified.classified.data[0], cmap=cmap_classified, clim=(1,5),\n",
    "             extent=(np.min(im_classified.x.data)/1e3, np.max(im_classified.x.data)/1e3,\n",
    "                     np.min(im_classified.y.data)/1e3, np.max(im_classified.y.data)/1e3))\n",
    "ax[2].plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "           '-k', label='Glacier boundary')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "# dummy points for legend\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(0), label='Snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(1), label='Shadowed snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(2), label='Ice/firn')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(3), label='Rock/debris')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(4), label='Water')\n",
    "ax[2].set_xlim(ax[1].get_xlim())\n",
    "ax[2].set_ylim(ax[1].get_ylim())\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncols=6, markerscale=2, frameon=False)\n",
    "\n",
    "# Plot AOI\n",
    "for axis in ax[1:]:\n",
    "    axis.plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), \n",
    "              np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "              '-k', linewidth=1, label='Glacier boundary')\n",
    "    axis.set_yticks(np.arange(7030, 7046, step=5))\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['c', 'a', 'b']\n",
    "for i in range(0, len(ax)):\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0]) * 0.9 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0]) * 0.85 + ax[i].get_ylim()[0],\n",
    "                text_labels[i], fontweight='bold', fontsize=fontsize+4, horizontalalignment='center',\n",
    "              bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "\n",
    "# # Add caption\n",
    "# caption = (r\"\\noindent\\textbf{Figure 1. a)} Sentinel-2 surface reflectance image captured 2023-08-02 for one glacier (Randolph Glacier Inventory ID = 1.00037) \\\\\"\n",
    "#             r\"and the associated \\textbf{b)} classified image generated from the automated snow detection pipeline. \\textbf{c)} Map of the study glacier locations, \\\\\"\n",
    "#             r\"with marker sizes indicating the median accumulation area ratio (AAR) for the 2013–2023 study period. The maroon start marks the \\\\\"\n",
    "#             r\"location of the example glacier shown in panels \\textbf{a} and \\textbf{b}.\" )\n",
    "# fig.text(0.05, -0.02, caption, ha='left', wrap=True, fontsize=fontsize+1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'agu24_abstract_figure.png')\n",
    "save_figure(fig, fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092b8ae",
   "metadata": {},
   "source": [
    "### Snow cover GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb687748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Load inputs\n",
    "rgi_id = \"RGI60-01.00312\"\n",
    "aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "im_classified_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'classified', '*.nc')))\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "# subset to 2019\n",
    "im_classified_fns = [x for x in im_classified_fns if int(os.path.basename(x)[0:4]) == 2019] \n",
    "scs = scs.loc[scs['datetime'].dt.year == 2019]\n",
    "out_path = os.path.join(figures_out_path, 'timeseries_gif')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "\n",
    "# Define colormap for classified images\n",
    "cmap_dict = {\"Snow\": \"#4eb3d3\",  \"Shadowed_snow\": \"#636363\", \"Ice\": \"#084081\", \"Rock\": \"#fe9929\", \"Water\": \"#252525\"}\n",
    "colors = []\n",
    "for key in list(cmap_dict.keys()):\n",
    "    color = list(matplotlib.colors.to_rgb(cmap_dict[key]))\n",
    "    if key=='Rock':\n",
    "        color += [0.5]\n",
    "    colors.append(color)\n",
    "        \n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605f4f7-37a4-4238-b334-cb2787c4e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download images for 2019 ###\n",
    "\n",
    "import math\n",
    "import ee\n",
    "import geedim as gd\n",
    "import datetime\n",
    "from rasterio.features import geometry_mask\n",
    "    \n",
    "def query_gee_for_imagery_run_pipeline(dataset, aoi_utm, date_start, date_end,\n",
    "                                       month_start, month_end, site_name, \n",
    "                                       mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=None,\n",
    "                                       verbose=False, im_download=False):\n",
    "\n",
    "    # -----Grab optimal UTM zone from AOI CRS\n",
    "    epsg_utm = str(aoi_utm.crs.to_epsg())\n",
    "\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "\n",
    "    # -----Define function to query GEE for imagery\n",
    "    def query_gee(dataset, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        if dataset == 'Landsat8':\n",
    "            # Landsat 8\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Landsat9':\n",
    "            # Landsat 9\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC09/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Sentinel-2_TOA':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                         end_date=date_end,\n",
    "                                                                                         region=region,\n",
    "                                                                                         cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                         mask=mask_clouds)\n",
    "\n",
    "        elif dataset == 'Sentinel-2_SR':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                            end_date=date_end,\n",
    "                                                                                            region=region,\n",
    "                                                                                            cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                            mask=mask_clouds)\n",
    "        else:\n",
    "            print(\"'dataset' variable not recognized. Please set to 'Landsat', 'Sentinel-2_TOA', or 'Sentinel-2_SR'. \"\n",
    "                  \"Exiting...\")\n",
    "            return 'N/A'\n",
    "\n",
    "        return im_col_gd\n",
    "\n",
    "    # -----Define function to filter image IDs by month range\n",
    "    def filter_im_ids_month_range(im_ids, im_dts, month_start, month_end):\n",
    "        i = [int(ii) for ii in np.arange(0, len(im_dts)) if\n",
    "             (im_dts[ii].month >= month_start) and (im_dts[ii].month <= month_end)]  # indices of images to keep\n",
    "        im_ids, im_dts = [im_ids[ii] for ii in i], [im_dts[ii] for ii in i]  # subset of image IDs and datetimes\n",
    "        # return 'N/A' if no images remain after filtering by month range\n",
    "        if len(im_dts) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        return im_ids, im_dts\n",
    "\n",
    "    # -----Define function to couple image IDs captured within the same hour for mosaicking\n",
    "    def image_mosaic_ids(im_col_gd):\n",
    "        # Grab image properties, IDs, and datetimes from image collection\n",
    "        properties = im_col_gd.properties\n",
    "        ims = dict(properties).keys()\n",
    "        im_ids = [properties[im]['system:id'] for im in ims]\n",
    "        # return if no images found\n",
    "        if len(im_ids) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        im_dts = np.array(\n",
    "            [datetime.datetime.utcfromtimestamp(properties[im]['system:time_start'] / 1000) for im in ims])\n",
    "\n",
    "        # Remove image datetimes and IDs outside the specified month range\n",
    "        im_ids, im_dts = filter_im_ids_month_range(im_ids, im_dts, month_start, month_end)\n",
    "\n",
    "        # Grab all unique hours in image datetimes\n",
    "        hours = np.array(im_dts, dtype='datetime64[h]')\n",
    "        unique_hours = sorted(set(hours))\n",
    "\n",
    "        # Create list of IDs for each unique hour\n",
    "        im_mosaic_ids_list, im_mosaic_dts_list = [], []\n",
    "        for unique_hour in unique_hours:\n",
    "            i = list(np.ravel(np.argwhere(hours == unique_hour)))\n",
    "            im_ids_list_hour = [im_ids[ii] for ii in i]\n",
    "            im_mosaic_ids_list.append(im_ids_list_hour)\n",
    "            im_dts_list_hour = [im_dts[ii] for ii in i]\n",
    "            im_mosaic_dts_list.append(im_dts_list_hour)\n",
    "\n",
    "        return im_mosaic_ids_list, im_mosaic_dts_list\n",
    "\n",
    "    # -----Define function for extracting valid image IDs\n",
    "    def extract_valid_image_ids(ds, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        # Initialize list of date ranges for querying\n",
    "        date_ranges = [(date_start, date_end)]\n",
    "        # Initialize list of error dates\n",
    "        error_dates = []\n",
    "        # Initialize error flag\n",
    "        error_occurred = True\n",
    "        # Iterate until no errors occur\n",
    "        while error_occurred:\n",
    "            error_occurred = False  # Reset the error flag at the beginning of each iteration\n",
    "            try:\n",
    "                # Initialize list of image collections\n",
    "                im_col_gd_list = []\n",
    "                # Iterate over date ranges\n",
    "                for date_range in date_ranges:\n",
    "                    # Query GEE for imagery\n",
    "                    im_col_gd = query_gee(ds, date_range[0], date_range[1], region, cloud_cover_max, mask_clouds)\n",
    "                    properties = im_col_gd.properties  # Error will occur here if an image is inaccessible!\n",
    "                    im_col_gd_list.append(im_col_gd)\n",
    "                # Initialize list of filtered image IDs and datetimes\n",
    "                im_mosaic_ids_list_full, im_mosaic_dts_list_full = [], []  # Initialize lists of\n",
    "                # Filter image IDs for month range and couple IDs for mosaicking\n",
    "                for im_col_gd in im_col_gd_list:\n",
    "                    im_mosaic_ids_list, im_mosaic_dts_list = image_mosaic_ids(im_col_gd)\n",
    "                    if type(im_mosaic_ids_list) is str:\n",
    "                        return 'N/A', 'N/A'\n",
    "                    # append to list\n",
    "                    im_mosaic_ids_list_full = im_mosaic_ids_list_full + im_mosaic_ids_list\n",
    "                    im_mosaic_dts_list_full = im_mosaic_dts_list_full + im_mosaic_dts_list\n",
    "\n",
    "                return im_mosaic_ids_list_full, im_mosaic_dts_list_full\n",
    "\n",
    "            except Exception as e:\n",
    "                error_id = str(e).split('ID=')[1].split(')')[0]\n",
    "                print(f\"Error querying GEE for {str(error_id)}\")\n",
    "\n",
    "                # Parse the error date from the exception message (replace this with your actual parsing logic)\n",
    "                error_date = datetime.datetime.strptime(error_id[0:8], '%Y%m%d')\n",
    "                error_dates.append(error_date)\n",
    "\n",
    "                # Update date ranges excluding the problematic date\n",
    "                date_starts = [date_start] + [str(error_date + datetime.timedelta(days=1))[0:10] for error_date in\n",
    "                                              error_dates]\n",
    "                date_ends = [str(error_date - datetime.timedelta(days=1))[0:10] for error_date in error_dates] + [\n",
    "                    date_end]\n",
    "                date_ranges = list(zip(date_starts, date_ends))\n",
    "\n",
    "                # Set the error flag to indicate that an error occurred\n",
    "                error_occurred = True\n",
    "\n",
    "    # -----Apply functions\n",
    "    if dataset == 'Landsat':  # must run Landsat 8 and 9 separately\n",
    "        im_ids_list_8, im_dts_list_8 = extract_valid_image_ids('Landsat8', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        im_ids_list_9, im_dts_list_9 = extract_valid_image_ids('Landsat9', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        if (type(im_ids_list_8) is str) and (type(im_ids_list_9) is str):\n",
    "            im_ids_list, im_dts_list = 'N/A', 'N/A'\n",
    "        elif type(im_ids_list_9) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_8, im_dts_list_8\n",
    "        elif type(im_ids_list_8) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_9, im_dts_list_9\n",
    "        else:\n",
    "            im_ids_list = im_ids_list_8 + im_ids_list_9\n",
    "            im_dts_list = im_dts_list_8 + im_dts_list_9\n",
    "    else:\n",
    "        im_ids_list, im_dts_list = extract_valid_image_ids(dataset, date_start, date_end, region, cloud_cover_max,\n",
    "                                                           mask_clouds)\n",
    "\n",
    "    # -----Check if any images were found after filtering\n",
    "    if type(im_ids_list) is str:\n",
    "        print('No images found or error in one or more image IDs, exiting...')\n",
    "        return 'N/A'\n",
    "\n",
    "    if dataset=='Landsat':\n",
    "        res = 30\n",
    "        image_scalar = 36363.63636363636\n",
    "        no_data_value = 0\n",
    "    elif 'Sentinel-2' in dataset:\n",
    "        res = 10\n",
    "        image_scalar = 1e4\n",
    "        no_data_value = -9999\n",
    "\n",
    "    # -----Create xarray.Datasets from list of image IDs\n",
    "    # loop through image IDs\n",
    "    for i in tqdm(range(0, len(im_ids_list))):\n",
    "\n",
    "        # subset image IDs and image datetimes\n",
    "        im_ids, im_dts = im_ids_list[i], im_dts_list[i]\n",
    "\n",
    "        # make directory for outputs (out_path) if it doesn't exist\n",
    "        if not os.path.exists(im_out_path):\n",
    "            os.mkdir(im_out_path)\n",
    "            print('Made directory for image downloads: ' + im_out_path)\n",
    "            \n",
    "        # define filename\n",
    "        if len(im_dts) > 1:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '_MOSAIC.tif'\n",
    "        else:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '.tif'\n",
    "            \n",
    "        # check file does not already exist in directory, download\n",
    "        if not os.path.exists(os.path.join(im_out_path, im_fn)):\n",
    "            # create list of MaskedImages from IDs\n",
    "            im_gd_list = [gd.MaskedImage.from_id(im_id) for im_id in im_ids]\n",
    "            # combine into new MaskedCollection\n",
    "            im_collection = gd.MaskedCollection.from_list(im_gd_list)\n",
    "            # create image composite\n",
    "            im_composite = im_collection.composite(method=gd.CompositeMethod.q_mosaic,\n",
    "                                                   mask=mask_clouds,\n",
    "                                                   region=region)\n",
    "            # download to file\n",
    "            im_composite.download(os.path.join(im_out_path, im_fn),\n",
    "                                  region=region,\n",
    "                                  scale=res,\n",
    "                                  crs='EPSG:4326',\n",
    "                                  dtype='int16',\n",
    "                                  bands=im_composite.refl_bands)\n",
    "\n",
    "    return\n",
    "\n",
    "# Reproject AOI to optimal UTM zone\n",
    "epsg_utm = f.convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "aoi_utm = aoi.to_crs(epsg_utm)\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# Landsat\n",
    "dataset = 'Landsat'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_SR'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Remove images with < 70% coverage of AOI\n",
    "fns = sorted(glob.glob(os.path.join(out_path, '*.tif')))\n",
    "fn_remove_list = []\n",
    "for fn in tqdm(fns):\n",
    "    im = rxr.open_rasterio(fn).squeeze()\n",
    "    im = im.rio.reproject(epsg_utm)\n",
    "    im = im.isel(band=0)\n",
    "    mask = geometry_mask(aoi_utm.geometry, transform=im.rio.transform(), invert=True, out_shape=im.shape)\n",
    "    im_masked = xr.where(mask==1, im, np.nan)\n",
    "    im_masked = xr.where((im_masked==-9999) | (im_masked==-32768), np.nan, im_masked)\n",
    "    nreal = im_masked.notnull().sum().item()\n",
    "    npx = np.count_nonzero(mask)\n",
    "    percentage_real = (nreal / npx) * 100\n",
    "    if percentage_real < 70:\n",
    "        fn_remove_list.append(fn)\n",
    "# for fn in fn_remove_list:\n",
    "#     os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over classified images\n",
    "for im_classified_fn in tqdm(im_classified_fns):\n",
    "    # Load classified image\n",
    "    im_classified = rxr.open_rasterio(im_classified_fn, decode_times=False).squeeze()\n",
    "    im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "    im_classified = xr.where(im_classified==2, 1, im_classified)\n",
    "    date = pd.Timestamp(datetime.datetime.strptime(os.path.basename(im_classified_fn).split('_')[0], \"%Y%m%dT%H%M%S\"))\n",
    "    source = os.path.basename(im_classified_fn).split(rgi_id + '_')[1].split('_classified.nc')[0]\n",
    "\n",
    "    # Load multispec image\n",
    "    try:\n",
    "        im_fn = glob.glob(os.path.join(out_path, f\"{source}_{str(date)[0:10].replace('-', '')}*.tif\"))[0]\n",
    "    except:\n",
    "        continue\n",
    "    im = rxr.open_rasterio(im_fn).squeeze()\n",
    "    if source=='Landsat':\n",
    "        image_scaler = 36363.63636363636\n",
    "    else:\n",
    "        image_scaler = 1e4\n",
    "    im = xr.where((im==-9999) | (im==-32768), np.nan, im / image_scaler)\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[2,1])\n",
    "    ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[1,:])]\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im.isel(band=2).data, im.isel(band=1).data, im.isel(band=0).data]),\n",
    "                 extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    aoi.plot(ax=ax[0], facecolor='None', edgecolor='k')\n",
    "    # classified image\n",
    "    xmin, xmax = -145.3555, -145.045\n",
    "    ymin, ymax = 63.144, 63.325\n",
    "    ax[1].imshow(im_classified.data, cmap=cmap, clim=(1,5), \n",
    "                 extent=(xmin, xmax, ymax, ymin))\n",
    "    ax[1].invert_yaxis()\n",
    "    aoi.plot(ax=ax[1], facecolor='None', edgecolor='k')\n",
    "    # dummy points for legend\n",
    "    ax[1].plot(0, 0, 's', color=colors[0], markersize=12, label='Snow')\n",
    "    ax[1].plot(0, 0, 's', color=colors[2], markersize=12, label='Ice')\n",
    "    ax[1].plot(0, 0, 's', color=colors[3], markersize=12, label='Rock')\n",
    "    ax[1].plot(0, 0, 's', color=colors[4], markersize=12, label='Water')\n",
    "    ax[1].legend(loc='lower right', frameon=False)\n",
    "    for axis in ax[0:2]:\n",
    "        axis.set_xlim(xmin, xmax)\n",
    "        axis.set_ylim(ymin, ymax)\n",
    "        axis.set_xticks(axis.get_xticks()[1::2])\n",
    "        axis.set_yticks(axis.get_yticks()[1::2])\n",
    "\n",
    "    # AAR time series\n",
    "    ax[2].plot(scs['datetime'], scs['AAR'], '.k')\n",
    "    scs_date = scs.loc[(scs['datetime']==date) & (scs['source']==source)]\n",
    "    ax[2].plot(scs_date['datetime'], scs_date['AAR'], '*m', markersize=15)\n",
    "    ax[2].set_ylim(0,1.05)\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_ylabel('Snow area ratio')\n",
    "    fig.suptitle(f\"{date}\\n{source.replace('_', ' ')}\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig_fn = os.path.join(out_path, f\"{date}_{rgi_id}_snow_cover.png\")\n",
    "    save_figure(fig, fig_fn)\n",
    "    plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7960ae",
   "metadata": {},
   "source": [
    "### Map of study sites with climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load climate clusters / mean climate\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Climate clusters loaded')\n",
    "\n",
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add climate cluster column\n",
    "aois = aois.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "aois.rename(columns={'clustName': 'Climate class'}, inplace=True)\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax.imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax, alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax.set_yticks(np.linspace(45, 65, num=6))\n",
    "ax.set_xlim(-167, -112)\n",
    "ax.set_ylim(46, 66.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_aspect(2.1)\n",
    "# Site locations\n",
    "obj = sns.scatterplot(data=aois, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, \n",
    "                      hue='Climate class', hue_order=cluster_order, palette=cluster_cmap_dict, alpha=1, ax=ax)\n",
    "sns.move_legend(obj, loc='lower left', markerscale=2, bbox_to_anchor=[0.15, 0.1, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax.text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'agu24_study_sites_map.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f05c0",
   "metadata": {},
   "source": [
    "### Model SLA animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "\n",
    "# Define output directory\n",
    "out_path = os.path.join(figures_out_path, 'model_SMB_to_SLA_animation')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "    \n",
    "# Grab modeled SMB file names\n",
    "rgi_id = '1.00032'\n",
    "bin_fn = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', f'{rgi_id}*.nc')))[0]\n",
    "\n",
    "# Load binned data\n",
    "bin = xr.open_dataset(bin_fn)\n",
    "# grab data variables\n",
    "h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "x = bin.bin_distance.data[0]\n",
    "b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "# iterate over each time period after 2013\n",
    "times = [time for time in times if time >= np.datetime64('2012-10-01')]\n",
    "elas = np.nan*np.zeros(len(times)) # initialize transient ELAs\n",
    "for j, time in enumerate(tqdm(times)):\n",
    "    # subset binned data to time\n",
    "    bin_time = bin.isel(time=j)\n",
    "    # grab the SMB \n",
    "    b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "    # add the previous SMB (restart the count in October)\n",
    "    if months[j] != 10: \n",
    "        b_sum[j,:] += b_sum[j-1,:]\n",
    "    # If all SMB > 0, ELA = minimum elevation\n",
    "    if all(b_sum[j,:] > 0):\n",
    "        elas[j] = np.min(h)\n",
    "    # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "    elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "        elas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "    # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "    elif all(b_sum[j,:] < 0):\n",
    "        X, y = b_sum[j,:], h\n",
    "        elas[j] = np.nanmax(h)\n",
    "    else:\n",
    "        print('issue')\n",
    "        \n",
    "    # Plot results\n",
    "    if time >= np.datetime64('2013-01-01'):\n",
    "        fig, ax = plt.subplots(2, 1, gridspec_kw=dict(height_ratios=[3,1]), figsize=(6,6))\n",
    "        # surface profile\n",
    "        ax[0].fill_between(x, np.zeros(len(x)), h, color='gray', edgecolor='k', alpha=0.5)\n",
    "        positive_mask = b_sum[j,:] > 0\n",
    "        negative_mask = b_sum[j,:] < 0\n",
    "        # positive mass balance bars\n",
    "        ax[0].bar(x[positive_mask], b_sum[j,positive_mask]*50, width=126, \n",
    "            bottom=h[positive_mask], color='blue', alpha=0.6, label='Positive Mass Balance')\n",
    "        # negative mass balance bars\n",
    "        ax[0].bar(x[negative_mask], b_sum[j,negative_mask]*50, width=126, \n",
    "                bottom=h[negative_mask], color='red', alpha=0.6, label='Negative Mass Balance')\n",
    "        # SLA\n",
    "        ax[0].axhline(elas[j], 0, np.nanmax(x), color='k')\n",
    "        ax[0].text(7e3, elas[j]+50, 'Snowline altitude', color='k', ha='right')\n",
    "        ax[0].set_ylim(1.25e3, 3.3e3)\n",
    "        ax[0].set_yticks([1500, 2000, 2500, 3000])\n",
    "        ax[0].set_ylabel('Elevation [m]')\n",
    "        ax[0].set_xlim(0, np.nanmax(x))\n",
    "        ax[0].set_xticks(np.arange(0, 7.1e3, step=1e3))\n",
    "        ax[0].set_xticklabels(np.divide(ax[0].get_xticks(), 1e3).astype(int).astype(str))\n",
    "        ax[0].set_xlabel('km')\n",
    "        ax[0].spines[['top', 'right']].set_visible(False)\n",
    "        ax[0].set_title(str(time)[0:7])\n",
    "        # SLA time series\n",
    "        ax[1].plot(times, elas, '.k', markersize=5)\n",
    "        ax[1].plot(times[j], elas[j], '*m', markersize=10)\n",
    "        ax[1].set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "        ax[1].set_ylim(1330, 2600)\n",
    "        ax[1].set_ylabel('Snowline altitude [m]')\n",
    "        fig.tight_layout()\n",
    "        # Save figure\n",
    "        fig_fn = os.path.join(out_path, f\"{str(time)[0:7]}.png\")\n",
    "        save_figure(fig, fig_fn)\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Site location map\n",
    "\n",
    "# # Load GTOPO30\n",
    "# gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "# gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "# gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "# print('GTOPO30 loaded')\n",
    "# Load country outlines\n",
    "countries_fn = '/Users/raineyaberle/Research/PhD/GIS_data/countries_shp/countries.shp'\n",
    "countries = gpd.read_file(countries_fn)\n",
    "countries = countries.loc[(countries['NAME']=='Canada') | (countries['NAME']=='United States')]\n",
    "\n",
    "# Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax.imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# country outlines\n",
    "countries.plot(ax=ax, facecolor='None', edgecolor='gray')\n",
    "# Glaciers\n",
    "# aois.plot(facecolor='#08519c', edgecolor='None', ax=ax)\n",
    "# Site location\n",
    "rgi_id = 'RGI60-01.00032'\n",
    "site_centroid = aois.loc[aois['RGIId']==rgi_id].geometry[0].centroid.coords.xy\n",
    "ax.plot(*site_centroid, '*', markersize=40, markerfacecolor='m', markeredgecolor='k', linewidth=0.5)\n",
    "\n",
    "ax.set_xlim(-170, -120)\n",
    "ax.set_ylim(45, 71)\n",
    "ax.set_aspect(2.1)\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'model_SMB_to_SLA_animation', 'site_location_map.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27009dcf",
   "metadata": {},
   "source": [
    "## Grad Student Showcase 2025: AARs timings and magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('Glacier boundaries loaded')\n",
    "\n",
    "# -----Load climate clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_scs_fn = os.path.join(out_path, 'minimum_snow_cover_stats.csv')\n",
    "min_scs = pd.read_csv(min_scs_fn)\n",
    "# Add difference from September AAR\n",
    "min_scs['AAR_Sept-obs'] = min_scs['AAR_Sept'] - min_scs['AAR_median']\n",
    "# Add Subregion and climate cluster info\n",
    "min_scs[['CenLon', 'CenLat', 'Subregion', 'clustName']] = 0, 0, '', ''\n",
    "for rgi_id in min_scs['RGIId'].drop_duplicates().values:\n",
    "    cenlon, cenlat, subregion = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']].values[0]\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    min_scs.loc[min_scs['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion', 'clustName']] = cenlon, cenlat, subregion, cluster\n",
    "# Sort by subregion order\n",
    "min_scs['Subregion'] = pd.Categorical(min_scs['Subregion'], subregion_order)\n",
    "min_scs.sort_values(by='Subregion', inplace=True)\n",
    "print('Median AARs loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b581603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=14\n",
    "lw = 1.5\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif':'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(10,2, wspace=0.1, hspace=0.0)\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = []\n",
    "\n",
    "# Iterate over subregions\n",
    "median_color = 'w'\n",
    "fill_color = '#2477BF'\n",
    "Iax = -1\n",
    "for i, subregion in enumerate(min_scs['Subregion'].unique()):\n",
    "    min_scs_subregion = min_scs.loc[min_scs['Subregion']==subregion]\n",
    "    \n",
    "    # a) AAR timings\n",
    "    ax.append(fig.add_subplot(gs[i,0]))\n",
    "    Iax += 1\n",
    "    k = sns.kdeplot(min_scs_subregion['WOY_median'], vertical=False, color=fill_color, \n",
    "                    fill=True, edgecolor='k', linewidth=lw, alpha=1, ax=ax[Iax], zorder=2)\n",
    "    median = min_scs_subregion['WOY_median'].median()\n",
    "    ax[Iax].plot([median, median], [0, ax[Iax].get_ylim()[1]*0.9], '-', color='w', linewidth=lw+0.5, zorder=3)\n",
    "    ax[Iax].plot([14,45], [0,0], '-', color='k', linewidth=2)\n",
    "    ax[Iax].set_xlim(24,47)\n",
    "    ax[Iax].set_ylabel('')\n",
    "    ax[Iax].set_xticks([])\n",
    "    ax[Iax].set_xlabel('')\n",
    "    if i==9:\n",
    "        ax[Iax].set_xticks([26, 31, 35, 39, 44])\n",
    "        ax[Iax].set_xticklabels(['Jul', 'Aug', 'Sep', 'Oct', 'Nov'])\n",
    "        ax[Iax].set_xlabel('Time of year')\n",
    "    if i==0:\n",
    "        ax[Iax].set_title('a) Snow minima timings', fontsize=fontsize+4, color='k')\n",
    "        ax[Iax].legend(loc='upper center', frameon=False, bbox_to_anchor=[0.4, 1.6, 0.2, 0.2])\n",
    "    ax[Iax].spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "    ax[Iax].set_ylabel(subregion, ha='right', va='center', color='k', rotation=0)\n",
    "    ax[Iax].set_yticks([ax[Iax].get_ylim()[1]*0.5])\n",
    "    ax[Iax].set_yticklabels([])\n",
    "\n",
    "    # b) AAR magnitude differences\n",
    "    ax.append(fig.add_subplot(gs[i,1]))\n",
    "    Iax += 1\n",
    "    sns.boxplot(data=min_scs_subregion, x='AAR_Sept-obs', y='Subregion', showfliers=False, ax=ax[Iax],\n",
    "                boxprops=dict(edgecolor='k', linewidth=lw, facecolor=fill_color),\n",
    "                medianprops=dict(color='w', linewidth=lw-0.5), \n",
    "                whiskerprops=dict(color='k', linewidth=lw), \n",
    "                capprops=dict(color='k', linewidth=lw))\n",
    "    ax[Iax].set_xlim(-0.05,0.6)\n",
    "    ax[Iax].set_ylim(i-0.5, i+0.5)  \n",
    "    ax[Iax].spines[['right', 'top']].set_visible(False)\n",
    "    if i > 0:\n",
    "        ax[Iax].spines['top'].set_visible(True)\n",
    "        ax[Iax].spines['top'].set_color('gray')\n",
    "    handles, labels = ax[Iax].get_legend_handles_labels()\n",
    "    ax[Iax].legend().remove()\n",
    "    if i==0:\n",
    "        ax[Iax].set_title('b) September-only $-$ Observed AARs', fontsize=fontsize+4, color='k')\n",
    "        labels = ['Observed', 'September-only']\n",
    "        ax[Iax].legend(handles, labels, loc='upper center', ncols=1, frameon=False, \n",
    "                       bbox_to_anchor=[0.9, 0.9, 0.2, 0.2])\n",
    "    ax[Iax].set_ylabel('')\n",
    "    ax[Iax].set_yticklabels([])\n",
    "    if i < 9:\n",
    "        ax[Iax].set_xticks([])\n",
    "        ax[Iax].set_xlabel('')\n",
    "        ax[Iax].spines['bottom'].set_color('gray')\n",
    "    else:\n",
    "        ax[Iax].set_xlabel('AAR difference')\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig02_median_aars+timings_swapped_larger-text.png')\n",
    "save_figure(fig, fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920d061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca",
   "language": "python",
   "name": "gsca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
