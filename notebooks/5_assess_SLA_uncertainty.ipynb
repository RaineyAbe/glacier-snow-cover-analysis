{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess snowline altitude uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis/'\n",
    "sys.path.append(os.path.join(code_path, 'functions'))\n",
    "import utils as f\n",
    "\n",
    "# Define path to study sites\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping'\n",
    "\n",
    "# Define path for outputs\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "\n",
    "# Get names of study sites\n",
    "rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "rgi_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate SLA bounds (uncertainty) for all sites and classified images\n",
    "\n",
    "The original SLA was calculated by sampling the $1-AAR$ percentile of the DEM. For example, if the AAR is 0.8, the SLA is calculated as the 20th percentile of elevations over the glacier area. \n",
    "\n",
    "$P_{SLA} = 1-AAR$\n",
    "\n",
    "$SLA = P_{SLA}(DEM)$\n",
    "\n",
    "To estimate upper and lower bounds for SLA, identify \"misclassified\" pixels above and below the SLA, and use those to adjust the SLA percentile. \n",
    "\n",
    "For the upper bound, calculate the area of snow-free pixels above the SLA, convert that to a percentile relative to the total area, and add that to the original SLA percentile. Sample the $P_{upper}$ of the DEM.  \n",
    "\n",
    "$P_{upper} = \\frac{A_{snow free, above SLA}}{A_{glacier}} + P_{SLA}$\n",
    "\n",
    "$SLA_{upper} = P_{upper}(DEM)$\n",
    "\n",
    "For the lower bound, calculate the area of snow-covered pixels below the SLA, convert that to a percentile relative to the total area, and subtract that from the original SLA percentile. Sample the $P_{lower}$ of the DEM. \n",
    "\n",
    "$P_{lower} = -\\frac{A_{snow covered, below SLA}}{A_{glacier}} + P_{SLA}$\n",
    "\n",
    "$SLA_{lower} = P_{lower}(DEM)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file name\n",
    "sla_bounds_fn = os.path.join(out_path, 'SLA_uncertainty_analysis.csv')\n",
    "if not os.path.exists(sla_bounds_fn):\n",
    "    # Initialize results DataFrame\n",
    "    sla_bounds_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over sites\n",
    "    i=0\n",
    "    for rgi_id in rgi_ids:\n",
    "        # Load snow cover stats\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "        # skip if snow cover stats or classified images do not exist\n",
    "        if not os.path.exists(scs_fn):\n",
    "            continue\n",
    "        if not os.path.exists(os.path.join(scm_path, 'study-sites', rgi_id, 'classified')):\n",
    "            continue\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        \n",
    "        # Check if site already has uncertainties columns\n",
    "        if 'SLA_from_AAR_lower_bound_m' not in list(scs.keys()):\n",
    "            # Load DEM\n",
    "            dem_fn = glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'DEMs', '*.tif'))[0]\n",
    "            dem = rxr.open_rasterio(dem_fn).isel(band=0)\n",
    "            dem = xr.where((dem < -1e3) | (dem > 1e4), np.nan, dem)\n",
    "            dem = dem.rio.write_crs(\"EPSG:4326\")\n",
    "        \n",
    "            # Load AOI\n",
    "            aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "            aoi = gpd.read_file(aoi_fn)\n",
    "            aoi = aoi.to_crs(\"EPSG:4326\")\n",
    "            \n",
    "            # Clip DEM to AOI\n",
    "            dem = dem.rio.clip(aoi.geometry)\n",
    "            dem_min = float(dem.min())\n",
    "            dem_max = float(dem.max())\n",
    "    \n",
    "            # Iterate over snow cover observations\n",
    "            sla_originals = np.zeros(len(scs))\n",
    "            sla_lower_bounds = np.zeros(len(scs))\n",
    "            sla_upper_bounds = np.zeros(len(scs))\n",
    "            pbar = tqdm(total=len(scs))\n",
    "            for j in range(len(scs)):\n",
    "                # subset snow cover stats\n",
    "                sc = scs.iloc[j]\n",
    "                \n",
    "                # If AAR==1, classified image was completely snow-covered, so set both bounds to minimum elevation\n",
    "                if sc['AAR']==1:\n",
    "                    sla_lower_bounds[j], sla_upper_bounds[j] = dem_min, dem_min\n",
    "                \n",
    "                # If AAR==0, classified image was completely snow-free, so set both bounds to maximum elevation\n",
    "                elif sc['AAR']==0:\n",
    "                    sla_lower_bounds[j], sla_upper_bounds[j] = dem_max, dem_max\n",
    "                \n",
    "                # Otherwise, calculate bounds using DEM\n",
    "                else:\n",
    "                    # Load classified image file\n",
    "                    dt = sc['datetime']\n",
    "                    source = sc['source']\n",
    "                    classified_fn = glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'classified', \n",
    "                                                        f\"{dt[0:10].replace('-', '')}*_{rgi_id}_{source}_classified.nc\"))\n",
    "                    if len(classified_fn) < 1:\n",
    "                        print(rgi_id, dt, source)\n",
    "                    else:\n",
    "                        classified_fn = classified_fn[0]\n",
    "                    classified = rxr.open_rasterio(classified_fn).squeeze()\n",
    "                    classified = xr.where(classified==-9999, np.nan, classified)\n",
    "                    classified = classified.rio.write_crs(\"EPSG:4326\")\n",
    "                    \n",
    "                    # Create binary snow image\n",
    "                    snow_binary = xr.where((classified==1) | (classified==2), 1, 0)\n",
    "                    snow_binary = xr.where(np.isnan(classified), np.nan, snow_binary) # re-insert no data values\n",
    "                    \n",
    "                    # Regrid DEM to classified image grid\n",
    "                    dem_adj = dem.rio.reproject_match(classified)\n",
    "                    dem_adj = xr.where(dem_adj > 1e4, np.nan, dem_adj)\n",
    "                    dem_adj = dem_adj.rio.write_crs(\"EPSG:4326\")\n",
    "                    \n",
    "                    # Determine spatial resolution based on source\n",
    "                    if source=='Landsat':\n",
    "                        dx = 30\n",
    "                    elif 'Sentinel-2' in source:\n",
    "                        dx = 10\n",
    "                    \n",
    "                    # Calculate lower and upper bounds of snowline altitude\n",
    "                    sla_originals[j], sla_lower_bounds[j], sla_upper_bounds[j] = f.calculate_sla_bounds(sc, dem_adj, snow_binary, dx=dx, verbose=False)\n",
    "\n",
    "                pbar.update(1)\n",
    "            pbar.close()    \n",
    "                \n",
    "            # Add to snow cover stats file\n",
    "            scs['SLA_from_AAR_lower_bound_m'] = sla_lower_bounds\n",
    "            scs['SLA_from_AAR_upper_bound_m'] = sla_upper_bounds\n",
    "            \n",
    "            # Re-save to file\n",
    "            scs.to_csv(scs_fn)\n",
    "\n",
    "            \n",
    "        # Save in DataFrame\n",
    "        df = scs[['RGIId', 'datetime', 'source', 'SLA_from_AAR_m', 'SLA_from_AAR_lower_bound_m', 'SLA_from_AAR_upper_bound_m']]\n",
    "        # Concatenate to results DataFrame\n",
    "        sla_bounds_df = pd.concat([sla_bounds_df, df], axis=0)\n",
    "        \n",
    "        i+=1 \n",
    "        print(f\"{i} / {len(rgi_ids)}\")\n",
    "\n",
    "    # Save results to file\n",
    "    sla_bounds_df.reset_index(drop=True, inplace=True)\n",
    "    sla_bounds_df.to_csv(sla_bounds_fn, index=False)\n",
    "    print('SLA bounds saved to file:', sla_bounds_fn)\n",
    "    \n",
    "else:\n",
    "    sla_bounds_df = pd.read_csv(sla_bounds_fn)\n",
    "\n",
    "# Add column for total range and describe stats\n",
    "sla_bounds_df['SLA_bounds_range_m'] = np.abs(sla_bounds_df['SLA_from_AAR_upper_bound_m'] - sla_bounds_df['SLA_from_AAR_lower_bound_m'])\n",
    "plt.boxplot(sla_bounds_df['SLA_bounds_range_m'], showfliers=False)\n",
    "plt.show()\n",
    "\n",
    "sla_bounds_df['SLA_bounds_range_m'].describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
