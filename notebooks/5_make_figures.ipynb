{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a268cb2-8765-4785-a416-f29c9289f371",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c886bd1-2d50-401f-aaef-8353f32db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from scipy.stats import median_abs_deviation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239e6df1-03f5-472a-8c1c-222f8f92ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import utils as f\n",
    "\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "# scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "eras_fn = os.path.join(scm_path, 'analysis', 'all_era_data.csv')\n",
    "clusters_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7489240-51f6-4b35-8c16-4927ccc6c615",
   "metadata": {},
   "source": [
    "## Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c52145-2f0b-4149-8645-7e855d2118db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climate clusters\n",
    "cluster_cmap_dict = {'W. Aleutians': '#dd3497', \n",
    "                     'Maritime': '#018571',\n",
    "                     'Transitional-Maritime': '#80cdc1',\n",
    "                     'Transitional-Continental': '#dfc27d',\n",
    "                     'Continental': '#a6611a',\n",
    "                     }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16cf12a-9f84-44c5-a7a3-b198eafb234b",
   "metadata": {},
   "source": [
    "## Define order of clusters and subregions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e28883b-0bc0-4d7e-8b92-64d6f5be83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_order = ['W. Aleutians', 'Maritime', 'Transitional-Maritime', 'Transitional-Continental', 'Continental']\n",
    "subregion_order = ['N. Rockies', 'Alaska Range', 'W. Chugach Mtns.', 'St. Elias Mtns.', 'N. Coast Ranges',\n",
    "                   'Aleutians', 'N. Cascades', 'C. Rockies', 'S. Cascades', 'S. Rockies']\n",
    "# separate subregions for plotting\n",
    "group1 = ['N. Rockies', 'W. Chugach Mtns.', 'N. Coast Ranges', 'N. Cascades', 'S. Cascades']\n",
    "group2 = ['Alaska Range', 'St. Elias Mtns.', 'Aleutians', 'C. Rockies', 'S. Rockies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc505172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total number of observations\n",
    "rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "nobs = np.zeros(len(rgi_ids))\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    if rgi_id=='RGI60-01.17761':\n",
    "        im_classified_fns = glob.glob(os.path.join(scm_path, 'study-sites', 'RGI60-01.17761', 'imagery', 'snowlines', '20*.csv'))\n",
    "    else:\n",
    "        im_classified_fns = (glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'classified', '20*.nc'))\n",
    "                             + glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'classified', '20*.nc')))\n",
    "    if len(im_classified_fns) < 1:\n",
    "        print(rgi_id)\n",
    "    nobs[i] = len(im_classified_fns)\n",
    "\n",
    "print(f'Total number of observations = {np.sum(nobs)}')\n",
    "print(f'Mean observations per site = {np.mean(nobs)}')\n",
    "print(f'Median observations per site = {np.median(nobs)}')\n",
    "print(f'Minimum observations per site = {np.min(nobs)}')\n",
    "print(f'Maximum observations per site = {np.max(nobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bed560",
   "metadata": {},
   "source": [
    "## Figure 1. Study sites and climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f99b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load climate clusters / mean climate\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Climate clusters loaded')\n",
    "\n",
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add climate clusters column\n",
    "aois = pd.merge(aois, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75365c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [fig.add_subplot(1,1,1),\n",
    "      fig.add_axes([0.26, 0.24, 0.27, 0.23]),\n",
    "      fig.add_axes([0.82, 0.58, 0.07, 0.07])]\n",
    "\n",
    "### a) Map view\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 66)\n",
    "ax[0].set_xlabel('Longitude ($^{\\circ}$E)')\n",
    "ax[0].set_ylabel('Latitude ($^{\\circ}$N)')\n",
    "ax[0].set_aspect(2.2)\n",
    "# Site locations\n",
    "sns.scatterplot(data=aois, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, s=20,\n",
    "                hue='clustName', palette=cluster_cmap_dict, hue_order=cluster_order, alpha=1, ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().remove()\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "background_color = [1, 1, 1, 0.5]\n",
    "ax[0].text(-160.5, 56.5, f\"Aleutians\\n(N={len(aois.loc[aois['Subregion']=='Aleutians'])})\", ha='center', color=color, rotation=35, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-157, 62.5, f\"Alaska Range\\n(N={len(aois.loc[aois['Subregion']=='Alaska Range'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-147.7, 57.5, f\"W. Chugach \\nMtns.\\n(N={len(aois.loc[aois['Subregion']=='W. Chugach Mtns.'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[0].text(-141.5, 57.9, f\"St. Elias Mtns.\\n(N={len(aois.loc[aois['Subregion']=='St. Elias Mtns.'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.9, 0, 0.6, color=color, linewidth=2, head_width=0.34, head_length=0.2)\n",
    "ax[0].text(-141, 56, f\"N. Coast Ranges\\n(N={len(aois.loc[aois['Subregion']=='N. Coast Ranges'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-137.1, 56.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-133, 50.9, f\"N. Cascades\\n(N={len(aois.loc[aois['Subregion']=='N. Cascades'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-129.9, 51.4, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-129.5, 47, f\"S. Cascades\\n(N={len(aois.loc[aois['Subregion']=='S. Cascades'])})\", ha='center', color=color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].arrow(-126.3, 47.5, 1.2, 0, color=color, linewidth=2, head_width=0.2, head_length=0.3)\n",
    "ax[0].text(-129.5, 64.5, f\"N. Rockies\\n(N={len(aois.loc[aois['Subregion']=='N. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-119, 54.5, f\"C. Rockies\\n(N={len(aois.loc[aois['Subregion']=='C. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "ax[0].text(-115, 47, f\"S. Rockies\\n(N={len(aois.loc[aois['Subregion']=='S. Rockies'])})\", ha='center', color=color, backgroundcolor=background_color, rotation=0, fontsize=fontsize-3, fontweight=fontweight)\n",
    "\n",
    "### b) Mean weather conditions\n",
    "sns.scatterplot(data=clusters, x='mean_annual_temp_range', y='mean_annual_precip_cumsum', s=20,\n",
    "                edgecolor='k', linewidth=0.5, hue='clustName', palette=cluster_cmap_dict, alpha=1, \n",
    "                hue_order=cluster_order, legend=False, ax=ax[1])\n",
    "ax[1].set_xlabel('Air temperature range [$^{\\circ}$C]', fontsize=fontsize-2)\n",
    "ax[1].set_ylabel('Precipitation sum [m]', fontsize=fontsize-2)\n",
    "ax[1].tick_params(labelsize=fontsize-2)\n",
    "\n",
    "# Legend\n",
    "legend = fig.legend(handles, labels, loc='upper right', title='Climate cluster', bbox_to_anchor=[0.7, 0.46, 0.2, 0.2], \n",
    "                    fontsize=fontsize-1, markerscale=2, alignment='left', labelspacing=0.6, framealpha=1)\n",
    "\n",
    "# pie chart inset\n",
    "pie_labels = cluster_order\n",
    "pie_sizes = [len(clusters.loc[clusters['clustName']==label]) for label in pie_labels]\n",
    "ax[2].pie(pie_sizes, colors=list(cluster_cmap_dict.values()), autopct=lambda p: f'{p * sum(pie_sizes) / 100:.0f}', \n",
    "        wedgeprops={'linewidth': 0.5, 'edgecolor': 'k'}, textprops={'fontsize': 6, 'fontweight': 'bold', 'color': 'k'},\n",
    "        pctdistance=0.8)\n",
    "ax[2].set_zorder(legend.get_zorder() + 1)\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['a', 'b']\n",
    "for i, axis in enumerate(ax[0:-1]):\n",
    "    if i==1:\n",
    "        scale = 0.85\n",
    "    else:\n",
    "        scale = 0.93\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*scale + axis.get_xlim()[0],\n",
    "              (axis.get_ylim()[1] - axis.get_ylim()[0])*scale + axis.get_ylim()[0],\n",
    "              text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig01_study_sites_clusters.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6f855-6b1b-46cc-b4a5-e8e2c27763ae",
   "metadata": {},
   "source": [
    "## Figure 2. Median AARs and timing comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df22e2b-3bf1-4602-ace4-d954143b840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('mako', n_colors=len(subregion_order)+1)\n",
    "cmap_dict = dict([[subregion, color] for subregion, color in list(zip(subregion_order, cmap))])\n",
    "\n",
    "# -----Load glacier boundaries\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('Glacier boundaries loaded')\n",
    "\n",
    "# -----Load climate clusters\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_scs_fn = os.path.join(scm_path, 'analysis', 'min_snow_cover_stats.csv')\n",
    "min_scs = pd.read_csv(min_scs_fn)\n",
    "# Add difference from September AAR\n",
    "# min_scs['AAR_P50_diff'] = min_scs['AAR_P50_WOY39'] - min_scs['AAR_P50_min']\n",
    "# Add Subregion and climate cluster info\n",
    "min_scs[['CenLon', 'CenLat', 'Subregion', 'clustName']] = 0, 0, '', ''\n",
    "for rgi_id in min_scs['RGIId'].drop_duplicates().values:\n",
    "    cenlon, cenlat, subregion = aois.loc[aois['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion']].values[0]\n",
    "    cluster = clusters.loc[clusters['RGIId']==rgi_id, 'clustName'].values[0]\n",
    "    min_scs.loc[min_scs['RGIId']==rgi_id, ['CenLon', 'CenLat', 'Subregion', 'clustName']] = cenlon, cenlat, subregion, cluster\n",
    "# Sort by subregion order\n",
    "min_scs['Subregion'] = pd.Categorical(min_scs['Subregion'], subregion_order)\n",
    "min_scs.sort_values(by='Subregion', inplace=True)\n",
    "print('Median AARs loaded')\n",
    "\n",
    "# -----Load melt season timings estimate\n",
    "melt_season_fn = os.path.join(scm_path, 'analysis', 'melt_season_timing.csv')\n",
    "melt_season = pd.read_csv(melt_season_fn)\n",
    "# Add subregion column\n",
    "melt_season['Subregion'] = ''\n",
    "for rgi_id in melt_season['RGIId'].drop_duplicates().values:\n",
    "    subregion = aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]\n",
    "    melt_season.loc[melt_season['RGIId']==rgi_id, 'Subregion'] = subregion\n",
    "# Sort by subregion order\n",
    "melt_season['Subregion'] = pd.Categorical(melt_season['Subregion'], subregion_order)\n",
    "melt_season.sort_values(by='Subregion', inplace=True)\n",
    "print('Melt season timing loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f7edb-9d20-4c6f-8687-2c135b4ed769",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "lw = 1.0\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif':'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(10,2, wspace=0.0, hspace=0.0)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = []\n",
    "\n",
    "# Iterate over subregions\n",
    "median_color = 'w'\n",
    "fill_color = '#2171b5'\n",
    "Iax = -1\n",
    "for i, subregion in enumerate(min_scs['Subregion'].unique()):\n",
    "    min_scs_subregion = min_scs.loc[min_scs['Subregion']==subregion]\n",
    "    \n",
    "    # a) AARs\n",
    "    ax.append(fig.add_subplot(gs[i,0]))\n",
    "    Iax += 1\n",
    "    min_scs_subregion_melt = pd.melt(\n",
    "            min_scs_subregion,\n",
    "            id_vars=['RGIId', 'Subregion'],\n",
    "            value_vars=['AAR', 'AAR_Sept'],\n",
    "            var_name='AAR_type',\n",
    "            value_name='blank'\n",
    "    )\n",
    "    min_scs_subregion_melt.rename(columns={'blank': 'AAR'}, inplace=True)\n",
    "    sns.violinplot(data=min_scs_subregion_melt, x='AAR', hue='AAR_type', cut=0, orient='h', \n",
    "                   split=True, inner='quart', fill=True, linewidth=lw, linecolor='k', dodge=True, \n",
    "                   width=0.8, palette={'AAR': fill_color, 'AAR_Sept': 'w'}, ax=ax[Iax])\n",
    "    ax[Iax].set_xlim(0,1)\n",
    "    ax[Iax].spines[['right', 'top']].set_visible(False)\n",
    "    if i > 0:\n",
    "        ax[Iax].spines['top'].set_visible(True)\n",
    "        ax[Iax].spines['top'].set_color('gray')\n",
    "    handles, labels = ax[Iax].get_legend_handles_labels()\n",
    "    ax[Iax].legend().remove()\n",
    "    if i==0:\n",
    "        ax[Iax].text(0.8, 0.7, 'a', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+4, color='k')\n",
    "        ax[Iax].set_xlabel('AAR')\n",
    "        labels = ['Observed AAR', 'September AAR']\n",
    "        ax[Iax].legend(handles, labels, loc='upper center', ncols=2, frameon=False, bbox_to_anchor=[0.4, 1.6, 0.2, 0.2])\n",
    "    else:\n",
    "        ax[Iax].set_ylabel('')\n",
    "    if i < 9:\n",
    "        ax[Iax].set_xticks([])\n",
    "        ax[Iax].set_xlabel('')\n",
    "        ax[Iax].spines['bottom'].set_color('gray')\n",
    "    else:\n",
    "        ax[Iax].set_xlabel('AAR')\n",
    "    ax[Iax].set_ylabel(subregion, ha='right', va='center', color='k', rotation=0)\n",
    "\n",
    "    # b) AAR timing and melt season duration\n",
    "    ax.append(fig.add_subplot(gs[i,1]))\n",
    "    Iax += 1\n",
    "    k = sns.kdeplot(min_scs_subregion['WOY'], vertical=False, color=fill_color, \n",
    "                    fill=True, edgecolor='k', linewidth=lw, alpha=1, ax=ax[Iax], zorder=2)\n",
    "    median = min_scs_subregion['WOY'].median()\n",
    "    ax[Iax].plot([median, median], [0, ax[Iax].get_ylim()[1]*0.9], '-', color='w', linewidth=lw+0.5, zorder=3)\n",
    "    ax[Iax].plot([14,45], [0,0], '-', color='k', linewidth=2)\n",
    "    melt_season_subregion = melt_season.loc[melt_season['Subregion']==subregion]\n",
    "    melt_season_start = melt_season_subregion['melt_season_start_WOY'].mean()\n",
    "    melt_season_end = melt_season_subregion['melt_season_end_WOY'].mean()\n",
    "    ax[Iax].fill_between([melt_season_start, melt_season_end],  \n",
    "                         [0, 0], [ax[Iax].get_ylim()[1], ax[Iax].get_ylim()[1]],\n",
    "                         facecolor='k', edgecolor='None', alpha=0.15, zorder=1, label='Melt season duration')\n",
    "    ax[Iax].set_xlim(13,45)\n",
    "    ax[Iax].set_yticks([])\n",
    "    ax[Iax].set_ylabel('')\n",
    "    ax[Iax].set_xticks([])\n",
    "    ax[Iax].set_xlabel('')\n",
    "    if i==9:\n",
    "        ax[Iax].set_xticks([18, 22, 26, 31, 35, 39, 44])\n",
    "        ax[Iax].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov'])\n",
    "        ax[Iax].set_xlabel('Time of year')\n",
    "    if i==0:\n",
    "        ax[Iax].legend(loc='upper center', frameon=False, bbox_to_anchor=[0.4, 1.6, 0.2, 0.2])\n",
    "        ax[Iax].text(0.8, 0.7, 'b', transform=ax[Iax].transAxes, \n",
    "                     fontweight='bold', fontsize=fontsize+4, color='k')\n",
    "    ax[Iax].spines[['left', 'right', 'top', 'bottom']].set_visible(False)\n",
    "            \n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig02_median_aars+timings.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d5d7b5-d596-4908-bf9b-2821e4b1dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check WOY and months\n",
    "# df = pd.DataFrame({'Date': pd.date_range('2013-01-01', '2023-12-30')})\n",
    "# df['WOY'] = df['Date'].dt.isocalendar().week\n",
    "# df['year'] = df['Date'].dt.year\n",
    "# df['month'] = df['Date'].dt.month\n",
    "# df['day'] = df['Date'].dt.day\n",
    "# df.loc[(df['month']==10) & (df['day']==1)]['WOY']#.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bf271-88ea-4056-9048-04652a4b339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "\n",
    "print('All sites:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs['AAR'].describe())\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs['WOY'].describe()) \n",
    "\n",
    "print(' ')\n",
    "print('By subregion:')\n",
    "print('-----------')\n",
    "print('---AARs---')\n",
    "print(min_scs.groupby('Subregion')['AAR'].describe())\n",
    "\n",
    "print('\\n---WOYs---')\n",
    "print(min_scs.groupby('Subregion')['WOY'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76ef00-65c4-465e-84f1-2bcf3ce753bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at which sites had AARs < 0.1\n",
    "aar_lt_p1 = min_scs.loc[min_scs['AAR'] < 0.1]\n",
    "print(len(aar_lt_p1))\n",
    "aar_lt_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c91ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print differences in AARs using full time series and only September observations\n",
    "print(\"Median AAR difference for all sites:\", (min_scs['AAR_Sept']-min_scs['AAR']).median())\n",
    "print(\"Median differences by subregion:\")\n",
    "print(min_scs.groupby('Subregion')['AAR_Sept'].median() - min_scs.groupby('Subregion')['AAR'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scs_MCs_fn = os.path.join(scm_path, 'analysis', 'median_snow_cover_stats_MC.nc')\n",
    "scs_MCs = xr.open_dataset(scs_MCs_fn)\n",
    "\n",
    "# AARs\n",
    "min_aar = scs_MCs.AAR.min(dim='WOY')\n",
    "q0 = min_aar.quantile(q=0, dim='MC_run')\n",
    "q100 = min_aar.quantile(q=1, dim='MC_run')\n",
    "range = q100 - q0\n",
    "std_dev = min_aar.std(dim='MC_run')  \n",
    "\n",
    "# Average IQR and standard deviation across all sites\n",
    "mean_range = range.mean(dim='RGIId')\n",
    "mean_std_dev = std_dev.mean(dim='RGIId')\n",
    "# Print results\n",
    "print(\"AAR:\")\n",
    "print(f\"Mean range across all sites: {mean_range.values}\")\n",
    "print(f\"Mean standard deviation across sites: {mean_std_dev.values}\")\n",
    "\n",
    "# Snow minima timings\n",
    "woy_min_aar = scs_MCs['WOY'].isel(WOY=scs_MCs.AAR.argmin(dim='WOY'))\n",
    "q0_woy = woy_min_aar.quantile(q=0, dim='MC_run')\n",
    "q25_woy = woy_min_aar.quantile(q=0.25, dim='MC_run')\n",
    "q75_woy = woy_min_aar.quantile(q=0.75, dim='MC_run')\n",
    "q100_woy = woy_min_aar.quantile(q=1, dim='MC_run')\n",
    "range_woy = q100_woy - q0_woy\n",
    "iqr_woy = q75_woy - q25_woy\n",
    "std_dev_woy = woy_min_aar.std(dim='MC_run')\n",
    "# Average IQR and standard deviation for WOY across all sites\n",
    "mean_range_woy = range_woy.mean(dim='RGIId')\n",
    "mean_iqr_woy = iqr_woy.mean(dim='RGIId')\n",
    "mean_std_dev_woy = std_dev_woy.mean(dim='RGIId')\n",
    "# Print results\n",
    "print(\"\\nSnow minima timings:\")\n",
    "print(f\"Mean WOY range across sites: {mean_range_woy.values}\")\n",
    "print(f\"Mean WOY IQR across sites: {mean_iqr_woy.values}\")\n",
    "print(f\"Mean WOY standard deviation across sites: {mean_std_dev_woy.values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ad611-f047-4fae-95c5-865070735b4d",
   "metadata": {},
   "source": [
    "## Figure 3. Observed vs. modeled SMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc9f0ca-49f0-4d0f-b177-9a01e0e34ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged ELAs and SLAs loaded\n",
      "Compiled AOIs loaded\n",
      "Clusters loaded\n",
      "Melt factors of snow loaded\n",
      "SMB at SLAs loaded\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cluster_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m fsnow \u001b[38;5;241m=\u001b[39m fsnow\u001b[38;5;241m.\u001b[39mmerge(aois[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubregion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCenLon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCenLat\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m fsnow \u001b[38;5;241m=\u001b[39m fsnow\u001b[38;5;241m.\u001b[39mmerge(clusters[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustName\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m fsnow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mCategorical(fsnow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustName\u001b[39m\u001b[38;5;124m'\u001b[39m], categories\u001b[38;5;241m=\u001b[39m\u001b[43mcluster_order\u001b[49m, ordered\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m fsnow\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustName\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m slas \u001b[38;5;241m=\u001b[39m slas\u001b[38;5;241m.\u001b[39mmerge(aois[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubregion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCenLon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCenLat\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGIId\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_order' is not defined"
     ]
    }
   ],
   "source": [
    "# Load merged monthly transient and annual ELAs \n",
    "slas_fn = os.path.join(scm_path, 'analysis', 'monthly_SLAs_modeled_observed_merged.csv')\n",
    "slas = pd.read_csv(slas_fn)\n",
    "slas['Date'] = pd.to_datetime(slas['Date'])\n",
    "slas['Month'] = pd.DatetimeIndex(slas['Date']).month\n",
    "elas_fn = os.path.join(scm_path, 'analysis', 'annual_ELAs_modeled_observed_merged.csv')\n",
    "elas = pd.read_csv(elas_fn)\n",
    "print('Merged ELAs and SLAs loaded')\n",
    "\n",
    "# Load AOIs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Compiled AOIs loaded')\n",
    "\n",
    "# Load climate clusters\n",
    "clusters_fn = os.path.join(scm_path, 'analysis', 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Clusters loaded')\n",
    "\n",
    "# Load observed and modeled melt factors of snow\n",
    "fsnow_obs_fn = os.path.join(scm_path, 'analysis', 'fsnow_observed.csv')\n",
    "fsnow_obs = pd.read_csv(fsnow_obs_fn)\n",
    "fsnow_mod_fn = os.path.join(scm_path, 'analysis', 'fsnow_modeled.csv')\n",
    "fsnow_mod = pd.read_csv(fsnow_mod_fn)\n",
    "fsnow = fsnow_obs.merge(fsnow_mod, on='RGIId')\n",
    "# adjust units for interpretability\n",
    "fsnow['fsnow_obs_mm/C/d'] = fsnow['fsnow_obs_m/C/d'] * 1e3\n",
    "fsnow['fsnow_mod_mm/C/d'] = fsnow['fsnow_mod_m/C/d'] * 1e3\n",
    "print('Melt factors of snow loaded')\n",
    "\n",
    "# Load SMB at observed SLAs\n",
    "smb_at_slas_fn = os.path.join(scm_path, 'analysis', 'monthly_SLAs_modeled.csv')\n",
    "smb_at_slas = pd.read_csv(smb_at_slas_fn)\n",
    "smb_at_slas['Date'] = pd.to_datetime(smb_at_slas['Date'])\n",
    "print('SMB at SLAs loaded')\n",
    "\n",
    "# Add subregion, centroid coordinates, and climate cluster columns\n",
    "fsnow = fsnow.merge(aois[['RGIId', 'Subregion', 'CenLon', 'CenLat']], on='RGIId')\n",
    "fsnow = fsnow.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "fsnow['clustName'] = pd.Categorical(fsnow['clustName'], categories=cluster_order, ordered=True)\n",
    "fsnow.sort_values(by='clustName', inplace=True)\n",
    "slas = slas.merge(aois[['RGIId', 'Subregion', 'CenLon', 'CenLat']], on='RGIId')\n",
    "slas = slas.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "elas = elas.merge(aois[['RGIId', 'Subregion', 'CenLon', 'CenLat']], on='RGIId')\n",
    "elas = elas.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "smb_at_slas = smb_at_slas.merge(aois[['RGIId', 'Subregion', 'CenLon', 'CenLat']], on='RGIId')\n",
    "smb_at_slas = smb_at_slas.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "print('Added columns for plotting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b035d-5552-4cf2-aa56-a8cefb9b37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10,8), gridspec_kw=dict(width_ratios=[1.5,1]))\n",
    "ax = ax.flatten()\n",
    "\n",
    "### a) Monthly snowline differences\n",
    "slas['SLA_mod-obs_m'] = slas['SLA_mod_m'] - slas['SLA_obs_m']\n",
    "sns.boxplot(data=slas, x='Month', y='SLA_mod-obs_m', hue='clustName', hue_order=cluster_order,\n",
    "            palette=cluster_cmap_dict, saturation=1, showfliers=False,\n",
    "            boxprops=dict(linewidth=1, edgecolor='k'), whiskerprops=dict(linewidth=1, color='k'), \n",
    "            ax=ax[0])\n",
    "ax[0].set_xticks(np.arange(0,5))\n",
    "ax[0].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "ax[0].set_ylim(-700, 1200)\n",
    "ax[0].set_title('a) Modeled $-$ observed snowline altitudes')\n",
    "ax[0].set_ylabel('Snowline altitude difference [m]')\n",
    "# add minor grid lines\n",
    "ax[0].xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.5))\n",
    "ax[0].grid(which='minor')\n",
    "# remove minor ticks\n",
    "ax[0].tick_params(axis='x', which='minor', bottom=False)\n",
    "\n",
    "### b) ELA differences\n",
    "elas['ELA_mod-obs_m'] = elas['ELA_mod_m'] - elas['ELA_obs_m']\n",
    "sns.kdeplot(data=elas, y='ELA_mod-obs_m', hue='clustName', hue_order=cluster_order, fill=True, alpha=0.1,\n",
    "            linewidth=2, palette=cluster_cmap_dict, ax=ax[1])\n",
    "# Add median lines\n",
    "# xmin, xmax = ax[1].get_xlim()\n",
    "# for clustName in elas_merged_annual['clustName'].drop_duplicates().values:\n",
    "#     med = elas_merged_annual.loc[elas_merged_annual['clustName']==clustName, 'ELA_mod-obs_m'].median()\n",
    "#     ax[1].axhline(y=med, xmin=0, xmax=1, color=cluster_cmap_dict[clustName], linestyle='-', linewidth=1)\n",
    "ax[1].set_ylim(ax[0].get_ylim())\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_title('b) Modeled $-$ observed ELAs')\n",
    "ax[1].set_ylabel('ELA difference [m]')\n",
    "ax[1].set_xticks([])\n",
    "\n",
    "### c) Modeled SMB at remotely-sensed snowlines\n",
    "smb_at_slas['Month'] = smb_at_slas['Date'].dt.month\n",
    "sns.boxplot(data=smb_at_slas, x='Month', y='SMB_at_obs_SLA_mwe', hue='clustName', hue_order=cluster_order,\n",
    "            palette=cluster_cmap_dict, saturation=1, showfliers=False, legend=False,\n",
    "            boxprops=dict(linewidth=1, edgecolor='k'), whiskerprops=dict(linewidth=1, color='k'), \n",
    "            ax=ax[2])\n",
    "ax[2].set_xlabel('Month')\n",
    "ax[2].set_xticks(np.arange(4,9))\n",
    "ax[2].set_xticklabels(['May', 'Jun', 'Jul', 'Aug', 'Sep'])\n",
    "ax[2].set_xlim(3.5, 8.5)\n",
    "ax[2].set_ylabel('SMB [m.w.e.]')\n",
    "ax[2].set_title('c) Modeled SMB at observed snowline altitude')\n",
    "# add minor grid lines\n",
    "ax[2].xaxis.set_minor_locator(matplotlib.ticker.MultipleLocator(0.5))\n",
    "ax[2].grid(which='minor')\n",
    "# remove minor ticks\n",
    "ax[2].tick_params(axis='x', which='minor', bottom=False)\n",
    "\n",
    "### d) Violin plots of observed and modeled melt factors\n",
    "lw = 0.5\n",
    "fsnow_melt = pd.melt(fsnow, id_vars=['RGIId', 'Subregion', 'CenLat', 'CenLon', 'clustName'], \n",
    "                     value_vars=['fsnow_obs_mm/C/d', 'fsnow_mod_mm/C/d'], \n",
    "                     var_name='fsnow_type', value_name='fsnow_mm/C/d')\n",
    "sns.violinplot(fsnow_melt, x='clustName', y='fsnow_mm/C/d', hue='fsnow_type', \n",
    "               split=True, inner='quart', legend=False, ax=ax[3])\n",
    "# Customize the colors\n",
    "for violin, color in zip(plt.gca().collections[::2], cluster_cmap_dict.values()):\n",
    "    violin.set_facecolor(color)\n",
    "    violin.set_edgecolor('k')\n",
    "    violin.set_linewidth(lw)\n",
    "for violin in plt.gca().collections[1::2]:\n",
    "    violin.set_facecolor('None')\n",
    "    violin.set_edgecolor('k')\n",
    "    violin.set_linewidth(lw)\n",
    "# Plot dummy points for legend\n",
    "xlim, ylim = plt.gca().get_xlim(), plt.gca().get_ylim()\n",
    "ax[3].plot(-5,-5, 's', markersize=15, markerfacecolor='gray', markeredgecolor='k', markeredgewidth=lw, label='Observed')\n",
    "ax[3].plot(-5,-5, 's', markersize=15, markerfacecolor='w', markeredgecolor='k', markeredgewidth=lw, label='Modeled')\n",
    "ax[3].legend(loc='lower left', frameon=False, labelspacing=1.0, handletextpad=0.3)\n",
    "ax[3].set_xlim(xlim)\n",
    "ax[3].set_ylim(ylim)\n",
    "# ax[3].set_xticks([])\n",
    "ax[3].set_xticklabels([])\n",
    "ax[3].set_xlabel('Climate cluster')\n",
    "ax[3].set_ylabel('$f_{snow}$ [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "ax[3].set_title('d) Degree day factors of snow')\n",
    "\n",
    "# Add legend\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend().remove()\n",
    "ax[1].legend().remove()\n",
    "fig.legend(handles, labels, loc='lower center', ncols=5, frameon=False,\n",
    "           bbox_to_anchor=[0.43, -0.05, 0.2, 0.2], labelspacing=0.6, handletextpad=0.3)\n",
    "\n",
    "# Add text label and line at 0\n",
    "text_labels = ['a', 'b']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.axhline(0, color='k')\n",
    "    # axis.text((axis.get_xlim()[1]-axis.get_xlim()[0])*0.9 + axis.get_xlim()[0],\n",
    "    #           (axis.get_ylim()[1]-axis.get_ylim()[0])*0.9 + axis.get_ylim()[0],\n",
    "    #           text_labels[i], fontsize=fontsize+4, fontweight='bold')\n",
    "    \n",
    "# fig.subplots_adjust(wspace=0.2)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'fig03_modeled_observed_SMB_differences.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858b44f-472d-492e-87bc-ff3d04df7811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA stats\n",
    "print(slas.groupby(['Month'])['SLA_mod-obs_m'].describe())\n",
    "slas.groupby(['clustName', 'Month'])['SLA_mod-obs_m'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELA stats\n",
    "print(elas['ELA_mod-obs_m'].describe())\n",
    "elas.groupby('clustName')['ELA_mod-obs_m'].describe().sort_values(by='50%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11854fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fsnow stats\n",
    "print('OBSERVED')\n",
    "print(fsnow['fsnow_obs_mm/C/d'].describe())\n",
    "print(fsnow.groupby('clustName')['fsnow_obs_mm/C/d'].describe())\n",
    "\n",
    "print('\\nMODELED')\n",
    "print(fsnow['fsnow_mod_mm/C/d'].describe())\n",
    "print(fsnow.groupby('clustName')['fsnow_mod_mm/C/d'].describe())\n",
    "\n",
    "print('\\nMODELED - OBSERVED:')\n",
    "fsnow['fsnow_mod-obs_mm/C/d'] = fsnow['fsnow_mod_mm/C/d'] - fsnow['fsnow_obs_mm/C/d']\n",
    "sns.boxplot(fsnow, x='clustName', y='fsnow_mod-obs_mm/C/d', hue='clustName', \n",
    "            palette=cluster_cmap_dict, showfliers=False)\n",
    "plt.show()\n",
    "print(fsnow['fsnow_mod-obs_mm/C/d'].describe())\n",
    "fsnow.groupby('clustName')['fsnow_mod-obs_mm/C/d'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d16aaf-de80-4b97-af97-29651c5f68b3",
   "metadata": {},
   "source": [
    "## Figure S1. Sites distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba3fc8-3053-4e30-a488-99df69cce4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analyzed glacier boundaires\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "cols = ['O1Region', 'O2Region', 'Zmed', 'Aspect', 'Slope', 'Area']\n",
    "for col in cols:\n",
    "    aois[col] = aois[col].astype(float)\n",
    "aois['Subregion'] = ''\n",
    "for o1region, o2region in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region), 'Subregion'] = subregion_name\n",
    "\n",
    "    \n",
    "# Load all glacier boundaries in O1 regions 1 and 2\n",
    "rgi_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/RGI/'\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp',\n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "rgi = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    file = gpd.read_file(os.path.join(rgi_path, rgi_fn))\n",
    "    rgi = pd.concat([rgi, file])\n",
    "rgi[['O1Region', 'O2Region']] = rgi[['O1Region', 'O2Region']].astype(int)\n",
    "# Add column for subregion name\n",
    "rgi['Subregion'] = ''\n",
    "for o1region, o2region in rgi[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    rgi.loc[(rgi['O1Region']==o1region) & (rgi['O2Region']==o2region), 'Subregion'] = subregion_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d356a8-0d7c-48cd-bc3a-fea96869aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting variables\n",
    "columns = ['Zmed', 'Aspect', 'Slope', 'Area']\n",
    "xlabels = ['Median elevation [m]', 'Aspect [degrees]', 'Slope [degrees]', 'Area [km$^2$]']\n",
    "bins_list = [np.linspace(0, 361, num=20), # Aspect\n",
    "             np.linspace(0, 51, num=20), # Slope\n",
    "             np.linspace(0, 300, num=50)] # Area\n",
    "\n",
    "# Set up figure\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial', 'font.size': 12})\n",
    "fig, ax = plt.subplots(len(subregion_order)+1, 4, figsize=(12, (len(subregion_order)+1)*1.5))\n",
    "aois_color = '#b35806'\n",
    "\n",
    "# All subregions\n",
    "for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "    if column=='Zmed': \n",
    "        bins = np.linspace(rgi['Zmed'].min(), rgi['Zmed'].max(), num=20)\n",
    "    else:\n",
    "        bins  = bins_list[j-1]\n",
    "    ax[0,j].hist(rgi[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "    ax[0,j].set_title(xlabel)\n",
    "    ax2 = ax[0,j].twinx()\n",
    "    ax2.hist(aois[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "    ax2.set_yticks(ax2.get_yticks())\n",
    "    ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "    ax2.spines['right'].set_color(aois_color)\n",
    "    ax2.tick_params(axis='y', color=aois_color)\n",
    "ax[0,0].set_ylabel('All regions', fontweight='bold')\n",
    "\n",
    "# Individual subregions\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    # Subset glaciers\n",
    "    aois_subregion = aois.loc[aois['Subregion']==subregion]\n",
    "    rgi_subregion = rgi.loc[rgi['Subregion']==subregion]\n",
    "\n",
    "    # Plot all glaciers in subregion\n",
    "    for j, (column, xlabel) in enumerate(zip(columns, xlabels)):\n",
    "        if column=='Zmed':\n",
    "            bins = np.linspace(rgi_subregion['Zmed'].min(), rgi_subregion['Zmed'].max(), num=20)\n",
    "        else:\n",
    "            bins = bins_list[j-1]\n",
    "        ax[i+1,j].hist(rgi_subregion[column].values, bins=bins, facecolor='k', alpha=0.6)\n",
    "        if j==0:\n",
    "            ax[i+1,j].set_ylabel(subregion)\n",
    "        ax2 = ax[i+1,j].twinx()\n",
    "        ax2.hist(aois_subregion[column].values, bins=bins, facecolor=aois_color, alpha=0.6)\n",
    "        ax2.set_yticks(ax2.get_yticks())\n",
    "        ax2.set_yticklabels(ax2.get_yticklabels(), color=aois_color)\n",
    "        ax2.spines['right'].set_color(aois_color)\n",
    "        ax2.tick_params(axis='y', color=aois_color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_site_distributions.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dc78f",
   "metadata": {},
   "source": [
    "## Figure S2. Snowline altitude uncertainty analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example site ID\n",
    "rgi_id = 'RGI60-01.01104'\n",
    "epsg_utm = \"EPSG:32606\"\n",
    "\n",
    "# Load SLA bounds\n",
    "sla_bounds_fn = os.path.join(scm_path, 'analysis', 'SLA_uncertainty_analysis.csv')\n",
    "sla_bounds_df = pd.read_csv(sla_bounds_fn)\n",
    "\n",
    "# Load all AOIs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add elevation range column\n",
    "aois['Zrange'] = aois['Zmax'] - aois['Zmin']\n",
    "# Merge with SLA bounds\n",
    "sla_bounds_df = sla_bounds_df.merge(aois[['RGIId', 'Zrange']], on='RGIId')\n",
    "\n",
    "# Load snow cover stats\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs = scs.loc[scs['AAR'] < 1].reset_index(drop=True) # remove rows where AAR = 1\n",
    "# Load DEM\n",
    "dem_fn = glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'DEMs', '*.tif'))[0]\n",
    "dem = rxr.open_rasterio(dem_fn).isel(band=0)\n",
    "dem = xr.where((dem < -1e3) | (dem > 1e4), np.nan, dem)\n",
    "dem = dem.rio.write_crs(\"EPSG:4326\")\n",
    "# Load AOI\n",
    "aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "# Clip DEM to AOI\n",
    "dem = dem.rio.clip(aoi.geometry)\n",
    "# Choose an image with some masking and relatively low transient AAR for demonstration\n",
    "sc = scs.loc[(scs['datetime']=='2019-07-03T20:18:52') & (scs['source']=='Sentinel-2_SR')].reset_index(drop=True).iloc[0]\n",
    "# sc = scs.loc[(scs['AAR'] < 0.5) & (scs['source']=='Sentinel-2_SR')].reset_index(drop=True).iloc[0]\n",
    "classified_fn = glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'classified', \n",
    "                                       f\"{sc['datetime'].replace('-', '').replace(':','')}_{rgi_id}_{sc['source']}_classified.nc\"))[0]\n",
    "classified = rxr.open_rasterio(classified_fn).squeeze()\n",
    "classified = xr.where(classified==-9999, np.nan, classified)\n",
    "classified = classified.rio.write_crs(\"EPSG:4326\")\n",
    "classified_utm = classified.rio.reproject(epsg_utm)\n",
    "classified_utm = xr.where(classified_utm > 1e30, np.nan, classified_utm)\n",
    "\n",
    "# Create binary snow image\n",
    "snow_binary = xr.where((classified==1) | (classified==2), 1, 0)\n",
    "snow_binary = xr.where(np.isnan(classified), np.nan, snow_binary) # re-insert no data values\n",
    "\n",
    "# Regrid DEM to classified image grid\n",
    "dem = dem.rio.reproject_match(classified)\n",
    "dem = xr.where(dem > 1e4, np.nan, dem)\n",
    "dem = dem.rio.write_crs(\"EPSG:4326\")\n",
    "dem_utm = dem.rio.reproject(epsg_utm)\n",
    "\n",
    "# Calculate lower and upper bounds of snowline altitude\n",
    "if sc['source']=='Landsat':\n",
    "    dx=30\n",
    "elif 'Sentinel-2' in sc['source']:\n",
    "    dx=10\n",
    "sla, sla_upper_bound, sla_lower_bound = f.calculate_sla_bounds(sc, dem, snow_binary, dx=dx, verbose=True)   \n",
    "dem_utm = xr.where(dem_utm > 1e30, np.nan, dem_utm)\n",
    "# Define colormap for classified images\n",
    "cmap_dict = {\"Snow\": \"#4eb3d3\",  \"Shadowed_snow\": \"#4eb3d3\", \"Ice\": \"#084081\", \"Rock\": \"#fe9929\", \"Water\": \"#969696\"}\n",
    "colors = []\n",
    "for key in list(cmap_dict.keys()):\n",
    "    color = list(matplotlib.colors.to_rgb(cmap_dict[key]))\n",
    "    if key=='Rock':\n",
    "        color += [0.5]\n",
    "    colors.append(color)\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=11\n",
    "lw=1.5\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif': 'Arial'})\n",
    "gs = matplotlib.gridspec.GridSpec(3,2, height_ratios=[1, 1.5, 1.5])\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]),\n",
    "      fig.add_subplot(gs[1,:]),\n",
    "      fig.add_subplot(gs[2,:])]\n",
    "\n",
    "# DEM\n",
    "im = ax[0].imshow(dem_utm.data, cmap='terrain', \n",
    "                  extent=(np.min(dem_utm.x)/1e3, np.max(dem_utm.x)/1e3, np.min(dem_utm.y)/1e3, np.max(dem_utm.y)/1e3))\n",
    "cbar = fig.colorbar(im, ax=ax[0], shrink=0.9, label='Elevation [m]')\n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "\n",
    "# classified image\n",
    "im = ax[1].imshow(classified_utm.data, cmap=cmap, clim=(0.5,5.5),\n",
    "                  extent=(np.min(classified_utm.x)/1e3, np.max(classified_utm.x)/1e3, np.min(classified_utm.y)/1e3, np.max(classified_utm.y)/1e3))\n",
    "cbar = fig.colorbar(im, ax=ax[1], shrink=0.9, ticks=[2, 3, 4, 5])\n",
    "cbar.ax.set_yticklabels(['Snow', 'Ice/firn', 'Rock', 'Water'])\n",
    "cbar.ax.set_ylim(1.5,5.5)\n",
    "ax[1].set_yticklabels([])\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('')\n",
    "\n",
    "# SLA contours\n",
    "x_mesh, y_mesh = np.meshgrid(np.divide(dem_utm.x.data, 1e3), np.divide(dem_utm.y.data, 1e3))\n",
    "for axis in ax[0:2]:\n",
    "    axis.contour(x_mesh, y_mesh, dem_utm.data, levels=[sla], colors='k', linewidth=lw)\n",
    "\n",
    "# histograms and bounds\n",
    "bin_edges = np.arange(700, 1481, step=10)\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# all elevations\n",
    "counts, _ = np.histogram(dem_utm.data, bins=bin_edges)\n",
    "areas = counts * dx**2 / 1e6 # km^2\n",
    "ax[2].bar(bin_centers, areas, width=bin_edges[1]-bin_edges[0], facecolor='gray', \n",
    "          edgecolor='k', alpha=0.5, linewidth=lw-1, label='All elevations')\n",
    "# snow-covered elevations\n",
    "dem_snow = xr.where(classified_utm==1, dem_utm, np.nan)\n",
    "counts, _ = np.histogram(dem_snow.data, bins=bin_edges)\n",
    "areas = counts * dx**2 / 1e6 # km^2\n",
    "ax[2].bar(bin_centers, areas, width=bin_edges[1]-bin_edges[0], facecolor=cmap_dict['Snow'], \n",
    "          edgecolor='k', alpha=1.0, linewidth=lw-1, label='Snow-covered elevations')\n",
    "ax[2].axvline(sla, color='k', linewidth=lw, label='Original SLA')\n",
    "ax[2].axvline(sla_lower_bound, color='k', linestyle='--', linewidth=lw, label='SLA lower bound')\n",
    "ax[2].axvline(sla_upper_bound, color='k', linestyle=':', linewidth=lw, label='SLA upper bound')\n",
    "ax[2].text(850, 0.28, \"Snow-covered area \\nbelow SLA = 0.78 km$^2$\", fontsize=9, ha='center',\n",
    "           bbox=dict(facecolor='w', edgecolor='None', alpha=0.7))\n",
    "ax[2].text(1350, 0.28, \"Snow-free area \\nabove SLA = 0.66 km$^2$\", fontsize=9, ha='center')\n",
    "ax[2].set_xlim(np.min(bin_edges)-20, np.max(bin_edges)+20)\n",
    "ax[2].set_xlabel('Elevation [m]')\n",
    "ax[2].set_ylabel('Area [km$^2$]')\n",
    "ax[2].legend(loc='upper left')\n",
    "\n",
    "# histogram of SLA ranges\n",
    "ax[3].hist(sla_bounds_df['SLA_bounds_range_m'], bins=np.linspace(0, 1000, num=101), \n",
    "           facecolor='gray', alpha=0.9, edgecolor='k', linewidth=0.5)\n",
    "range_median = np.nanmedian(sla_bounds_df['SLA_bounds_range_m'])\n",
    "range_mean = np.nanmean(sla_bounds_df['SLA_bounds_range_m'])\n",
    "range_p25 = np.nanpercentile(sla_bounds_df['SLA_bounds_range_m'], 25)\n",
    "range_p75 = np.nanpercentile(sla_bounds_df['SLA_bounds_range_m'], 75)\n",
    "ax[3].axvline(range_median, color='k', linewidth=lw, \n",
    "              label=f\"Median = {int(range_median)} m\")\n",
    "ax[3].axvline(range_mean, color='k', linewidth=lw, linestyle='--', \n",
    "              label=f\"Mean = {int(range_mean)} m\")\n",
    "ax[3].fill_between([range_p25, range_p75], [0,0], [400,400], color='k', alpha=0.1, edgecolor='k', linewidth=lw+1,\n",
    "                   label=f\"IQR = {int(range_p25)}{int(range_p75)} m\")\n",
    "ax[3].legend(loc='center right')\n",
    "ax[3].set_xlabel('Range of all SLA bounds [m]')\n",
    "ax[3].set_ylabel('Counts')\n",
    "ax[3].set_xlim(0, 600)\n",
    "ax[3].set_ylim(0,360)\n",
    "# add panel labels\n",
    "labels = ['a', 'b', 'c', 'd']\n",
    "for i, axis in enumerate(ax):\n",
    "    if i < 2:\n",
    "        xscale=0.85\n",
    "        yscale=0.9\n",
    "    else:\n",
    "        xscale=0.95\n",
    "        yscale=0.9\n",
    "    axis.text(xscale, yscale, labels[i], transform=axis.transAxes, fontsize=fontsize+4, fontweight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure to file\n",
    "fig_fn = os.path.join(figures_out_path, 'figS2_SLA_uncertainties.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a4ec9",
   "metadata": {},
   "source": [
    "## Figure S3. Median snow cover minima sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a6e26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "gs = matplotlib.gridspec.GridSpec(2,2, height_ratios=[1.5, 1])\n",
    "fig = plt.figure(figsize=(7,8))\n",
    "ax = [fig.add_subplot(gs[0,:]), \n",
    "      fig.add_subplot(gs[1,0]), fig.add_subplot(gs[1,1])]\n",
    "\n",
    "# Plot an example Monte Carlo simulation at South Cascade Glacier\n",
    "rgi_id = 'RGI60-02.18778'\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], errors='coerce')\n",
    "scs.dropna(subset=['datetime'], inplace=True)\n",
    "scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "scs['Year'] = scs['datetime'].dt.isocalendar().year\n",
    "scs['Year'] = pd.Categorical(scs['Year'])\n",
    "scs['SCA_km2'] = scs['SCA_m2'] / 1e6\n",
    "# Plot\n",
    "sns.scatterplot(scs, x='WOY', y='AAR', hue='Year', palette='viridis', s=10, legend=True, ax=ax[0])\n",
    "# Number of samples per simulation\n",
    "sample_fraction = 0.8\n",
    "nsamp = int(len(scs) * sample_fraction)\n",
    "nMC = 100\n",
    "# Monte Carlo simulations\n",
    "results = pd.DataFrame()\n",
    "for i in range(nMC):\n",
    "    sampled_indices = np.random.choice(scs.index, size=nsamp, replace=False)\n",
    "    scs_MC = scs.loc[sampled_indices].sort_values(by='datetime')\n",
    "    # Calculate weekly medians for AAR, SCA, and ELA\n",
    "    weekly_medians = scs_MC.groupby('WOY')[['AAR', 'SCA_km2', 'ELA_from_AAR_m']].median()\n",
    "    weekly_medians['MC_run'] = i    \n",
    "    # Plot\n",
    "    ax[0].plot(weekly_medians.index, weekly_medians['AAR'], '-', color='gray', linewidth=0.1)\n",
    "    results = pd.concat([results, weekly_medians])\n",
    "# Dummy line for legend\n",
    "ax[0].plot([0,0], [0,0], '-', color='gray', linewidth=2, label='MC simulation')\n",
    "# Estimate median AAR and snow minimum timing\n",
    "medians = results.groupby('WOY')['AAR'].median()\n",
    "value = medians.loc[medians==min(medians)]\n",
    "ax[0].scatter(value.index[0], medians.min(), marker='x', s=50, color='k', linewidth=2, label='Median AAR', zorder=5)\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].set_ylabel('Transient accumulation area ratio')\n",
    "ax[0].set_xlabel('Week of year')\n",
    "ax[0].set_xlim(16,45)\n",
    "ax[0].set_ylim(-0.1,1.1)\n",
    "# add legend\n",
    "ax[0].legend().remove()\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend(handles[:11:2] + handles[-2:], labels[:11:2] + labels[-2:], loc='lower left', markerscale=2)\n",
    "\n",
    "# Plot AAR and snow minima timing distributions for all sites\n",
    "median_scs_fn = os.path.join(scm_path, 'analysis', 'median_snow_cover_stats_MC.nc')\n",
    "median_scs = xr.open_dataset(median_scs_fn)\n",
    "aar_woy_mads = pd.DataFrame()\n",
    "# iterate over site names in median snow cover stats dataframe\n",
    "for rgi_id in tqdm(median_scs.RGIId.values):\n",
    "    # Calculate AAR median and MAD across MC simulations\n",
    "    aar_median = float(median_scs.sel(RGIId=rgi_id).AAR.min(dim='WOY').median().values)\n",
    "    aar_mad = median_abs_deviation(median_scs.sel(RGIId=rgi_id).AAR.min(dim='WOY').values)\n",
    "    Imins = median_scs.sel(RGIId=rgi_id).AAR.argmin(dim='WOY').values\n",
    "    woy_mad = median_abs_deviation(median_scs.WOY.values[Imins])\n",
    "    # Add to dataFrame\n",
    "    df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                       'AAR_MAD': [aar_mad],\n",
    "                       'WOY_MAD': [woy_mad]})\n",
    "    aar_woy_mads = pd.concat([aar_woy_mads, df], axis=0)\n",
    "# Plot\n",
    "ax[1].hist(aar_woy_mads['AAR_MAD'], bins=50, color='gray', edgecolor='k', linewidth=0.5)\n",
    "ax[1].text(0.95, 0.7, f\"Median = {np.round(np.nanmedian(df['AAR_MAD']),3)}\", ha='right', transform=ax[1].transAxes)\n",
    "ax[1].text(0.95, 0.63, f\"Mean = {np.round(np.nanmean(df['AAR_MAD']),3)}\", ha='right', transform=ax[1].transAxes)\n",
    "ax[1].set_xlabel('AAR MAD')\n",
    "ax[1].set_ylabel('Counts')\n",
    "ax[2].hist(aar_woy_mads['WOY_MAD'], bins=50, color='gray', edgecolor='k', linewidth=0.5)\n",
    "ax[2].text(0.95, 0.7, f\"Median = {np.round(np.nanmedian(df['WOY_MAD']),3)}\", ha='right', transform=ax[2].transAxes)\n",
    "ax[2].text(0.95, 0.63, f\"Mean = {np.round(np.nanmean(df['WOY_MAD']),3)}\", ha='right', transform=ax[2].transAxes)\n",
    "ax[2].set_xlabel('Snow minimum timing MAD [weeks]')\n",
    "\n",
    "# add panel labels\n",
    "labels = ['a', 'b', 'c']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.text(0.9, 0.92, labels[i], transform=axis.transAxes, fontsize=fontsize+2, fontweight='bold')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS3_AAR_MC_simulations.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c75d3",
   "metadata": {},
   "source": [
    "## Table S1. Study sites with subregion and climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0afd4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "aois = gpd.read_file(aois_fn)\n",
    "aois.rename(columns={'Subregion': 'Subregion name'}, inplace=True)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "\n",
    "# Add climate cluster\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "aois['Climate cluster'] = ''\n",
    "for rgi_id in aois['RGIId'].drop_duplicates().values:\n",
    "    aois.loc[aois['RGIId']==rgi_id, 'Climate cluster'] = clusters.loc[clusters['RGIId']==rgi_id, 'clustName']\n",
    "aois.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "\n",
    "# Format as LaTeX table\n",
    "columns = ['RGIId', 'O1Region', 'O2Region', 'Subregion name', 'Climate cluster']\n",
    "aois = aois[columns]\n",
    "\n",
    "# Save as Excel sheet\n",
    "aois_xl_fn = aois_fn.replace('all_aois.shp', 'TableS1_study_sites.xlsx')\n",
    "aois.to_excel(aois_xl_fn, index=False)\n",
    "print('Table saved as Excel spreadsheet:', aois_xl_fn)\n",
    "aois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28cbf85",
   "metadata": {},
   "source": [
    "## README figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7ed622",
   "metadata": {},
   "source": [
    "### Median weekly trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif': \"Arial\"})\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "# Load snow cover stats\n",
    "rgi_id = 'RGI60-02.18778'\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], errors='coerce')\n",
    "scs.dropna(subset=['datetime'], inplace=True)\n",
    "scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "scs['Year'] = scs['datetime'].dt.isocalendar().year\n",
    "scs['Year'] = pd.Categorical(scs['Year'])\n",
    "scs['SCA_km2'] = scs['SCA_m2'] / 1e6\n",
    "# Plot\n",
    "sns.scatterplot(scs, x='WOY', y='AAR', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[0])\n",
    "sns.scatterplot(scs, x='WOY', y='SCA_km2', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[1])\n",
    "sns.scatterplot(scs, x='WOY', y='ELA_from_AAR_m', hue='Year', palette='viridis', size=0.5, legend=False, ax=ax[2])\n",
    "\n",
    "\n",
    "# Number of samples per simulation\n",
    "sample_fraction = 0.8\n",
    "nsamp = int(len(scs) * sample_fraction)\n",
    "nMC = 100\n",
    "\n",
    "# Monte Carlo simulations\n",
    "results = pd.DataFrame()\n",
    "for i in range(nMC):\n",
    "    sampled_indices = np.random.choice(scs.index, size=nsamp, replace=False)\n",
    "    scs_MC = scs.loc[sampled_indices].sort_values(by='datetime')\n",
    "\n",
    "    # Calculate weekly medians for AAR, SCA, and ELA\n",
    "    weekly_medians = scs_MC.groupby('WOY')[['AAR', 'SCA_km2', 'ELA_from_AAR_m']].median()\n",
    "    weekly_medians['MC_run'] = i    \n",
    "    # Plot\n",
    "    if i==0:\n",
    "        label = 'MC simulations'\n",
    "    else:\n",
    "        label = '_nolegend'\n",
    "    ax[0].plot(weekly_medians.index, weekly_medians['AAR'], '-', color='gray', linewidth=0.1, label=label)\n",
    "    ax[1].plot(weekly_medians.index, weekly_medians['SCA_km2'], '-', color='gray', linewidth=0.1)\n",
    "    ax[2].plot(weekly_medians.index, weekly_medians['ELA_from_AAR_m'], '-', color='gray', linewidth=0.1)\n",
    "    results = pd.concat([results, weekly_medians])\n",
    "    \n",
    "# Estimate median AAR and snow minimum timing\n",
    "for i, column in enumerate(['AAR', 'SCA_km2', 'ELA_from_AAR_m']):\n",
    "    medians = results.groupby('WOY')[column].median()\n",
    "    if column!='ELA_from_AAR_m':\n",
    "        value = medians.loc[medians==min(medians)]\n",
    "    else:\n",
    "        value = medians.loc[medians==max(medians)]\n",
    "    ax[i].axvline(value.index[0], color='k', label='Minimum snow cover median')\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].set_ylabel('Transient accumulation area ratio')\n",
    "ax[1].set_ylabel('Snow covered area [km$^2$]')\n",
    "ax[2].set_ylabel('Snowline altitude [m]')\n",
    "for axis in ax:\n",
    "    axis.set_xlabel('Week of year')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'weekly_median_trends_example.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9db3d9b-f9b5-4c28-8ad2-156f59b64e56",
   "metadata": {},
   "source": [
    "## AGU24 figures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccff3b6",
   "metadata": {},
   "source": [
    "### Abstract figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc3a0c-69fc-44f1-91c2-29989208a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.color_palette('mako', n_colors=len(subregion_order)+2)\n",
    "\n",
    "# -----Load median AARs for all sites\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'results', 'min_snow_cover_stats.csv')\n",
    "min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "# Sort subregions\n",
    "min_snow_cover_stats['order'] = ''\n",
    "for i, subregion in enumerate(subregion_order):\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'order'] = i\n",
    "    min_snow_cover_stats.loc[min_snow_cover_stats['Subregion']==subregion, 'color'] = matplotlib.colors.to_hex(cmap[i])\n",
    "min_snow_cover_stats = min_snow_cover_stats.sort_values(by='order')\n",
    "print('Median AARs loaded from file')\n",
    "\n",
    "# # -----Load RGI O2 Regions\n",
    "# rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "#                                 'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "# rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# # remove Brooks Range\n",
    "# rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# # add subregion name and color column\n",
    "# rgi_O2[['Subregion', 'color']] = '', ''\n",
    "# for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "#     o1 = int(o1o2[0:2])\n",
    "#     o2 = int(o1o2[3:])\n",
    "#     subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "#     rgi_O2.loc[rgi_O2['o2region']==o1o2, 'color'] = dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values)[subregion]\n",
    "# print('RGI O2 regions loaded from file')\n",
    "\n",
    "# # -----Load GTOPO30\n",
    "# gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "# gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "# gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "# print('GTOPO30 loaded from file')\n",
    "\n",
    "# # -----Load classified image\n",
    "# site_name = 'RGI60-01.00037'\n",
    "# im_classified_fn = f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/imagery/classified/20230802T152742_RGI60-01.00037_Sentinel-2_SR_classified.nc'\n",
    "# im_classified = xr.open_dataset(im_classified_fn)\n",
    "# print('Classified image loaded')\n",
    "\n",
    "# # -----Load classified images colormap\n",
    "# import json\n",
    "# datasets_dict_fn = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/inputs-outputs/datasets_characteristics.json'\n",
    "# datasets_dict = json.load(open(datasets_dict_fn))\n",
    "# cmap_classified = matplotlib.colors.ListedColormap(datasets_dict['classified_image']['class_colors'].values())\n",
    "\n",
    "# # -----Load Sentinel-2 image from GEE\n",
    "# import math\n",
    "# import wxee as wx\n",
    "# import geedim as gd\n",
    "# import ee\n",
    "# ee.Initialize()\n",
    "# def convert_wgs_to_utm(lon: float, lat: float):\n",
    "#     utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "#     if len(utm_band) == 1:\n",
    "#         utm_band = '0' + utm_band\n",
    "#     if lat >= 0:\n",
    "#         epsg_code = '326' + utm_band\n",
    "#         return epsg_code\n",
    "#     epsg_code = '327' + utm_band\n",
    "#     return epsg_code\n",
    "# # Load AOI\n",
    "# aoi_fn = glob.glob(f'/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/{site_name}/AOIs/*.shp')[0]\n",
    "# aoi = gpd.read_file(aoi_fn)\n",
    "# aoi_bounds = aoi.geometry[0].bounds\n",
    "# region = ee.Geometry.Polygon([[aoi_bounds[0], aoi_bounds[1]], \n",
    "#                               [aoi_bounds[2], aoi_bounds[1]],\n",
    "#                               [aoi_bounds[2], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[3]],\n",
    "#                               [aoi_bounds[0], aoi_bounds[1]]])\n",
    "# # Load image collection\n",
    "# im_col = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date='2023-08-01',\n",
    "#                                                                              end_date='2023-08-03',\n",
    "#                                                                              region=region,\n",
    "#                                                                              mask=True)\n",
    "# im_ee = im_col.ee_collection.first()\n",
    "# im_ee = im_ee.clip(region)\n",
    "# im_ee = im_ee.select(['B4', 'B3', 'B2'])\n",
    "# # Convert to xarray.Dataset\n",
    "# im_xr = im_ee.wx.to_xarray(scale=30, crs='EPSG:4326')\n",
    "# im_xr = xr.where(im_xr==im_xr.attrs['_FillValue'], np.nan, im_xr / 1e4)\n",
    "# im_xr = im_xr.rio.write_crs('EPSG:4326')\n",
    "# print('Sentinel-2 SR image loaded')\n",
    "\n",
    "# # Reproject AOI and images to optimal UTM zone\n",
    "# epsg_utm = convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "# aoi_utm = aoi.to_crs(f'EPSG:{epsg_utm}')\n",
    "# im_xr = im_xr.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = im_classified.rio.write_crs(\"EPSG:4326\")\n",
    "# im_classified = im_classified.rio.reproject(f'EPSG:{epsg_utm}')\n",
    "# im_classified = xr.where(im_classified < 1, np.nan, im_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa0d1b-715d-45e8-bdb3-593af6912041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=14\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": fontsize,\n",
    "    \"font.sans-serif\": \"Arial\",\n",
    "    # \"font.family\": \"sans-serif\",\n",
    "    # \"font.sans-serif\": \"Computer Modern Sans Serif\",\n",
    "    \"text.usetex\": False\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[1,2])\n",
    "ax = [fig.add_subplot(gs[2:]),\n",
    "      fig.add_subplot(gs[0]),\n",
    "      fig.add_subplot(gs[1])]\n",
    "\n",
    "# ----- Study sites\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax[0].imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax[0], alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax[0].set_yticks(np.linspace(45, 65, num=6))\n",
    "ax[0].set_xlim(-167, -112)\n",
    "ax[0].set_ylim(46, 67)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_aspect(2.2)\n",
    "# Median AARs\n",
    "sns.scatterplot(data=min_snow_cover_stats, x='CenLon', y='CenLat', edgecolor='w', linewidth=0.5, \n",
    "                hue='Subregion', hue_order=subregion_order, palette=dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values), \n",
    "                alpha=1, size='AAR_P50_min', sizes=(2,100), ax=ax[0])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "Ikeep = np.argwhere(['0.' in x for x in np.array(labels)]).flatten()\n",
    "handles, labels = [handles[i] for i in Ikeep], [labels[i] for i in Ikeep] \n",
    "ax[0].legend(handles, labels, loc='lower left', title='20132023 median AAR', bbox_to_anchor=[0.2, 0.05, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax[0].text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax[0].text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax[0].text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "# Example site location\n",
    "min_snow_cover_stats_site = min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name]\n",
    "ax[0].plot(min_snow_cover_stats_site['CenLon'], min_snow_cover_stats_site['CenLat'], '*', \n",
    "           markeredgecolor='k', markerfacecolor='#e7298a', markersize=15, linewidth=2)\n",
    "    \n",
    "# -----b) Sentinel-2 image\n",
    "ax[1].imshow(np.dstack([im_xr.B4.data[0], im_xr.B3.data[0], im_xr.B2.data[0]]),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, \n",
    "                     np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].set_ylabel('Northing [km]')\n",
    "\n",
    "# -----c) Classified image\n",
    "ax[2].imshow(im_classified.classified.data[0], cmap=cmap_classified, clim=(1,5),\n",
    "             extent=(np.min(im_classified.x.data)/1e3, np.max(im_classified.x.data)/1e3,\n",
    "                     np.min(im_classified.y.data)/1e3, np.max(im_classified.y.data)/1e3))\n",
    "ax[2].plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "           '-k', label='Glacier boundary')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "# dummy points for legend\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(0), label='Snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(1), label='Shadowed snow')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(2), label='Ice/firn')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(3), label='Rock/debris')\n",
    "ax[2].plot(0, 0, 's', color=cmap_classified(4), label='Water')\n",
    "ax[2].set_xlim(ax[1].get_xlim())\n",
    "ax[2].set_ylim(ax[1].get_ylim())\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncols=6, markerscale=2, frameon=False)\n",
    "\n",
    "# Plot AOI\n",
    "for axis in ax[1:]:\n",
    "    axis.plot(np.divide(aoi_utm.geometry[0].exterior.coords.xy[0], 1e3), \n",
    "              np.divide(aoi_utm.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "              '-k', linewidth=1, label='Glacier boundary')\n",
    "    axis.set_yticks(np.arange(7030, 7046, step=5))\n",
    "\n",
    "# Add text labels\n",
    "text_labels = ['c', 'a', 'b']\n",
    "for i in range(0, len(ax)):\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0]) * 0.9 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0]) * 0.85 + ax[i].get_ylim()[0],\n",
    "                text_labels[i], fontweight='bold', fontsize=fontsize+4, horizontalalignment='center',\n",
    "              bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "\n",
    "# # Add caption\n",
    "# caption = (r\"\\noindent\\textbf{Figure 1. a)} Sentinel-2 surface reflectance image captured 2023-08-02 for one glacier (Randolph Glacier Inventory ID = 1.00037) \\\\\"\n",
    "#             r\"and the associated \\textbf{b)} classified image generated from the automated snow detection pipeline. \\textbf{c)} Map of the study glacier locations, \\\\\"\n",
    "#             r\"with marker sizes indicating the median accumulation area ratio (AAR) for the 20132023 study period. The maroon start marks the \\\\\"\n",
    "#             r\"location of the example glacier shown in panels \\textbf{a} and \\textbf{b}.\" )\n",
    "# fig.text(0.05, -0.02, caption, ha='left', wrap=True, fontsize=fontsize+1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'agu24_abstract_figure.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f092b8ae",
   "metadata": {},
   "source": [
    "### Snow cover GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb687748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Load inputs\n",
    "rgi_id = \"RGI60-01.00312\"\n",
    "aoi_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'AOIs', f\"{rgi_id}_outline.shp\")\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "im_classified_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'classified', '*.nc')))\n",
    "scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f\"{rgi_id}_snow_cover_stats.csv\")\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "# subset to 2019\n",
    "im_classified_fns = [x for x in im_classified_fns if int(os.path.basename(x)[0:4]) == 2019] \n",
    "scs = scs.loc[scs['datetime'].dt.year == 2019]\n",
    "out_path = os.path.join(figures_out_path, 'timeseries_gif')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "\n",
    "# Define colormap for classified images\n",
    "cmap_dict = {\"Snow\": \"#4eb3d3\",  \"Shadowed_snow\": \"#636363\", \"Ice\": \"#084081\", \"Rock\": \"#fe9929\", \"Water\": \"#252525\"}\n",
    "colors = []\n",
    "for key in list(cmap_dict.keys()):\n",
    "    color = list(matplotlib.colors.to_rgb(cmap_dict[key]))\n",
    "    if key=='Rock':\n",
    "        color += [0.5]\n",
    "    colors.append(color)\n",
    "        \n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605f4f7-37a4-4238-b334-cb2787c4e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download images for 2019 ###\n",
    "\n",
    "import math\n",
    "import ee\n",
    "import geedim as gd\n",
    "import datetime\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "def convert_wgs_to_utm(lon: float, lat: float):\n",
    "    utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0' + utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = 'EPSG:326' + utm_band\n",
    "        return epsg_code\n",
    "    epsg_code = 'EPSG:327' + utm_band\n",
    "    return epsg_code\n",
    "    \n",
    "def query_gee_for_imagery_run_pipeline(dataset, aoi_utm, date_start, date_end,\n",
    "                                       month_start, month_end, site_name, \n",
    "                                       mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=None,\n",
    "                                       verbose=False, im_download=False):\n",
    "\n",
    "    # -----Grab optimal UTM zone from AOI CRS\n",
    "    epsg_utm = str(aoi_utm.crs.to_epsg())\n",
    "\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "\n",
    "    # -----Define function to query GEE for imagery\n",
    "    def query_gee(dataset, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        if dataset == 'Landsat8':\n",
    "            # Landsat 8\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Landsat9':\n",
    "            # Landsat 9\n",
    "            im_col_gd = gd.MaskedCollection.from_name('LANDSAT/LC09/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                       end_date=date_end,\n",
    "                                                                                       region=region,\n",
    "                                                                                       cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                       mask=mask_clouds)\n",
    "        elif dataset == 'Sentinel-2_TOA':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                         end_date=date_end,\n",
    "                                                                                         region=region,\n",
    "                                                                                         cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                         mask=mask_clouds)\n",
    "\n",
    "        elif dataset == 'Sentinel-2_SR':\n",
    "            im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                            end_date=date_end,\n",
    "                                                                                            region=region,\n",
    "                                                                                            cloudless_portion=100 - cloud_cover_max,\n",
    "                                                                                            mask=mask_clouds)\n",
    "        else:\n",
    "            print(\"'dataset' variable not recognized. Please set to 'Landsat', 'Sentinel-2_TOA', or 'Sentinel-2_SR'. \"\n",
    "                  \"Exiting...\")\n",
    "            return 'N/A'\n",
    "\n",
    "        return im_col_gd\n",
    "\n",
    "    # -----Define function to filter image IDs by month range\n",
    "    def filter_im_ids_month_range(im_ids, im_dts, month_start, month_end):\n",
    "        i = [int(ii) for ii in np.arange(0, len(im_dts)) if\n",
    "             (im_dts[ii].month >= month_start) and (im_dts[ii].month <= month_end)]  # indices of images to keep\n",
    "        im_ids, im_dts = [im_ids[ii] for ii in i], [im_dts[ii] for ii in i]  # subset of image IDs and datetimes\n",
    "        # return 'N/A' if no images remain after filtering by month range\n",
    "        if len(im_dts) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        return im_ids, im_dts\n",
    "\n",
    "    # -----Define function to couple image IDs captured within the same hour for mosaicking\n",
    "    def image_mosaic_ids(im_col_gd):\n",
    "        # Grab image properties, IDs, and datetimes from image collection\n",
    "        properties = im_col_gd.properties\n",
    "        ims = dict(properties).keys()\n",
    "        im_ids = [properties[im]['system:id'] for im in ims]\n",
    "        # return if no images found\n",
    "        if len(im_ids) < 1:\n",
    "            return 'N/A', 'N/A'\n",
    "        im_dts = np.array(\n",
    "            [datetime.datetime.utcfromtimestamp(properties[im]['system:time_start'] / 1000) for im in ims])\n",
    "\n",
    "        # Remove image datetimes and IDs outside the specified month range\n",
    "        im_ids, im_dts = filter_im_ids_month_range(im_ids, im_dts, month_start, month_end)\n",
    "\n",
    "        # Grab all unique hours in image datetimes\n",
    "        hours = np.array(im_dts, dtype='datetime64[h]')\n",
    "        unique_hours = sorted(set(hours))\n",
    "\n",
    "        # Create list of IDs for each unique hour\n",
    "        im_mosaic_ids_list, im_mosaic_dts_list = [], []\n",
    "        for unique_hour in unique_hours:\n",
    "            i = list(np.ravel(np.argwhere(hours == unique_hour)))\n",
    "            im_ids_list_hour = [im_ids[ii] for ii in i]\n",
    "            im_mosaic_ids_list.append(im_ids_list_hour)\n",
    "            im_dts_list_hour = [im_dts[ii] for ii in i]\n",
    "            im_mosaic_dts_list.append(im_dts_list_hour)\n",
    "\n",
    "        return im_mosaic_ids_list, im_mosaic_dts_list\n",
    "\n",
    "    # -----Define function for extracting valid image IDs\n",
    "    def extract_valid_image_ids(ds, date_start, date_end, region, cloud_cover_max, mask_clouds):\n",
    "        # Initialize list of date ranges for querying\n",
    "        date_ranges = [(date_start, date_end)]\n",
    "        # Initialize list of error dates\n",
    "        error_dates = []\n",
    "        # Initialize error flag\n",
    "        error_occurred = True\n",
    "        # Iterate until no errors occur\n",
    "        while error_occurred:\n",
    "            error_occurred = False  # Reset the error flag at the beginning of each iteration\n",
    "            try:\n",
    "                # Initialize list of image collections\n",
    "                im_col_gd_list = []\n",
    "                # Iterate over date ranges\n",
    "                for date_range in date_ranges:\n",
    "                    # Query GEE for imagery\n",
    "                    im_col_gd = query_gee(ds, date_range[0], date_range[1], region, cloud_cover_max, mask_clouds)\n",
    "                    properties = im_col_gd.properties  # Error will occur here if an image is inaccessible!\n",
    "                    im_col_gd_list.append(im_col_gd)\n",
    "                # Initialize list of filtered image IDs and datetimes\n",
    "                im_mosaic_ids_list_full, im_mosaic_dts_list_full = [], []  # Initialize lists of\n",
    "                # Filter image IDs for month range and couple IDs for mosaicking\n",
    "                for im_col_gd in im_col_gd_list:\n",
    "                    im_mosaic_ids_list, im_mosaic_dts_list = image_mosaic_ids(im_col_gd)\n",
    "                    if type(im_mosaic_ids_list) is str:\n",
    "                        return 'N/A', 'N/A'\n",
    "                    # append to list\n",
    "                    im_mosaic_ids_list_full = im_mosaic_ids_list_full + im_mosaic_ids_list\n",
    "                    im_mosaic_dts_list_full = im_mosaic_dts_list_full + im_mosaic_dts_list\n",
    "\n",
    "                return im_mosaic_ids_list_full, im_mosaic_dts_list_full\n",
    "\n",
    "            except Exception as e:\n",
    "                error_id = str(e).split('ID=')[1].split(')')[0]\n",
    "                print(f\"Error querying GEE for {str(error_id)}\")\n",
    "\n",
    "                # Parse the error date from the exception message (replace this with your actual parsing logic)\n",
    "                error_date = datetime.datetime.strptime(error_id[0:8], '%Y%m%d')\n",
    "                error_dates.append(error_date)\n",
    "\n",
    "                # Update date ranges excluding the problematic date\n",
    "                date_starts = [date_start] + [str(error_date + datetime.timedelta(days=1))[0:10] for error_date in\n",
    "                                              error_dates]\n",
    "                date_ends = [str(error_date - datetime.timedelta(days=1))[0:10] for error_date in error_dates] + [\n",
    "                    date_end]\n",
    "                date_ranges = list(zip(date_starts, date_ends))\n",
    "\n",
    "                # Set the error flag to indicate that an error occurred\n",
    "                error_occurred = True\n",
    "\n",
    "    # -----Apply functions\n",
    "    if dataset == 'Landsat':  # must run Landsat 8 and 9 separately\n",
    "        im_ids_list_8, im_dts_list_8 = extract_valid_image_ids('Landsat8', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        im_ids_list_9, im_dts_list_9 = extract_valid_image_ids('Landsat9', date_start, date_end, region,\n",
    "                                                               cloud_cover_max, mask_clouds)\n",
    "        if (type(im_ids_list_8) is str) and (type(im_ids_list_9) is str):\n",
    "            im_ids_list, im_dts_list = 'N/A', 'N/A'\n",
    "        elif type(im_ids_list_9) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_8, im_dts_list_8\n",
    "        elif type(im_ids_list_8) is str:\n",
    "            im_ids_list, im_dts_list = im_ids_list_9, im_dts_list_9\n",
    "        else:\n",
    "            im_ids_list = im_ids_list_8 + im_ids_list_9\n",
    "            im_dts_list = im_dts_list_8 + im_dts_list_9\n",
    "    else:\n",
    "        im_ids_list, im_dts_list = extract_valid_image_ids(dataset, date_start, date_end, region, cloud_cover_max,\n",
    "                                                           mask_clouds)\n",
    "\n",
    "    # -----Check if any images were found after filtering\n",
    "    if type(im_ids_list) is str:\n",
    "        print('No images found or error in one or more image IDs, exiting...')\n",
    "        return 'N/A'\n",
    "\n",
    "    if dataset=='Landsat':\n",
    "        res = 30\n",
    "        image_scalar = 36363.63636363636\n",
    "        no_data_value = 0\n",
    "    elif 'Sentinel-2' in dataset:\n",
    "        res = 10\n",
    "        image_scalar = 1e4\n",
    "        no_data_value = -9999\n",
    "\n",
    "    # -----Create xarray.Datasets from list of image IDs\n",
    "    # loop through image IDs\n",
    "    for i in tqdm(range(0, len(im_ids_list))):\n",
    "\n",
    "        # subset image IDs and image datetimes\n",
    "        im_ids, im_dts = im_ids_list[i], im_dts_list[i]\n",
    "\n",
    "        # make directory for outputs (out_path) if it doesn't exist\n",
    "        if not os.path.exists(im_out_path):\n",
    "            os.mkdir(im_out_path)\n",
    "            print('Made directory for image downloads: ' + im_out_path)\n",
    "            \n",
    "        # define filename\n",
    "        if len(im_dts) > 1:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '_MOSAIC.tif'\n",
    "        else:\n",
    "            im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '.tif'\n",
    "            \n",
    "        # check file does not already exist in directory, download\n",
    "        if not os.path.exists(os.path.join(im_out_path, im_fn)):\n",
    "            # create list of MaskedImages from IDs\n",
    "            im_gd_list = [gd.MaskedImage.from_id(im_id) for im_id in im_ids]\n",
    "            # combine into new MaskedCollection\n",
    "            im_collection = gd.MaskedCollection.from_list(im_gd_list)\n",
    "            # create image composite\n",
    "            im_composite = im_collection.composite(method=gd.CompositeMethod.q_mosaic,\n",
    "                                                   mask=mask_clouds,\n",
    "                                                   region=region)\n",
    "            # download to file\n",
    "            im_composite.download(os.path.join(im_out_path, im_fn),\n",
    "                                  region=region,\n",
    "                                  scale=res,\n",
    "                                  crs='EPSG:4326',\n",
    "                                  dtype='int16',\n",
    "                                  bands=im_composite.refl_bands)\n",
    "\n",
    "    return\n",
    "\n",
    "# Reproject AOI to optimal UTM zone\n",
    "epsg_utm = convert_wgs_to_utm(aoi.geometry[0].centroid.coords.xy[0][0], aoi.geometry[0].centroid.coords.xy[1][0])\n",
    "aoi_utm = aoi.to_crs(epsg_utm)\n",
    "\n",
    "ee.Initialize()\n",
    "\n",
    "# Landsat\n",
    "dataset = 'Landsat'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_SR'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Sentinel-2 SR\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "query_gee_for_imagery_run_pipeline(dataset, aoi_utm, \"2019-01-01\", \"2019-12-01\", 5, 11, rgi_id, \n",
    "                                   mask_clouds=True, cloud_cover_max=70, aoi_coverage=70, im_out_path=out_path,\n",
    "                                   verbose=False, im_download=True)\n",
    "\n",
    "# Remove images with < 70% coverage of AOI\n",
    "fns = sorted(glob.glob(os.path.join(out_path, '*.tif')))\n",
    "fn_remove_list = []\n",
    "for fn in tqdm(fns):\n",
    "    im = rxr.open_rasterio(fn).squeeze()\n",
    "    im = im.rio.reproject(epsg_utm)\n",
    "    im = im.isel(band=0)\n",
    "    mask = geometry_mask(aoi_utm.geometry, transform=im.rio.transform(), invert=True, out_shape=im.shape)\n",
    "    im_masked = xr.where(mask==1, im, np.nan)\n",
    "    im_masked = xr.where((im_masked==-9999) | (im_masked==-32768), np.nan, im_masked)\n",
    "    nreal = im_masked.notnull().sum().item()\n",
    "    npx = np.count_nonzero(mask)\n",
    "    percentage_real = (nreal / npx) * 100\n",
    "    if percentage_real < 70:\n",
    "        fn_remove_list.append(fn)\n",
    "# for fn in fn_remove_list:\n",
    "#     os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over classified images\n",
    "for im_classified_fn in tqdm(im_classified_fns):\n",
    "    # Load classified image\n",
    "    im_classified = rxr.open_rasterio(im_classified_fn, decode_times=False).squeeze()\n",
    "    im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "    im_classified = xr.where(im_classified==2, 1, im_classified)\n",
    "    date = pd.Timestamp(datetime.datetime.strptime(os.path.basename(im_classified_fn).split('_')[0], \"%Y%m%dT%H%M%S\"))\n",
    "    source = os.path.basename(im_classified_fn).split(rgi_id + '_')[1].split('_classified.nc')[0]\n",
    "\n",
    "    # Load multispec image\n",
    "    try:\n",
    "        im_fn = glob.glob(os.path.join(out_path, f\"{source}_{str(date)[0:10].replace('-', '')}*.tif\"))[0]\n",
    "    except:\n",
    "        continue\n",
    "    im = rxr.open_rasterio(im_fn).squeeze()\n",
    "    if source=='Landsat':\n",
    "        image_scaler = 36363.63636363636\n",
    "    else:\n",
    "        image_scaler = 1e4\n",
    "    im = xr.where((im==-9999) | (im==-32768), np.nan, im / image_scaler)\n",
    "    \n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 2, figure=fig, height_ratios=[2,1])\n",
    "    ax = [fig.add_subplot(gs[0,0]), fig.add_subplot(gs[0,1]), fig.add_subplot(gs[1,:])]\n",
    "    # RGB image\n",
    "    ax[0].imshow(np.dstack([im.isel(band=2).data, im.isel(band=1).data, im.isel(band=0).data]),\n",
    "                 extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    aoi.plot(ax=ax[0], facecolor='None', edgecolor='k')\n",
    "    # classified image\n",
    "    xmin, xmax = -145.3555, -145.045\n",
    "    ymin, ymax = 63.144, 63.325\n",
    "    ax[1].imshow(im_classified.data, cmap=cmap, clim=(1,5), \n",
    "                 extent=(xmin, xmax, ymax, ymin))\n",
    "    ax[1].invert_yaxis()\n",
    "    aoi.plot(ax=ax[1], facecolor='None', edgecolor='k')\n",
    "    # dummy points for legend\n",
    "    ax[1].plot(0, 0, 's', color=colors[0], markersize=12, label='Snow')\n",
    "    ax[1].plot(0, 0, 's', color=colors[2], markersize=12, label='Ice')\n",
    "    ax[1].plot(0, 0, 's', color=colors[3], markersize=12, label='Rock')\n",
    "    ax[1].plot(0, 0, 's', color=colors[4], markersize=12, label='Water')\n",
    "    ax[1].legend(loc='lower right', frameon=False)\n",
    "    for axis in ax[0:2]:\n",
    "        axis.set_xlim(xmin, xmax)\n",
    "        axis.set_ylim(ymin, ymax)\n",
    "        axis.set_xticks(axis.get_xticks()[1::2])\n",
    "        axis.set_yticks(axis.get_yticks()[1::2])\n",
    "\n",
    "    # AAR time series\n",
    "    ax[2].plot(scs['datetime'], scs['AAR'], '.k')\n",
    "    scs_date = scs.loc[(scs['datetime']==date) & (scs['source']==source)]\n",
    "    ax[2].plot(scs_date['datetime'], scs_date['AAR'], '*m', markersize=15)\n",
    "    ax[2].set_ylim(0,1.05)\n",
    "    ax[2].grid(True)\n",
    "    ax[2].set_ylabel('Snow area ratio')\n",
    "    fig.suptitle(f\"{date}\\n{source.replace('_', ' ')}\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig_fn = os.path.join(out_path, f\"{date}_{rgi_id}_snow_cover.png\")\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7960ae",
   "metadata": {},
   "source": [
    "### Map of study sites with climate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load climate clusters / mean climate\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "print('Climate clusters loaded')\n",
    "\n",
    "# Load AOIs\n",
    "aois = gpd.read_file(aois_fn)\n",
    "# Add climate cluster column\n",
    "aois = aois.merge(clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "aois.rename(columns={'clustName': 'Climate class'}, inplace=True)\n",
    "print('AOIs loaded')\n",
    "\n",
    "# Load RGI O2 Regions\n",
    "rgi_O2_fn = os.path.join(scm_path, '..', 'GIS_data', 'RGI', 'RGIv7_02Regions', \n",
    "                                'RGI2000-v7.0-o2regions-Alaska-westernCanadaUS_clipped_to_country_outlines.shp')\n",
    "rgi_O2 = gpd.read_file(rgi_O2_fn)\n",
    "# remove Brooks Range\n",
    "rgi_O2 = rgi_O2.loc[rgi_O2['o2region']!='01-01']\n",
    "# add subregion name and color column\n",
    "rgi_O2[['Subregion', 'color']] = '', ''\n",
    "for i, o1o2 in enumerate(rgi_O2['o2region'].drop_duplicates().values):\n",
    "    o1 = int(o1o2[0:2])\n",
    "    o2 = int(o1o2[3:])\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    rgi_O2.loc[rgi_O2['o2region']==o1o2, 'Subregion'] = subregion_name\n",
    "print('RGI O2 regions loaded')\n",
    "\n",
    "# Load GTOPO30\n",
    "gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "print('GTOPO30 loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax.imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# RGI O2 region outlines\n",
    "color = '#525252'\n",
    "rgi_O2.plot(ax=ax, alpha=1.0, facecolor='None', edgecolor=color, linewidth=1)\n",
    "ax.set_yticks(np.linspace(45, 65, num=6))\n",
    "ax.set_xlim(-167, -112)\n",
    "ax.set_ylim(46, 66.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_aspect(2.1)\n",
    "# Site locations\n",
    "obj = sns.scatterplot(data=aois, x='CenLon', y='CenLat', edgecolor='k', linewidth=0.5, \n",
    "                      hue='Climate class', hue_order=cluster_order, palette=cluster_cmap_dict, alpha=1, ax=ax)\n",
    "sns.move_legend(obj, loc='lower left', markerscale=2, bbox_to_anchor=[0.15, 0.1, 0.2, 0.2])\n",
    "# Add region labels and arrows\n",
    "fontweight = 'bold'\n",
    "ax.text(-163, 56, 'Aleutians', color=color, rotation=35, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-158, 62.3, 'Alaska Range', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-147.9, 57.8, 'W. Chugach \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-147.6, 58.8, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-141.7, 57.7, 'St. Elias \\nMtns.', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-141.5, 58.7, 0, 0.8, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-139.6, 56.4, 'N. Coast \\nRanges', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-137.3, 56.8, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-133, 51.3, 'N. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-129.4, 51.4, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-129.7, 47, 'S. Cascades', color=color, rotation=0, horizontalalignment='center', fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.arrow(-126, 47.1, 1.3, 0, color=color, linewidth=2, head_width=0.25, head_length=0.2)\n",
    "ax.text(-132, 64, 'N. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-122, 55, 'C. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "ax.text(-117.7, 47, 'S. Rockies', color=color, rotation=0, fontsize=fontsize-1, fontweight=fontweight)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'agu24_study_sites_map.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6f05c0",
   "metadata": {},
   "source": [
    "### Model SLA animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "\n",
    "# Define output directory\n",
    "out_path = os.path.join(figures_out_path, 'model_SMB_to_SLA_animation')\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "    print('Made directory for outputs:', out_path)\n",
    "    \n",
    "# Grab modeled SMB file names\n",
    "rgi_id = '1.00032'\n",
    "bin_fn = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', f'{rgi_id}*.nc')))[0]\n",
    "\n",
    "# Load binned data\n",
    "bin = xr.open_dataset(bin_fn)\n",
    "# grab data variables\n",
    "h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "x = bin.bin_distance.data[0]\n",
    "b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "# iterate over each time period after 2013\n",
    "times = [time for time in times if time >= np.datetime64('2012-10-01')]\n",
    "elas = np.nan*np.zeros(len(times)) # initialize transient ELAs\n",
    "for j, time in enumerate(tqdm(times)):\n",
    "    # subset binned data to time\n",
    "    bin_time = bin.isel(time=j)\n",
    "    # grab the SMB \n",
    "    b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "    # add the previous SMB (restart the count in October)\n",
    "    if months[j] != 10: \n",
    "        b_sum[j,:] += b_sum[j-1,:]\n",
    "    # If all SMB > 0, ELA = minimum elevation\n",
    "    if all(b_sum[j,:] > 0):\n",
    "        elas[j] = np.min(h)\n",
    "    # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "    elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "        elas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "    # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "    elif all(b_sum[j,:] < 0):\n",
    "        X, y = b_sum[j,:], h\n",
    "        elas[j] = np.nanmax(h)\n",
    "    else:\n",
    "        print('issue')\n",
    "        \n",
    "    # Plot results\n",
    "    if time >= np.datetime64('2013-01-01'):\n",
    "        fig, ax = plt.subplots(2, 1, gridspec_kw=dict(height_ratios=[3,1]), figsize=(6,6))\n",
    "        # surface profile\n",
    "        ax[0].fill_between(x, np.zeros(len(x)), h, color='gray', edgecolor='k', alpha=0.5)\n",
    "        positive_mask = b_sum[j,:] > 0\n",
    "        negative_mask = b_sum[j,:] < 0\n",
    "        # positive mass balance bars\n",
    "        ax[0].bar(x[positive_mask], b_sum[j,positive_mask]*50, width=126, \n",
    "            bottom=h[positive_mask], color='blue', alpha=0.6, label='Positive Mass Balance')\n",
    "        # negative mass balance bars\n",
    "        ax[0].bar(x[negative_mask], b_sum[j,negative_mask]*50, width=126, \n",
    "                bottom=h[negative_mask], color='red', alpha=0.6, label='Negative Mass Balance')\n",
    "        # SLA\n",
    "        ax[0].axhline(elas[j], 0, np.nanmax(x), color='k')\n",
    "        ax[0].text(7e3, elas[j]+50, 'Snowline altitude', color='k', ha='right')\n",
    "        ax[0].set_ylim(1.25e3, 3.3e3)\n",
    "        ax[0].set_yticks([1500, 2000, 2500, 3000])\n",
    "        ax[0].set_ylabel('Elevation [m]')\n",
    "        ax[0].set_xlim(0, np.nanmax(x))\n",
    "        ax[0].set_xticks(np.arange(0, 7.1e3, step=1e3))\n",
    "        ax[0].set_xticklabels(np.divide(ax[0].get_xticks(), 1e3).astype(int).astype(str))\n",
    "        ax[0].set_xlabel('km')\n",
    "        ax[0].spines[['top', 'right']].set_visible(False)\n",
    "        ax[0].set_title(str(time)[0:7])\n",
    "        # SLA time series\n",
    "        ax[1].plot(times, elas, '.k', markersize=5)\n",
    "        ax[1].plot(times[j], elas[j], '*m', markersize=10)\n",
    "        ax[1].set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "        ax[1].set_ylim(1330, 2600)\n",
    "        ax[1].set_ylabel('Snowline altitude [m]')\n",
    "        fig.tight_layout()\n",
    "        # Save figure\n",
    "        fig_fn = os.path.join(out_path, f\"{str(time)[0:7]}.png\")\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Site location map\n",
    "\n",
    "# # Load GTOPO30\n",
    "# gtopo_fn = '/Users/raineyaberle/Research/PhD/GIS_data/GTOPO30_clip.tif'\n",
    "# gtopo = rxr.open_rasterio(gtopo_fn)\n",
    "# gtopo = xr.where(gtopo==-32768, np.nan, gtopo)\n",
    "# print('GTOPO30 loaded')\n",
    "# Load country outlines\n",
    "countries_fn = '/Users/raineyaberle/Research/PhD/GIS_data/countries_shp/countries.shp'\n",
    "countries = gpd.read_file(countries_fn)\n",
    "countries = countries.loc[(countries['NAME']=='Canada') | (countries['NAME']=='United States')]\n",
    "\n",
    "# Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "# GTOPO hillshade\n",
    "ls = matplotlib.colors.LightSource(azdeg=90, altdeg=45)\n",
    "ax.imshow(ls.hillshade(gtopo.data[0], vert_exag=0.002), cmap='gray', alpha=0.5,\n",
    "             extent=(np.min(gtopo.x.data), np.max(gtopo.x.data), \n",
    "                     np.min(gtopo.y.data), np.max(gtopo.y.data)))\n",
    "# country outlines\n",
    "countries.plot(ax=ax, facecolor='None', edgecolor='gray')\n",
    "# Glaciers\n",
    "# aois.plot(facecolor='#08519c', edgecolor='None', ax=ax)\n",
    "# Site location\n",
    "rgi_id = 'RGI60-01.00032'\n",
    "site_centroid = aois.loc[aois['RGIId']==rgi_id].geometry[0].centroid.coords.xy\n",
    "ax.plot(*site_centroid, '*', markersize=40, markerfacecolor='m', markeredgecolor='k', linewidth=0.5)\n",
    "\n",
    "ax.set_xlim(-170, -120)\n",
    "ax.set_ylim(45, 71)\n",
    "ax.set_aspect(2.1)\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'model_SMB_to_SLA_animation', 'site_location_map.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27009dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c7258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-snow-cover-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
