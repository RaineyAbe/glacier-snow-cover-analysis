{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19aea-f65a-4694-aa1b-1c1c24efbf58",
   "metadata": {},
   "source": [
    "# Compare modeled and remotely-sensed surface mass balance\n",
    "\n",
    "Requires mass balance model outputs at each site from [PyGEM](https://github.com/PyGEM-Community/PyGEM) (Rounce et al., 2023), which can be downloaded form the Carnegie Mellon data repository.\n",
    "\n",
    "The files downloaded for this work were:\n",
    "- Monthly surface mass balance along glacier centerlines for 2000â€“2022: \"binned\", downloaded from [global_ERA5_2000_2022](https://cmu.app.box.com/s/rzk8aeasg40dd3p0xr3yngkc5c0m8kxt/folder/251139952066)\n",
    "- Calibrated model parameters (degree-day factors of snow and temperature biases): \"{RGI ID}_modelprms_dict.json\" files downloaded from [pygem_datasets > Calibration](https://cmu.app.box.com/s/p8aiby5s9f3n6ycgmhknbgo4htk3pn9j/folder/298954564072)\n",
    "\n",
    "All files were placed in a folder called \"Rounce_et_al_2023\", defined with the `model_path` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10b4fc-0d6f-45af-b2ac-9c92269f5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde12fe-197d-41a7-8ec0-5b0d34956a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for inputs and outputs\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "model_path = os.path.join(scm_path, 'Rounce_et_al_2023')\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis'\n",
    "figures_path = os.path.join(code_path, 'figures')\n",
    "# import utility functions\n",
    "sys.path.append(os.path.join(code_path, 'scripts'))\n",
    "import utils as f\n",
    "\n",
    "# Load glacier boundaries for RGI IDs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2430",
   "metadata": {},
   "source": [
    "## 1. Monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d98cc",
   "metadata": {},
   "source": [
    "### Remotely-sensed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_obs_fn = os.path.join(out_path, 'monthly_SLAs_observed.nc')\n",
    "if not os.path.exists(slas_obs_fn):\n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_obs_list = []\n",
    "    \n",
    "    # sample SLA observations +/- 1 week of the first of each month\n",
    "    tdelta = np.timedelta64(1, 'W')  \n",
    "\n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(sorted(aois['RGIId'].drop_duplicates().values)):\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_classifications.zarr')\n",
    "        scs = f.load_snow_cover_stats(scs_fn)\n",
    "        scs = scs.assign_coords({'Year': scs['time'].dt.isocalendar().year})\n",
    "        scs = scs.assign_coords({'Month': scs['time'].dt.month})\n",
    "        scs = scs.assign_coords({'Day': scs['time'].dt.day})\n",
    "        \n",
    "        # Grab monthly snowline altitude\n",
    "        dates = []\n",
    "        slas = []\n",
    "        nobs = []\n",
    "        for year, month in np.unique(np.array(list(zip(scs['Year'].values, scs['Month'].values))), axis=0):\n",
    "            target_time = np.datetime64(f'{year}-0{month}-01') if month < 10 else np.datetime64(f'{year}-{month}-01')\n",
    "            scs_near = scs.sel(time=(scs['time'] >= target_time - tdelta) & \n",
    "                                    (scs['time'] <= target_time + tdelta))\n",
    "            scs_near = scs_near.dropna(dim='time')\n",
    "            if len(scs_near['SLA'].values) > 0:\n",
    "                dates.append(target_time)\n",
    "                slas.append(float(scs_near['SLA'].mean().values))\n",
    "                nobs.append(len(scs_near['SLA'].values))\n",
    "        \n",
    "        scs_monthly = pd.DataFrame({'RGIId': [rgi_id]*len(dates), \n",
    "                                    'Date': dates,\n",
    "                                    'SLA_obs': slas,\n",
    "                                    'num_obs': nobs})\n",
    "        slas_obs_list.append(scs_monthly)\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    slas_obs = pd.concat(slas_obs_list)\n",
    "    \n",
    "    # Pivot\n",
    "    slas_obs_pivot = slas_obs.pivot(index='Date', columns='RGIId', values='SLA_obs')\n",
    "    slas_obs_pivot = slas_obs_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_obs_xr = xr.Dataset(\n",
    "        {\"SLA_obs\": (['time', 'RGIId'], slas_obs_pivot.values)},\n",
    "        coords={\"time\": slas_obs_pivot.index.values,\n",
    "                \"RGIId\": slas_obs_pivot.columns.values}\n",
    "    )\n",
    "\n",
    "    # set attributes\n",
    "    slas_obs_xr['SLA_obs'].attrs['long_name'] = 'observed snowline altitude'\n",
    "    slas_obs_xr['SLA_obs'].attrs['units'] = 'meters above sea level'\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_obs_xr.to_netcdf(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs saved to file:', slas_obs_fn)\n",
    "\n",
    "else:\n",
    "    # Load from file\n",
    "    slas_obs_xr = xr.load_dataset(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs loaded from file.')\n",
    "\n",
    "slas_obs_xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c326-e946-4ec3-b453-2149a351f311",
   "metadata": {},
   "source": [
    "### Modeled SLAs and SMB at observed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b26c4-b13f-4f05-b233-bd6e773bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file already exists\n",
    "slas_mod_fn = os.path.join(out_path, 'monthly_SLAs_modeled.nc')\n",
    "if not os.path.exists(slas_mod_fn):\n",
    "    \n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_mod_list = []\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load modeled monthly SMB\n",
    "        smb_fn = glob.glob(os.path.join(model_path, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn)\n",
    "        # calculate cumulative SMB\n",
    "        def water_year(date):\n",
    "            if date.month >= 10:\n",
    "                return date.year\n",
    "            else:\n",
    "                return date.year - 1\n",
    "        smb = smb.assign_coords({'water_year': (['time'], [water_year(t) for t in smb.time.values])})\n",
    "        smb['bin_massbalclim_monthly_cumsum'] = smb['bin_massbalclim_monthly'].groupby('water_year').cumsum()\n",
    "        smb['time'] = smb.time.values.astype('datetime64[D]')\n",
    "        h = smb['bin_surface_h_initial'].data.ravel()\n",
    "        \n",
    "        # Interpolate modeled SLA as where SMB = 0 and SMB at the observed SLA\n",
    "        slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        smb_at_slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        for j, t in enumerate(smb.time.data):\n",
    "            smb_time = smb.sel(time=t)['bin_massbalclim_monthly_cumsum'].data[0]\n",
    "            # when SMB <= 0 everywhere, set SLA to maximum glacier elevation\n",
    "            if np.all(smb_time <= 0):\n",
    "                slas[j] = np.max(h)\n",
    "            # when SMB >= 0 everywhere, set SLA to minimum glacier elevation\n",
    "            elif np.all(smb_time >= 0):\n",
    "                slas[j] = np.min(h)\n",
    "            # otherwise, linearly interpolate SLA\n",
    "            else:\n",
    "                sorted_indices = np.argsort(h)\n",
    "                slas[j] = np.interp(0, smb_time[sorted_indices], h[sorted_indices])\n",
    "            # interpolate the modeled SMB at the observed SLA\n",
    "            sla_obs = slas_obs.loc[(slas_obs['RGIId']==rgi_id) & (slas_obs['Date']==t), 'SLA_obs']\n",
    "            if len(sla_obs) > 0:\n",
    "                smb_at_slas[j] = np.interp(sla_obs.values[0], h, smb_time)\n",
    "\n",
    "        # Save results in dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id]*len(smb.time.data),\n",
    "                            'Date': smb.time.data,\n",
    "                            'SLA_mod': slas,\n",
    "                            'SMB_at_SLA_obs': smb_at_slas})\n",
    "        # concatenate to dataframe list\n",
    "        slas_mod_list.append(df)\n",
    "        \n",
    "    # Combine all DataFrames\n",
    "    slas_mod = pd.concat(slas_mod_list)\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    slas_mod_pivot = slas_mod.pivot(index='Date', columns='RGIId', values=['SLA_mod', 'SMB_at_SLA_obs'])\n",
    "    slas_mod_pivot = slas_mod_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_mod_xr = xr.Dataset(\n",
    "        {\"SLA_mod\": (['time', 'RGIId'], slas_mod_pivot['SLA_mod'].values),\n",
    "         \"SMB_at_SLA_obs\":(['time', 'RGIId'], slas_mod_pivot['SMB_at_SLA_obs'].values)},\n",
    "        coords={\"time\": slas_mod_pivot.index.values,\n",
    "                \"RGIId\": slas_mod_pivot.columns.levels[1].values}\n",
    "    )\n",
    "\n",
    "    # assign attributes\n",
    "    slas_mod_xr['SLA_mod'].attrs['long_name'] = 'modeled snowline altitude'\n",
    "    slas_mod_xr['SLA_mod'].attrs['units'] = 'meters above sea level'\n",
    "    slas_mod_xr['SMB_at_SLA_obs'].attrs['long_name'] = 'modeled surface mass balance at observed snowline altitude'\n",
    "    slas_mod_xr['SMB_at_SLA_obs'].attrs['units'] = 'meters water equivalent'\n",
    "\n",
    "    # Save to NetCDF file\n",
    "    slas_mod_xr.to_netcdf(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs and snowline SMB saved to file:', slas_mod_fn)\n",
    "    \n",
    "else:\n",
    "    # Load from file\n",
    "    slas_mod_xr = xr.load_dataset(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs loaded from file.')\n",
    "\n",
    "slas_mod_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512e6fe",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "slas_merged_fn = os.path.join(out_path, 'monthly_SLAs_observed_modeled.nc')\n",
    "if not os.path.exists(slas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed SLAs and modeled SMB at observed snowline\n",
    "    slas_merged = xr.merge([slas_obs_xr, slas_mod_xr])\n",
    "    \n",
    "    # Remove 2000-2012 (no observed values) and 2023 (no modeled values)\n",
    "    slas_merged.sel(time=slice(\"2013-01-01\",\"2023-01-01\"))\n",
    "    \n",
    "    # Remove observations outside May - November (no observed values)\n",
    "    def filter_month_range(month):\n",
    "        return (month >= 5) & (month <= 10)\n",
    "    slas_merged = slas_merged.sel(time=filter_month_range(slas_merged['time.month']))\n",
    "    \n",
    "    # Save results\n",
    "    slas_merged.to_netcdf(slas_merged_fn)\n",
    "    print('Merged monthly SLAs saved to file:', slas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_merged = xr.load_dataset(slas_merged_fn)\n",
    "    print('Merged monthly SLAs loaded from file.')\n",
    "\n",
    "slas_merged['SLA_mod-obs'] = slas_merged['SLA_mod'] - slas_merged['SLA_obs']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(slas_merged['SLA_mod-obs'].values.ravel(), bins=50)\n",
    "ax.set_xlabel('SLA$_{mod}$ - SLA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((slas_merged[\"SLA_mod-obs\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((slas_merged[\"SLA_mod-obs\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((slas_merged[\"SLA_mod-obs\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((slas_merged[\"SLA_mod-obs\"]).values.ravel(), nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2ecd9",
   "metadata": {},
   "source": [
    "## 2. ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c401e-1035-42af-a623-2d6d03f8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_elas_merged_fn = os.path.join(out_path, 'monthly_SLAs_annual_ELAs_observed_modeled.nc')\n",
    "\n",
    "if not os.path.exists(slas_elas_merged_fn):\n",
    "    # Make a copy of the SLAs\n",
    "    slas_elas_merged = slas_merged.copy()\n",
    "    \n",
    "    # Identify maximum annual observed SLAs\n",
    "    slas_elas_merged['ELA_obs'] = slas_elas_merged['SLA_obs'].groupby(['time.year']).max()\n",
    "    # Set ELAs before 2016 to NaN\n",
    "    slas_elas_merged['ELA_obs'] = xr.where(slas_elas_merged['ELA_obs'].year < 2016, np.nan, slas_elas_merged['ELA_obs'])\n",
    "    \n",
    "    # Identify maximum annual modeled SLAs \n",
    "    slas_elas_merged['ELA_mod'] = slas_elas_merged['SLA_mod'].groupby(['time.year']).max()\n",
    "    \n",
    "    # Set attributes\n",
    "    slas_elas_merged['ELA_obs'].attrs['long_name'] = 'observed equilibrium line altitude'\n",
    "    slas_elas_merged['ELA_obs'].attrs['units'] = 'meters above sea level'\n",
    "    slas_elas_merged['ELA_mod'].attrs['long_name'] = 'modeled equilibrium line altitude'\n",
    "    slas_elas_merged['ELA_mod'].attrs['units'] = 'meters above sea level'\n",
    "\n",
    "    # Save to NetCDF file\n",
    "    slas_elas_merged.to_netcdf(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs saved to file:', slas_elas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_elas_merged = xr.load_dataset(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs loaded from file.')\n",
    "\n",
    "slas_elas_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d85ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_elas_merged['ELA_mod-obs'] = slas_elas_merged['ELA_mod'] - slas_elas_merged['ELA_obs']\n",
    "plt.hist(slas_elas_merged['ELA_mod-obs'].values.ravel(), bins=50)\n",
    "plt.show()\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f\"Mean diff = {np.nanmean(slas_elas_merged['ELA_mod-obs'].values)} m\")\n",
    "print(f\"Std. diff = {np.nanstd(slas_elas_merged['ELA_mod-obs'].values)} m\")\n",
    "print(f\"Median diff = {np.nanmedian(slas_elas_merged['ELA_mod-obs'].values)} m\")\n",
    "print(f\"MAD diff = {MAD(slas_elas_merged['ELA_mod-obs'].values.ravel(), nan_policy='omit')} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0a680",
   "metadata": {},
   "source": [
    "## 3. Assess agreement with new PyGEM runs at the USGS Benchmark Glaciers\n",
    "\n",
    "For remotely-sensed and modeled snowline time series, bin estimates by week of year, characterize the timing of snowline rise and fall and maximum snowline altitude. Select the PyGEM run that agrees best.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load glacier IDs from model runs\n",
    "pygem_new_path = os.path.join(scm_path, 'Brandon_new_PyGEM_runs')\n",
    "rgi_ids = [x for x in sorted(os.listdir(pygem_new_path)) if os.path.isdir(os.path.join(pygem_new_path, x))]\n",
    "names = ['Gulkana', 'Wolverine', 'Lemon Creek', 'Sperry', 'South Cascade']\n",
    "print('RGI IDs for glaciers with PyGEM runs:', rgi_ids)\n",
    "\n",
    "# Iterate over RGI IDs\n",
    "df_results_full = pd.DataFrame() # initialize dataframe for full results\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    name = names[i]\n",
    "    print(name, rgi_id)\n",
    "\n",
    "    # Load observed snow cover data\n",
    "    scs_fn = os.path.join(scm_path, 'study-sites', f\"RGI60-0{rgi_id}\", f\"RGI60-0{rgi_id}_classifications.zarr\")\n",
    "    scs = f.load_snow_cover_stats(scs_fn)\n",
    "    \n",
    "    # Get file names of runs\n",
    "    run_fns = sorted(glob.glob(os.path.join(pygem_new_path, rgi_id, '*.nc')))\n",
    "\n",
    "    # Define output file\n",
    "    out_fn = os.path.join(scm_path, 'analysis', f\"PyGEM_comparison_RGI60-0{rgi_id}.nc\")\n",
    "    if not os.path.exists(out_fn):\n",
    "\n",
    "        # Compile modeled snowline altitudes and ELAs\n",
    "        model_runs_list = []\n",
    "        # iterate over model runs\n",
    "        for fn in tqdm(run_fns):\n",
    "            ds = xr.open_dataset(fn)\n",
    "            ds['time'] = ds.indexes['time'].to_datetimeindex()\n",
    "            # load model_parameters\n",
    "            params = json.loads(ds.model_parameters)\n",
    "            kp = params[\"kp\"]\n",
    "            tbias = params[\"tbias\"]\n",
    "            ddfsnow = params[\"ddfsnow\"]\n",
    "            tsnow_threshold = params[\"tsnow_threshold\"]\n",
    "            precgrad = params[\"precgrad\"]\n",
    "            # extract snowline and ELA variables\n",
    "            snowline = ds['glac_snowline_monthly']\n",
    "            ela = ds['glac_ELA_annual']\n",
    "            # create a new dataset with parameters and add to list\n",
    "            run_ds = xr.Dataset({\n",
    "                'glac_snowline_monthly': snowline,\n",
    "                'glac_ELA_annual': ela,\n",
    "                'kp': xr.DataArray(kp, dims=()),\n",
    "                'tbias': xr.DataArray(tbias, dims=()),\n",
    "                'ddfsnow': xr.DataArray(ddfsnow, dims=()),\n",
    "            })\n",
    "            model_runs_list.append(run_ds)\n",
    "        # combine all runs into a single dataset\n",
    "        combined_ds = xr.concat(model_runs_list, dim='run')\n",
    "        # trim to post-2013, May to November (no observed snowline data outside then)\n",
    "        combined_ds = combined_ds.sel(time=slice('2013-01-01', None))\n",
    "        combined_ds = combined_ds.sel(time=combined_ds['time.month'].isin([5, 6, 7, 8, 9, 10]))\n",
    "        # add glacier ID\n",
    "        combined_ds['rgi_id'] = xr.DataArray(rgi_id, dims=())    \n",
    "\n",
    "        # Merge with observed SLAs\n",
    "        combined_ds = xr.merge([combined_ds, slas_obs_xr.sel(RGIId=f\"RGI60-0{rgi_id}\")])\n",
    "\n",
    "        # Calculate difference\n",
    "        combined_ds['mod-obs_SLA'] = combined_ds['glac_snowline_monthly'] - combined_ds['SLA_obs_m']\n",
    "\n",
    "        # Save to file\n",
    "        combined_ds.to_netcdf(out_fn)\n",
    "\n",
    "    else:\n",
    "        # Load combined dataset rom file\n",
    "        combined_ds = xr.open_dataset(out_fn)\n",
    "\n",
    "    # Load original model parameters\n",
    "    modelprms_fn = os.path.join(model_path, '..', 'Rounce_et_al_2023', 'modelprms', f\"{rgi_id}-modelprms_dict.pkl\")\n",
    "    modelprms = pd.read_pickle(modelprms_fn)\n",
    "\n",
    "    # Calculate RMSE for each run's snowline altitudes\n",
    "    diff = combined_ds['mod-obs_SLA']\n",
    "    rmse_by_run = np.sqrt((diff**2).mean(dim='time'))\n",
    "    combined_ds['rmse'] = rmse_by_run\n",
    "\n",
    "    # identify parameter combinations with the lowest RMSE\n",
    "    df_plot = combined_ds[['tbias', 'ddfsnow', 'kp', 'rmse', 'mod-obs_SLA']].to_dataframe().reset_index()\n",
    "    df_plot = df_plot.dropna(subset=['rmse'])\n",
    "    df_plot_best = df_plot.loc[df_plot['rmse'].idxmin()]\n",
    "\n",
    "    # subset the model dataset for the original vs. best runs\n",
    "    # original\n",
    "    squared_diffs = sum((combined_ds[var] - modelprms['emulator'][var][0])**2 for var in ['kp', 'tbias', 'ddfsnow'])\n",
    "    best_run_idx = squared_diffs.argmin(dim=\"run\")\n",
    "    combined_ds_original = combined_ds.sel(run=best_run_idx, glac=0)\n",
    "    # best\n",
    "    combined_ds_best = combined_ds.sel(run=int(df_plot_best['run']), glac=0)\n",
    "\n",
    "    # Plot RMSE as a function of tbias, ddfsnow, and kp\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), gridspec_kw=dict(height_ratios=[2,2,1]))\n",
    "    sns.scatterplot(df_plot, x='tbias', y='ddfsnow', \n",
    "                    size='kp', hue='rmse', palette='viridis_r', sizes=(2,50), \n",
    "                    ax=ax[0])\n",
    "    # original parameter combinations\n",
    "    ax[0].plot(modelprms['emulator']['tbias'], modelprms['emulator']['ddfsnow'], 's', \n",
    "            markersize=15, markeredgecolor='m', markerfacecolor='None', markeredgewidth=2, label='Original')\n",
    "    # best parameter combination\n",
    "    ax[0].plot(df_plot_best['tbias'], df_plot_best['ddfsnow'], \n",
    "            '*', markersize=15, markeredgecolor='m', markerfacecolor='None', markeredgewidth=2, label='Lowest RMSE')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f'{name} ({rgi_id})')\n",
    "    ax[0].grid(True)\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('rmse', 'RMSE [m]').replace('kp', 'Precipitation factor') for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right', bbox_to_anchor=[1.1, 0.4, 0.2, 0.2])\n",
    "    # modeled and observed snowline time series\n",
    "    ax[1].plot(combined_ds_original.time, combined_ds_original.glac_snowline_monthly, '-', color='gray', label='Original model')\n",
    "    ax[1].plot(combined_ds_best.time, combined_ds_best.glac_snowline_monthly, '-k', label='Best model')\n",
    "    ax[1].plot(scs['time'], scs['SLA'], '.m', markersize=5, label='Observed')\n",
    "    ax[1].legend(loc='center left', bbox_to_anchor=[1.0, 0.4, 0.2, 0.2])\n",
    "    ax[1].set_ylabel('Snowline altitude [m.a.s.l.]')\n",
    "    # observed - modeled snowline altitude\n",
    "    ax[2].plot(combined_ds_original.time, combined_ds_original['mod-obs_SLA'], '.', color='gray', markersize=10, label='Original model')\n",
    "    ax[2].plot(combined_ds_best.time, combined_ds_best['mod-obs_SLA'], '.k', markersize=5, label='Best model')\n",
    "    ax[2].legend(loc='center left', bbox_to_anchor=[1.0, 0.4, 0.2, 0.2])\n",
    "    ax[2].grid()\n",
    "    ax[2].set_ylabel('Modeled $-$ observed\\nsnowline altitude [m]')\n",
    "    ax[1].set_xlim(ax[2].get_xlim())\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Save figure to file\n",
    "    fig_fn = os.path.join(figures_path, f\"{rgi_id}_PyGEM_comparison.png\")\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Compile results in dataframe\n",
    "    df_results = pd.DataFrame({'Original': \n",
    "                           [modelprms['emulator']['tbias'][0],\n",
    "                            modelprms['emulator']['ddfsnow'][0],\n",
    "                            modelprms['emulator']['kp'][0]],\n",
    "                            'Best': \n",
    "                            [df_plot_best['tbias'],\n",
    "                             df_plot_best['ddfsnow'],\n",
    "                             df_plot_best['kp']]\n",
    "                           },\n",
    "                           index=['tbias', 'ddfsnow', 'kp'])\n",
    "    # add site RGI ID and name columns\n",
    "    df_results.index = pd.MultiIndex.from_product([[f\"RGI60-0{rgi_id}\"], df_results.index], \n",
    "                                                names=['RGIId', 'parameter'])\n",
    "    df_results['name'] = name\n",
    "    # concatenate to full dataframe\n",
    "    df_results_full = pd.concat([df_results_full, df_results])\n",
    "\n",
    "# Add Original - Best column\n",
    "df_results_full['Original-Best'] = df_results_full['Original'] - df_results_full['Best']\n",
    "\n",
    "# Save full data frame to file\n",
    "df_results_full_fn = os.path.join(scm_path, 'analysis', 'PyGEM_comparison_params.csv')\n",
    "df_results_full.to_csv(df_results_full_fn, index=True)\n",
    "print('Results saved to file:', df_results_full_fn)\n",
    "df_results_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be1a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
