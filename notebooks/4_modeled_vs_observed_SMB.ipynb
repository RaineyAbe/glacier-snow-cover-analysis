{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19aea-f65a-4694-aa1b-1c1c24efbf58",
   "metadata": {},
   "source": [
    "# Compare modeled and remotely-sensed surface mass balance\n",
    "\n",
    "Requires mass balance model outputs at each site from [PyGEM](https://github.com/PyGEM-Community/PyGEM) (Rounce et al., 2023), which can be downloaded form the Carnegie Mellon data repository.\n",
    "\n",
    "The files downloaded for this work were:\n",
    "- Monthly surface mass balance along glacier centerlines for 2000â€“2022: \"binned\", downloaded from [global_ERA5_2000_2022](https://cmu.app.box.com/s/rzk8aeasg40dd3p0xr3yngkc5c0m8kxt/folder/251139952066)\n",
    "- Calibrated model parameters (degree-day factors of snow and temperature biases): \"{RGI ID}_modelprms_dict.json\" files downloaded from [pygem_datasets > Calibration](https://cmu.app.box.com/s/p8aiby5s9f3n6ycgmhknbgo4htk3pn9j/folder/298954564072)\n",
    "\n",
    "All files were placed in a folder called \"Rounce_et_al_2023\", defined with the `model_path` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10b4fc-0d6f-45af-b2ac-9c92269f5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde12fe-197d-41a7-8ec0-5b0d34956a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for inputs and outputs\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "model_path = os.path.join(scm_path, 'Rounce_et_al_2023')\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis'\n",
    "figures_path = os.path.join(code_path, 'figures')\n",
    "\n",
    "# Load glacier boundaries for RGI IDs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2430",
   "metadata": {},
   "source": [
    "## 1. Monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d98cc",
   "metadata": {},
   "source": [
    "### Remotely-sensed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_obs_fn = os.path.join(out_path, 'monthly_SLAs_observed_adjusted.nc')\n",
    "if not os.path.exists(slas_obs_fn):\n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_obs_list = []\n",
    "    \n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(sorted(aois['RGIId'].drop_duplicates().values)):\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats_adjusted.csv')\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        scs['Year'] = scs['datetime'].dt.year\n",
    "        scs['Month'] = scs['datetime'].dt.month\n",
    "        scs['Day'] = scs['datetime'].dt.day\n",
    "        \n",
    "        # Grab monthly snowline altitude\n",
    "        Imonths = []\n",
    "        dates = []\n",
    "        for year, month in scs[['Year', 'Month']].drop_duplicates().values:\n",
    "            first_of_month = pd.Timestamp(year=year, month=month, day=1)\n",
    "            \n",
    "            # Identify closest observation to this date\n",
    "            scs.loc[:, 'diff'] = np.abs(scs.loc[:, 'datetime'] - first_of_month)\n",
    "            Imonths.append(scs['diff'].idxmin())\n",
    "            dates.append(first_of_month)\n",
    "        \n",
    "        scs_monthly = scs.iloc[Imonths].reset_index(drop=True)\n",
    "        scs_monthly['Date'] = dates\n",
    "        scs_monthly['RGIId'] = rgi_id\n",
    "        slas_obs_list.append(scs_monthly[['RGIId', 'Date', 'SLA_m']])\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    slas_obs = pd.concat(slas_obs_list)\n",
    "    slas_obs.rename(columns={'SLA_m': 'SLA_obs_m'}, inplace=True)\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    slas_obs_pivot = slas_obs.pivot(index='Date', columns='RGIId', values='SLA_obs_m')\n",
    "    slas_obs_pivot = slas_obs_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_obs_xr = xr.Dataset(\n",
    "        {\"SLA_obs_m\": (['time', 'RGIId'], slas_obs_pivot.values)},\n",
    "        coords={\"time\": slas_obs_pivot.index.values,\n",
    "                \"RGIId\": slas_obs_pivot.columns.values}\n",
    "    )\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_obs_xr.to_netcdf(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs saved to file:', slas_obs_fn)\n",
    "\n",
    "else:\n",
    "    # Load from file\n",
    "    slas_obs_xr = xr.load_dataset(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs loaded from file.')\n",
    "\n",
    "slas_obs_xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c326-e946-4ec3-b453-2149a351f311",
   "metadata": {},
   "source": [
    "### Modeled SLAs and SMB at observed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b26c4-b13f-4f05-b233-bd6e773bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file already exists\n",
    "slas_mod_fn = os.path.join(out_path, 'monthly_SLAs_modeled.nc')\n",
    "if not os.path.exists(slas_mod_fn):\n",
    "    \n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_mod_list = []\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load modeled monthly SMB\n",
    "        smb_fn = glob.glob(os.path.join(model_path, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn)\n",
    "        # calculate cumulative SMB\n",
    "        def water_year(date):\n",
    "            if date.month >= 10:\n",
    "                return date.year\n",
    "            else:\n",
    "                return date.year - 1\n",
    "        smb = smb.assign_coords({'water_year': (['time'], [water_year(t) for t in smb.time.values])})\n",
    "        smb['bin_massbalclim_monthly_cumsum'] = smb['bin_massbalclim_monthly'].groupby('water_year').cumsum()\n",
    "        smb['time'] = smb.time.values.astype('datetime64[D]')\n",
    "        h = smb['bin_surface_h_initial'].data.ravel()\n",
    "        \n",
    "        # Interpolate modeled SLA as where SMB = 0 and SMB at the observed SLA\n",
    "        slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        smb_at_slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        for j, t in enumerate(smb.time.data):\n",
    "            smb_time = smb.sel(time=t)['bin_massbalclim_monthly_cumsum'].data[0]\n",
    "            # when SMB <= 0 everywhere, set SLA to maximum glacier elevation\n",
    "            if np.all(smb_time <= 0):\n",
    "                slas[j] = np.max(h)\n",
    "            # when SMB >= 0 everywhere, set SLA to minimum glacier elevation\n",
    "            elif np.all(smb_time >= 0):\n",
    "                slas[j] = np.min(h)\n",
    "            # otherwise, linearly interpolate SLA\n",
    "            else:\n",
    "                sorted_indices = np.argsort(h)\n",
    "                slas[j] = np.interp(0, smb_time[sorted_indices], h[sorted_indices])\n",
    "            # interpolate the modeled SMB at the observed SLA\n",
    "            sla_obs = slas_obs.loc[(slas_obs['RGIId']==rgi_id) & (slas_obs['Date']==t), 'SLA_obs_m']\n",
    "            if len(sla_obs) > 0:\n",
    "                smb_at_slas[j] = np.interp(sla_obs.values[0], h, smb_time)\n",
    "\n",
    "        # Save results in dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id]*len(smb.time.data),\n",
    "                            'Date': smb.time.data,\n",
    "                            'SLA_mod_m': slas,\n",
    "                            'SMB_at_SLA_obs_mwe': smb_at_slas})\n",
    "        # concatenate to dataframe list\n",
    "        slas_mod_list.append(df)\n",
    "        \n",
    "    # Combine all DataFrames\n",
    "    slas_mod = pd.concat(slas_mod_list)\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    slas_mod_pivot = slas_mod.pivot(index='Date', columns='RGIId', values=['SLA_mod_m', 'SMB_at_SLA_obs_mwe'])\n",
    "    slas_mod_pivot = slas_mod_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_mod_xr = xr.Dataset(\n",
    "        {\"SLA_mod_m\": (['time', 'RGIId'], slas_mod_pivot['SLA_mod_m'].values),\n",
    "         \"SMB_at_SLA_obs_mwe\":(['time', 'RGIId'], slas_mod_pivot['SMB_at_SLA_obs_mwe'].values)},\n",
    "        coords={\"time\": slas_mod_pivot.index.values,\n",
    "                \"RGIId\": slas_mod_pivot.columns.levels[1].values}\n",
    "    )\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_mod_xr.to_netcdf(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs and snowline SMB saved to file:', slas_mod_fn)\n",
    "    \n",
    "else:\n",
    "    # Load from file\n",
    "    slas_mod_xr = xr.load_dataset(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs loaded from file.')\n",
    "\n",
    "slas_mod_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512e6fe",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "slas_merged_fn = os.path.join(out_path, 'monthly_SLAs_observed_modeled_adjusted.nc')\n",
    "if not os.path.exists(slas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed SLAs and modeled SMB at observed snowline\n",
    "    slas_merged = xr.merge([slas_obs_xr, slas_mod_xr])\n",
    "    \n",
    "    # Remove 2000-2012 (no observed values) and 2023 (no modeled values)\n",
    "    slas_merged.sel(time=slice(\"2013-01-01\",\"2023-01-01\"))\n",
    "    \n",
    "    # Remove observations outside May - November (no observed values)\n",
    "    def filter_month_range(month):\n",
    "        return (month >= 5) & (month <= 10)\n",
    "    slas_merged = slas_merged.sel(time=filter_month_range(slas_merged['time.month']))\n",
    "    \n",
    "    # Save results\n",
    "    slas_merged.to_netcdf(slas_merged_fn)\n",
    "    print('Merged monthly SLAs saved to file:', slas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_merged = xr.load_dataset(slas_merged_fn)\n",
    "    print('Merged monthly SLAs loaded from file.')\n",
    "\n",
    "slas_merged['SLA_mod-obs_m'] = slas_merged['SLA_mod_m'] - slas_merged['SLA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(slas_merged['SLA_mod-obs_m'].values.ravel(), bins=50)\n",
    "ax.set_xlabel('SLA$_{mod}$ - SLA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((slas_merged[\"SLA_mod-obs_m\"]).values.ravel(), nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2ecd9",
   "metadata": {},
   "source": [
    "## 2. ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c401e-1035-42af-a623-2d6d03f8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_elas_merged_fn = os.path.join(out_path, 'monthly_SLAs_annual_ELAs_observed_modeled_adjusted.nc')\n",
    "\n",
    "if not os.path.exists(slas_elas_merged_fn):\n",
    "    # Make a copy of the SLAs\n",
    "    slas_elas_merged = slas_merged.copy()\n",
    "    \n",
    "    # Identify maximum annual observed SLAs\n",
    "    slas_elas_merged['ELA_obs_m'] = slas_elas_merged['SLA_obs_m'].groupby(['time.year']).max()\n",
    "    \n",
    "    # Identify maximum annual modeled SLAs \n",
    "    slas_elas_merged['ELA_mod_m'] = slas_elas_merged['SLA_mod_m'].groupby(['time.year']).max()\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_elas_merged.to_netcdf(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs saved to file:', slas_elas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_elas_merged = xr.load_dataset(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs loaded from file.')\n",
    "\n",
    "slas_elas_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0a680",
   "metadata": {},
   "source": [
    "## 3. Assess agreement with new PyGEM runs at the USGS Benchmark Glaciers\n",
    "\n",
    "For remotely-sensed and modeled snowline time series, bin estimates by week of year, characterize the timing of snowline rise and fall and maximum snowline altitude. Select the PyGEM run that agrees best.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load glacier IDs from model runs\n",
    "pygem_new_path = os.path.join(scm_path, 'Brandon_new_PyGEM_runs')\n",
    "rgi_ids = [x for x in sorted(os.listdir(pygem_new_path)) if os.path.isdir(os.path.join(pygem_new_path, x))]\n",
    "names = ['Gulkana', 'Wolverine', 'Lemon Creek', 'Sperry', 'South Cascade']\n",
    "print('RGI IDs for glaciers with PyGEM runs:', rgi_ids)\n",
    "\n",
    "# Iterate over RGI IDs\n",
    "df_results_full = pd.DataFrame() # initialize dataframe for full results\n",
    "for i, rgi_id in enumerate(rgi_ids):\n",
    "    name = names[i]\n",
    "    print(name, rgi_id)\n",
    "\n",
    "    # Load observed snow cover data\n",
    "    scs_fn = os.path.join(scm_path, 'study-sites', f\"RGI60-0{rgi_id}\", f\"RGI60-0{rgi_id}_snow_cover_stats_adjusted.csv\")\n",
    "    scs = pd.read_csv(scs_fn)\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'])\n",
    "    # scs = scs.set_index('datetime')\n",
    "    \n",
    "    # Get file names of runs\n",
    "    run_fns = sorted(glob.glob(os.path.join(pygem_new_path, rgi_id, '*.nc')))\n",
    "\n",
    "    # Define output file\n",
    "    out_fn = os.path.join(scm_path, 'analysis', f\"PyGEM_comparison_RGI60-0{rgi_id}.nc\")\n",
    "    if not os.path.exists(out_fn):\n",
    "\n",
    "        # Compile modeled snowline altitudes and ELAs\n",
    "        model_runs_list = []\n",
    "        # iterate over model runs\n",
    "        for fn in tqdm(run_fns):\n",
    "            ds = xr.open_dataset(fn)\n",
    "            ds['time'] = ds.indexes['time'].to_datetimeindex()\n",
    "            # load model_parameters\n",
    "            params = json.loads(ds.model_parameters)\n",
    "            kp = params[\"kp\"]\n",
    "            tbias = params[\"tbias\"]\n",
    "            ddfsnow = params[\"ddfsnow\"]\n",
    "            tsnow_threshold = params[\"tsnow_threshold\"]\n",
    "            precgrad = params[\"precgrad\"]\n",
    "            # extract snowline and ELA variables\n",
    "            snowline = ds['glac_snowline_monthly']\n",
    "            ela = ds['glac_ELA_annual']\n",
    "            # create a new dataset with parameters and add to list\n",
    "            run_ds = xr.Dataset({\n",
    "                'glac_snowline_monthly': snowline,\n",
    "                'glac_ELA_annual': ela,\n",
    "                'kp': xr.DataArray(kp, dims=()),\n",
    "                'tbias': xr.DataArray(tbias, dims=()),\n",
    "                'ddfsnow': xr.DataArray(ddfsnow, dims=()),\n",
    "            })\n",
    "            model_runs_list.append(run_ds)\n",
    "        # combine all runs into a single dataset\n",
    "        combined_ds = xr.concat(model_runs_list, dim='run')\n",
    "        # trim to post-2013, May to November (no observed snowline data outside then)\n",
    "        combined_ds = combined_ds.sel(time=slice('2013-01-01', None))\n",
    "        combined_ds = combined_ds.sel(time=combined_ds['time.month'].isin([5, 6, 7, 8, 9, 10]))\n",
    "        # add glacier ID\n",
    "        combined_ds['rgi_id'] = xr.DataArray(rgi_id, dims=())    \n",
    "\n",
    "        # Merge with observed SLAs\n",
    "        combined_ds = xr.merge([combined_ds, slas_obs_xr.sel(RGIId=f\"RGI60-0{rgi_id}\")])\n",
    "\n",
    "        # Calculate difference\n",
    "        combined_ds['mod-obs_SLA'] = combined_ds['glac_snowline_monthly'] - combined_ds['SLA_obs_m']\n",
    "\n",
    "        # Save to file\n",
    "        combined_ds.to_netcdf(out_fn)\n",
    "\n",
    "    else:\n",
    "        # Load combined dataset rom file\n",
    "        combined_ds = xr.open_dataset(out_fn)\n",
    "\n",
    "    # Load original model parameters\n",
    "    modelprms_fn = os.path.join(model_path, '..', 'Rounce_et_al_2023', 'modelprms', f\"{rgi_id}-modelprms_dict.pkl\")\n",
    "    modelprms = pd.read_pickle(modelprms_fn)\n",
    "\n",
    "    # Calculate RMSE for each run's snowline altitudes\n",
    "    \n",
    "    diff = combined_ds['mod-obs_SLA']\n",
    "    rmse_by_run = np.sqrt((diff**2).mean(dim='time'))\n",
    "    combined_ds['rmse'] = rmse_by_run\n",
    "\n",
    "    # identify parameter combinations with the lowest RMSE\n",
    "    df_plot = combined_ds[['tbias', 'ddfsnow', 'kp', 'rmse', 'mod-obs_SLA']].to_dataframe().reset_index()\n",
    "    df_plot = df_plot.dropna(subset=['rmse'])\n",
    "    df_plot_best = df_plot.loc[df_plot['rmse'].idxmin()]\n",
    "\n",
    "    # subset the model dataset for the original vs. best runs\n",
    "    # original\n",
    "    squared_diffs = sum((combined_ds[var] - modelprms['emulator'][var][0])**2 for var in ['kp', 'tbias', 'ddfsnow'])\n",
    "    best_run_idx = squared_diffs.argmin(dim=\"run\")\n",
    "    combined_ds_original = combined_ds.sel(run=best_run_idx, glac=0)\n",
    "    # best\n",
    "    combined_ds_best = combined_ds.sel(run=int(df_plot_best['run']), glac=0)\n",
    "\n",
    "    # Plot RMSE as a function of tbias, ddfsnow, and kp\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8), gridspec_kw=dict(height_ratios=[2,2,1]))\n",
    "    sns.scatterplot(df_plot, x='tbias', y='ddfsnow', \n",
    "                    size='kp', hue='rmse', palette='viridis_r', sizes=(2,50), \n",
    "                    ax=ax[0])\n",
    "    # original parameter combinations\n",
    "    ax[0].plot(modelprms['emulator']['tbias'], modelprms['emulator']['ddfsnow'], 's', \n",
    "            markersize=15, markeredgecolor='m', markerfacecolor='None', markeredgewidth=2, label='Original')\n",
    "    # best parameter combination\n",
    "    ax[0].plot(df_plot_best['tbias'], df_plot_best['ddfsnow'], \n",
    "            '*', markersize=15, markeredgecolor='m', markerfacecolor='None', markeredgewidth=2, label='Lowest RMSE')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(f'{name} ({rgi_id})')\n",
    "    ax[0].grid(True)\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('rmse', 'RMSE [m]').replace('kp', 'Precipitation factor') for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right', bbox_to_anchor=[1.1, 0.4, 0.2, 0.2])\n",
    "    # modeled and observed snowline time series\n",
    "    ax[1].plot(combined_ds_original.time, combined_ds_original.glac_snowline_monthly, '-', color='gray', label='Original model')\n",
    "    ax[1].plot(combined_ds_best.time, combined_ds_best.glac_snowline_monthly, '-k', label='Best model')\n",
    "    ax[1].plot(scs['datetime'], scs['SLA_m'], '.m', markersize=5, label='Observed')\n",
    "    ax[1].legend(loc='center left', bbox_to_anchor=[1.0, 0.4, 0.2, 0.2])\n",
    "    ax[1].set_ylabel('Snowline altitude [m.a.s.l.]')\n",
    "    # observed - modeled snowline altitude\n",
    "    ax[2].plot(combined_ds_original.time, combined_ds_original['mod-obs_SLA'], '.', color='gray', markersize=10, label='Original model')\n",
    "    ax[2].plot(combined_ds_best.time, combined_ds_best['mod-obs_SLA'], '.k', markersize=5, label='Best model')\n",
    "    ax[2].legend(loc='center left', bbox_to_anchor=[1.0, 0.4, 0.2, 0.2])\n",
    "    ax[2].grid()\n",
    "    ax[2].set_ylabel('Modeled $-$ observed\\nsnowline altitude [m]')\n",
    "    ax[1].set_xlim(ax[2].get_xlim())\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Save figure to file\n",
    "    fig_fn = os.path.join(figures_path, f\"{rgi_id}_PyGEM_comparison.png\")\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('Figure saved to file:', fig_fn)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Compile results in dataframe\n",
    "    df_results = pd.DataFrame({'Original': \n",
    "                           [modelprms['emulator']['tbias'][0],\n",
    "                            modelprms['emulator']['ddfsnow'][0],\n",
    "                            modelprms['emulator']['kp'][0]],\n",
    "                            'Best': \n",
    "                            [df_plot_best['tbias'],\n",
    "                             df_plot_best['ddfsnow'],\n",
    "                             df_plot_best['kp']]\n",
    "                           },\n",
    "                           index=['tbias', 'ddfsnow', 'kp'])\n",
    "    # add site RGI ID and name columns\n",
    "    df_results.index = pd.MultiIndex.from_product([[f\"RGI60-0{rgi_id}\"], df_results.index], \n",
    "                                                names=['RGIId', 'parameter'])\n",
    "    df_results['name'] = name\n",
    "    # concatenate to full dataframe\n",
    "    df_results_full = pd.concat([df_results_full, df_results])\n",
    "\n",
    "# Add Original - Best column\n",
    "df_results_full['Original-Best'] = df_results_full['Original'] - df_results_full['Best']\n",
    "\n",
    "# Save full data frame to file\n",
    "df_results_full_fn = os.path.join(scm_path, 'analysis', 'PyGEM_comparison_params.csv')\n",
    "df_results_full.to_csv(df_results_full_fn, index=True)\n",
    "print('Results saved to file:', df_results_full_fn)\n",
    "df_results_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "combined_ds_gb = combined_ds.sel(run=1, glac=0).glac_snowline_monthly.groupby(combined_ds['time'].dt.isocalendar().week)\n",
    "ax.fill_between(combined_ds_gb.reduce(np.nanpercentile, q=25).week,\n",
    "                 combined_ds_gb.reduce(np.nanpercentile, q=25),\n",
    "                 combined_ds_gb.reduce(np.nanpercentile, q=75), color='k', alpha=0.3, edgecolor='None')\n",
    "ax.plot(combined_ds_gb.reduce(np.nanpercentile, q=50).week, combined_ds_gb.reduce(np.nanpercentile, q=50), '-k')\n",
    "\n",
    "scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "scs.groupby('WOY')['SLA_from_AAR_m'].median().plot(color='m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DEGREE-DAY FACTORS OF SNOW #####\n",
    "\n",
    "### MODELED ###\n",
    "# # Check if already exists in file\n",
    "# fsnow_mod_fn = os.path.join(scm_path, 'analysis', 'fsnow_modeled.csv')\n",
    "# if not os.path.exists(fsnow_mod_fn):\n",
    "#     print('Compiling modeled melt factors of snow')\n",
    "#     # Initialize dataframe\n",
    "#     fsnow_mod = pd.DataFrame()\n",
    "#     # Iterate over RGI IDs\n",
    "#     for rgi_id in tqdm(slas_mod['RGIId'].drop_duplicates().values):\n",
    "#         # Load model parameters\n",
    "#         modelprm_fn = os.path.join(model_path, 'modelprms', f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "#         modelprm = pd.read_pickle(modelprm_fn)\n",
    "#         # Take the median of MCMC fsnow results (not much different than the mean)\n",
    "#         ddfsnow_mcmc = np.array(modelprm['MCMC']['ddfsnow']['chain_0'])\n",
    "#         df = pd.DataFrame({\"RGIId\": [rgi_id],\n",
    "#                            \"fsnow_mod_m/C/d\": [np.median(ddfsnow_mcmc)]})\n",
    "#         # Concatenate df to full dataframe\n",
    "#         fsnow_mod = pd.concat([fsnow_mod, df])\n",
    "#     # Save to file\n",
    "#     fsnow_mod.reset_index(drop=True, inplace=True)\n",
    "#     fsnow_mod.to_csv(fsnow_mod_fn, index=False)\n",
    "#     print('Compiled melt factors of snow saved to file:', fsnow_mod_fn)\n",
    "\n",
    "# else:\n",
    "#     fsnow_mod = pd.read_csv(fsnow_mod_fn)\n",
    "#     print('Compiled melt factors of snow loaded from file.')\n",
    "\n",
    "# plt.hist(fsnow_mod['fsnow_mod_m/C/d'] * 1e3, bins=100)\n",
    "# plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bb9d4",
   "metadata": {},
   "source": [
    "Adjust the modeled degree-day factors of snow ($f_{snow}$) using the modeled SMB and cumulative PDDs from ERA5 downscaled to the snowline.\n",
    "\n",
    "$SMB(x,t) = Accumulation - Melt = \\Sigma Snowfall(x,t) - \\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t \\cdot f_{snow}$\n",
    "\n",
    "where $t_{melt}$ is the start of the melt season and $\\Delta t$ is $t-t_{melt}$. \n",
    "\n",
    "At the snowline, SMB = 0. Rearranging:\n",
    "\n",
    "$f_{snow}(x,t) = \\frac{\\Sigma Snowfall(x,t)}{\\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t} $\n",
    "\n",
    "If SMB = 10 m at the snowline on day 100 of the melt season and the cumulative PDD are 100 $^{\\circ}C$, this means that the model underestimated melt by 10 m / (100 $^{\\circ}C \\cdot$ 100 days) = 0.001 m/C/d. If the modeled melt factor of snow is 2 m/C/d, adjust the fsnow to 2.001 m/C/d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### OBSERVED ###\n",
    "# fsnow_merged_fn = os.path.join(out_path, 'fsnow_observed_modeled.csv')\n",
    "# if not os.path.exists(fsnow_merged_fn):\n",
    "\n",
    "#     # Intialize results for all sites\n",
    "#     fsnow_merged = pd.DataFrame()\n",
    "    \n",
    "#     # Iterate over sites\n",
    "#     for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "#         if type(rgi_id) != str:\n",
    "#             continue\n",
    "#         if 'RGI' not in rgi_id:\n",
    "#             continue\n",
    "            \n",
    "#         ### Load input data\n",
    "#         # Get modeled fsnow\n",
    "#         fsnow_mod_site = fsnow_mod.loc[fsnow_mod['RGIId']==rgi_id, 'fsnow_mod_m/C/d'].values[0]\n",
    "#         # Load ERA-Land data\n",
    "#         era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f\"{rgi_id}_ERA5-Land_daily_means.csv\")\n",
    "#         era_df = pd.read_csv(era_fn)\n",
    "#         era_df['Date'] = pd.to_datetime(era_df['Date'])\n",
    "#         # Load model temperature bias\n",
    "#         modelprm_fn = os.path.join(model_path, 'modelprms', f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "#         modelprm = pd.read_pickle(modelprm_fn)\n",
    "#         tbias = np.median(modelprm['MCMC']['tbias']['chain_0'])\n",
    "#         # Load centerline elevation profile\n",
    "#         smb_fn = glob.glob(os.path.join(model_path, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "#         smb = xr.open_dataset(smb_fn).squeeze()\n",
    "#         h = smb.bin_surface_h_initial.values.ravel()\n",
    "#         # Grab snowlines for site\n",
    "#         slas_site = slas_merged.sel(RGIId=rgi_id)\n",
    "#         # Don't include dates after September\n",
    "#         def filter_month_range(month):\n",
    "#             return month < 9\n",
    "#         slas_site = slas_site.sel(time=filter_month_range(slas_site['time.month']))\n",
    "        \n",
    "#         ### Downscale air temperatures to glacier surface using lapse rates, calculate PDDs\n",
    "#         era_ds = xr.Dataset(\n",
    "#             coords={'h': h, 'time': era_df['Date']},\n",
    "#             data_vars={'temp_C': (['time'], era_df['mean_temperature_2m_C'].values),\n",
    "#                     'lapse_rate': (['time'], era_df['lapse_rate_C/m'])})\n",
    "#         # implement temperature bias\n",
    "#         era_ds['temp_C'] -= tbias\n",
    "#         era_ds['h_diff'] = era_df['ERA5_height_mean_m'].values[0] - era_ds['h']\n",
    "#         era_ds['temp_downscaled_C'] = era_ds['temp_C'] - era_ds['lapse_rate'] * era_ds['h_diff']\n",
    "#         era_ds['PDD'] = xr.where(era_ds['temp_downscaled_C'] > 0, era_ds['temp_downscaled_C'], 0)\n",
    "#         era_ds['PDD_cumsum'] = era_ds['PDD'].groupby('time.year').cumsum()\n",
    "        \n",
    "#         ### Identify the melt season start date (first PDD > 0) for each elevation\n",
    "#         def find_first_positive(group):\n",
    "#             # Mask PDD = 0\n",
    "#             mask = group > 0\n",
    "#             # Find the first index where PDD > 0\n",
    "#             first_index = mask.argmax(dim=\"time\")\n",
    "#             # Check if no positive PDD exists for the group\n",
    "#             no_positive = ~mask.any(dim=\"time\")\n",
    "#             # Grab the corresponding time values\n",
    "#             time_values = group[\"time\"].isel(time=first_index)\n",
    "#             # Replace invalid times with NaT for no_positive cases\n",
    "#             time_values = time_values.where(~no_positive, np.datetime64(\"NaT\"))\n",
    "#             return time_values\n",
    "#         # Apply the function to each year and elevation group\n",
    "#         era_ds['melt_season_start_date'] = (era_ds[\"PDD_cumsum\"]\n",
    "#                                             .groupby(\"time.year\")\n",
    "#                                             .map(find_first_positive))\n",
    "        \n",
    "#         ### Interpolate cumulative PDDs at SLAs\n",
    "#         pdds_slas = np.array([float(era_ds.sel(time=date, h=sla, method='nearest')['PDD_cumsum'].values) \n",
    "#                               for (date,sla) in list(zip(slas_site.time.values, slas_site['SLA_obs_m'].values))])\n",
    "\n",
    "#         ### Interpolate melt season start dates at SLAs\n",
    "#         melt_start_dates = np.array([era_ds.sel(year=year, h=sla, method='nearest')['melt_season_start_date'].values\n",
    "#                                      for (year,sla) in list(zip(slas_site['time.year'].values, slas_site['SLA_obs_m'].values))])\n",
    "\n",
    "#         # Compile in dataframe\n",
    "#         df = pd.DataFrame({'Date': slas_site.time.values,\n",
    "#                            'Year': slas_site['time.year'].values,\n",
    "#                            'SMB_at_SLA_obs_mwe': slas_site['SMB_at_SLA_obs_mwe'].values,\n",
    "#                            'PDD_cumsum_at_SLA_C': pdds_slas,\n",
    "#                            'melt_season_start_date': melt_start_dates})\n",
    "#         df['RGIId'] = rgi_id\n",
    "#         df.dropna(inplace=True)\n",
    "#         df = df.loc[df['PDD_cumsum_at_SLA_C'] > 0] # remove rows with 0 PDDs (to avoid dividing by 0)\n",
    "#         df['days_since_melt_season_start_date'] = ((df['Date'] - df['melt_season_start_date']) / np.timedelta64(1, 'D')).astype(int)\n",
    "        \n",
    "#         ### Calculate adjustment for modeled fsnow\n",
    "#         df['fsnow_mod_adj'] = (df['SMB_at_SLA_obs_mwe'] \n",
    "#                                 / (df['PDD_cumsum_at_SLA_C'] \n",
    "#                                     * df['days_since_melt_season_start_date']))\n",
    "                \n",
    "#         ### Add adjustment to fsnow_mod\n",
    "#         df['fsnow_obs'] = fsnow_mod_site + df['fsnow_mod_adj']\n",
    "        \n",
    "#         # Remove unrealistic values\n",
    "#         df.loc[df['fsnow_obs'] > 0.01, 'fsnow_obs'] = np.nan\n",
    "#         df.loc[df['fsnow_obs'] <= 0.0, 'fsnow_obs'] = np.nan\n",
    "        \n",
    "#         ### Save the median\n",
    "#         fsnow_df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "#                                  'fsnow_obs_m/C/d': [np.nanmedian(df['fsnow_obs'])],\n",
    "#                                  'fsnow_mod_m/C/d': [fsnow_mod_site]})\n",
    "#         fsnow_merged = pd.concat([fsnow_merged, fsnow_df], axis=0)\n",
    "        \n",
    "#         ### Plot an example\n",
    "#         if rgi_id=='RGI60-01.00032':\n",
    "#             plt.rcParams.update({'font.size': 12, 'font.sans-serif': 'Arial'})\n",
    "#             fig, ax = plt.subplots(3, 1, figsize=(8,10))\n",
    "#             ax[0].plot(era_ds.time.data, era_ds['temp_C'], '-k', linewidth=0.5)\n",
    "#             ax[0].set_ylabel('Air temperature [$^{\\circ}$C]')\n",
    "#             ax[0].grid()\n",
    "#             cmap = matplotlib.colors.LinearSegmentedColormap.from_list('my_cmap', ['w', '#fb6a4a', '#67000d']) # white to red\n",
    "#             era_ds['PDD_cumsum'].transpose().plot(cmap=cmap, ax=ax[1], \n",
    "#                                                 cbar_kwargs={'orientation': 'horizontal', \n",
    "#                                                             'shrink': 0.5, \n",
    "#                                                             'label': 'Cumulative PDD [$^{\\circ}$C]'})\n",
    "#             ax[1].set_ylabel('Elevation [m]')\n",
    "#             ax[1].plot(slas_site.time.values, slas_site['SLA_obs_m'].values, '*k', label='Snowline altitude')\n",
    "#             # melt season start\n",
    "#             for year in era_ds.year.data[1:]:\n",
    "#                 era_ds_year = era_ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-08-01\"))\n",
    "#                 xmesh, ymesh = np.meshgrid(era_ds_year.time.data, era_ds.h.data)\n",
    "#                 ax[1].contour(xmesh, ymesh, era_ds_year['PDD_cumsum'].data.transpose(), levels=[0], colors=['gray'])\n",
    "#             ax[1].plot(pd.Timestamp('2010-01-01'), 0, '-', color='gray', label='Melt season start')\n",
    "                        \n",
    "#             ax[1].legend(loc='upper left')\n",
    "#             ax[1].set_xlabel('')\n",
    "#             ax[2].plot(df['Date'], df['fsnow_obs'] * 1e3, '.k')\n",
    "#             ax[2].axhline(fsnow_merged['fsnow_mod_m/C/d'].values[0] * 1e3, color='k', linestyle='--', label='Modeled')\n",
    "#             ax[2].axhline(fsnow_merged['fsnow_obs_m/C/d'].values[0] * 1e3, color='k', label='Observed median')\n",
    "#             ax[2].legend(loc='lower left')\n",
    "#             ax[2].set_ylabel('$f_{snow}$ [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "#             ax[2].grid()\n",
    "#             ax[2].set_ylim(2, 5)\n",
    "#             labels = ['a', 'b', 'c']\n",
    "#             for i, axis in enumerate(ax):\n",
    "#                 axis.set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "#                 axis.text(0.97, 0.85, labels[i], transform=axis.transAxes, fontweight='bold', fontsize=16)\n",
    "#             fig.tight_layout()\n",
    "#             plt.show()\n",
    "#             # save figure\n",
    "#             fig_fn = os.path.join(code_path, 'figures', 'figS5_melt_factors_example.png')\n",
    "#             fig.savefig(fig_fn)\n",
    "#             print('Figure saved to file:', fig_fn)\n",
    "        \n",
    "#     # Save results to file\n",
    "#     fsnow_merged.to_csv(fsnow_merged_fn, index=False)\n",
    "#     print('Observed fsnow saved to file:', fsnow_merged_fn)  \n",
    "    \n",
    "# else:\n",
    "#     fsnow_merged = pd.read_csv(fsnow_merged_fn)\n",
    "#     print('Observed fsnow loaded from file')  \n",
    "\n",
    "# plt.hist(fsnow_merged['fsnow_obs_m/C/d'] * 1e3, bins=100)\n",
    "# plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "# plt.ylabel('Counts')\n",
    "# plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be1a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
